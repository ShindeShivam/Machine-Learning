{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:11.751348Z",
     "iopub.status.busy": "2025-09-15T07:15:11.751067Z",
     "iopub.status.idle": "2025-09-15T07:15:19.303117Z",
     "shell.execute_reply": "2025-09-15T07:15:19.302445Z",
     "shell.execute_reply.started": "2025-09-15T07:15:11.751321Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:19.304728Z",
     "iopub.status.busy": "2025-09-15T07:15:19.304364Z",
     "iopub.status.idle": "2025-09-15T07:15:19.420746Z",
     "shell.execute_reply": "2025-09-15T07:15:19.420201Z",
     "shell.execute_reply.started": "2025-09-15T07:15:19.304700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.device_count())\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:19.421513Z",
     "iopub.status.busy": "2025-09-15T07:15:19.421324Z",
     "iopub.status.idle": "2025-09-15T07:15:20.167995Z",
     "shell.execute_reply": "2025-09-15T07:15:20.167133Z",
     "shell.execute_reply.started": "2025-09-15T07:15:19.421497Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "def download_shakespeare_text():\n",
    "    path = Path(\"datasets/shakespeare/shakespeare.txt\")\n",
    "    if not path.is_file():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://homl.info/shakespeare\"\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "    return path.read_text()\n",
    "shakespeare_text = download_shakespeare_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:20.169995Z",
     "iopub.status.busy": "2025-09-15T07:15:20.169757Z",
     "iopub.status.idle": "2025-09-15T07:15:20.174082Z",
     "shell.execute_reply": "2025-09-15T07:15:20.173274Z",
     "shell.execute_reply.started": "2025-09-15T07:15:20.169977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:20.175290Z",
     "iopub.status.busy": "2025-09-15T07:15:20.175034Z",
     "iopub.status.idle": "2025-09-15T07:15:20.202637Z",
     "shell.execute_reply": "2025-09-15T07:15:20.202037Z",
     "shell.execute_reply.started": "2025-09-15T07:15:20.175273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(shakespeare_text.lower()))\n",
    "''.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:20.203504Z",
     "iopub.status.busy": "2025-09-15T07:15:20.203270Z",
     "iopub.status.idle": "2025-09-15T07:15:20.218136Z",
     "shell.execute_reply": "2025-09-15T07:15:20.217570Z",
     "shell.execute_reply.started": "2025-09-15T07:15:20.203483Z"
    }
   },
   "outputs": [],
   "source": [
    "char_to_id = {char:idx for idx, char in enumerate(vocab)}\n",
    "id_to_char = {idx:char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:20.218958Z",
     "iopub.status.busy": "2025-09-15T07:15:20.218723Z",
     "iopub.status.idle": "2025-09-15T07:15:20.237547Z",
     "shell.execute_reply": "2025-09-15T07:15:20.237023Z",
     "shell.execute_reply.started": "2025-09-15T07:15:20.218942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_id[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:20.238484Z",
     "iopub.status.busy": "2025-09-15T07:15:20.238179Z",
     "iopub.status.idle": "2025-09-15T07:15:20.254853Z",
     "shell.execute_reply": "2025-09-15T07:15:20.254253Z",
     "shell.execute_reply.started": "2025-09-15T07:15:20.238459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_char[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:20.255608Z",
     "iopub.status.busy": "2025-09-15T07:15:20.255417Z",
     "iopub.status.idle": "2025-09-15T07:15:20.269192Z",
     "shell.execute_reply": "2025-09-15T07:15:20.268476Z",
     "shell.execute_reply.started": "2025-09-15T07:15:20.255594Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    return torch.tensor([char_to_id[char] for char in text.lower()])\n",
    "def decode_text(char_ids):\n",
    "    return ''.join([id_to_char[char_id.item()] for char_id in char_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:20.271929Z",
     "iopub.status.busy": "2025-09-15T07:15:20.271250Z",
     "iopub.status.idle": "2025-09-15T07:15:20.339180Z",
     "shell.execute_reply": "2025-09-15T07:15:20.338624Z",
     "shell.execute_reply.started": "2025-09-15T07:15:20.271909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 17, 24, 24, 27,  1, 35, 27, 30, 24, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = encode_text(\"hello world\")\n",
    "encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:20.340053Z",
     "iopub.status.busy": "2025-09-15T07:15:20.339794Z",
     "iopub.status.idle": "2025-09-15T07:15:20.345278Z",
     "shell.execute_reply": "2025-09-15T07:15:20.344655Z",
     "shell.execute_reply.started": "2025-09-15T07:15:20.340037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_text(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:20.346243Z",
     "iopub.status.busy": "2025-09-15T07:15:20.345982Z",
     "iopub.status.idle": "2025-09-15T07:15:20.360841Z",
     "shell.execute_reply": "2025-09-15T07:15:20.360357Z",
     "shell.execute_reply.started": "2025-09-15T07:15:20.346218Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDatasetBuilder:\n",
    "    def __init__(self, series, window_length=56, horizon = 1):\n",
    "        self.encoded_text = encode_text(series)\n",
    "        self.window_length = window_length\n",
    "        self.horizon = horizon\n",
    "    def create_X_y(self):\n",
    "        X, y =[],[]\n",
    "        for i in range(len(self.encoded_text) - self.window_length):\n",
    "            window = self.encoded_text[i:i+self.window_length]\n",
    "            future = self.encoded_text[i+self.window_length:i+self.window_length+self.horizon]\n",
    "            X.append(window)\n",
    "            y.append(future)\n",
    "        return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:20.361537Z",
     "iopub.status.busy": "2025-09-15T07:15:20.361376Z",
     "iopub.status.idle": "2025-09-15T07:15:20.392549Z",
     "shell.execute_reply": "2025-09-15T07:15:20.391981Z",
     "shell.execute_reply.started": "2025-09-15T07:15:20.361516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : to be or n\n",
      "y : o\n",
      "x : o be or no\n",
      "y : t\n",
      "x :  be or not\n",
      "y :  \n",
      "x : be or not \n",
      "y : t\n",
      "x : e or not t\n",
      "y : o\n",
      "x :  or not to\n",
      "y :  \n",
      "x : or not to \n",
      "y : b\n",
      "x : r not to b\n",
      "y : e\n"
     ]
    }
   ],
   "source": [
    "to_be_dataset = TimeSeriesDatasetBuilder(series=\"to be or not to be\", window_length=10)\n",
    "X,y = to_be_dataset.create_X_y()\n",
    "for i in range(len(X)):\n",
    "    decoded_x = decode_text(X[i])\n",
    "    decoded_y = decode_text(y[i])\n",
    "    print(f\"x : {decoded_x}\")\n",
    "    print(f\"y : {decoded_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:20.393475Z",
     "iopub.status.busy": "2025-09-15T07:15:20.393287Z",
     "iopub.status.idle": "2025-09-15T07:15:39.503775Z",
     "shell.execute_reply": "2025-09-15T07:15:39.503020Z",
     "shell.execute_reply.started": "2025-09-15T07:15:20.393461Z"
    }
   },
   "outputs": [],
   "source": [
    "window_length = 56\n",
    "batch_size = 1024 \n",
    "builder = TimeSeriesDatasetBuilder(shakespeare_text,window_length)\n",
    "X, y = builder.create_X_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:39.504917Z",
     "iopub.status.busy": "2025-09-15T07:15:39.504642Z",
     "iopub.status.idle": "2025-09-15T07:15:39.510409Z",
     "shell.execute_reply": "2025-09-15T07:15:39.509680Z",
     "shell.execute_reply.started": "2025-09-15T07:15:39.504895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18, 21, 30, ...,  1, 31, 28],\n",
       "       [21, 30, 31, ..., 31, 28, 17],\n",
       "       [30, 31, 32, ..., 28, 17, 13],\n",
       "       ...,\n",
       "       [27, 30, 32, ..., 23, 21, 26],\n",
       "       [30, 32, 33, ..., 21, 26, 19],\n",
       "       [32, 33, 26, ..., 26, 19,  8]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:39.511367Z",
     "iopub.status.busy": "2025-09-15T07:15:39.511164Z",
     "iopub.status.idle": "2025-09-15T07:15:39.596363Z",
     "shell.execute_reply": "2025-09-15T07:15:39.595768Z",
     "shell.execute_reply.started": "2025-09-15T07:15:39.511351Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype = torch.long)\n",
    "y_tensor = torch.tensor(y, dtype = torch.long).squeeze(-1)\n",
    "\n",
    "train_set = TensorDataset(X_tensor[:1_000_000], y_tensor[:1_000_000])\n",
    "valid_set = TensorDataset(X_tensor[1_000_000:1_060_000],y_tensor[1_000_000:1_060_000])\n",
    "test_set = TensorDataset(X_tensor[1_060_000:], y_tensor[1_060_000:])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size,\n",
    "                         num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size,\n",
    "                        num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:39.597236Z",
     "iopub.status.busy": "2025-09-15T07:15:39.597033Z",
     "iopub.status.idle": "2025-09-15T07:15:49.903196Z",
     "shell.execute_reply": "2025-09-15T07:15:49.902557Z",
     "shell.execute_reply.started": "2025-09-15T07:15:39.597219Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "def evaluate_tm(model, data_loader, metric):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric.update(y_pred, y_batch)\n",
    "    return metric.compute()\n",
    "\n",
    "def train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs, patience=2,\n",
    "         factor=0.5):\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", patience=patience, factor=factor\n",
    "    )\n",
    "    history = {\"train_losses\":[],\"train_metrics\":[],\"valid_metrics\":[]}\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        metric.reset()\n",
    "        model.train()\n",
    "        for idx,( X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            metric.update(y_pred, y_batch)\n",
    "            print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n",
    "            print(f\", loss ={total_loss/(idx+1 ):.4f}\", end=\"\")\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        history[\"train_losses\"].append(mean_loss)\n",
    "        history[\"train_metrics\"].append(metric.compute().item())\n",
    "        val_metric = evaluate_tm(model, valid_loader, metric).item()\n",
    "        history[\"valid_metrics\"].append(val_metric)\n",
    "        scheduler.step(val_metric)\n",
    "        print(f\"Epoch:{epoch+1}/{n_epochs}, \"\n",
    "             f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n",
    "             f\"Train Metric: {history['train_metrics'][-1]:.4f}, \"\n",
    "             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:49.904213Z",
     "iopub.status.busy": "2025-09-15T07:15:49.903900Z",
     "iopub.status.idle": "2025-09-15T07:15:50.199250Z",
     "shell.execute_reply": "2025-09-15T07:15:50.198547Z",
     "shell.execute_reply.started": "2025-09-15T07:15:49.904197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPU's\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ShakespeareModel(\n",
       "    (embed): Embedding(39, 10)\n",
       "    (gru): GRU(10, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
       "    (output): Linear(in_features=128, out_features=39, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_hidden=128, n_layers=2, embed_size=10, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, n_hidden, num_layers=n_layers,\n",
    "                         batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        embeddings = self.embed(X)\n",
    "        outputs, _states = self.gru(embeddings)\n",
    "        return self.output(outputs[:, -1, :])\n",
    "model = ShakespeareModel(len(vocab)).to(device)\n",
    "if torch.cuda.device_count()>1:\n",
    "    print(\"Using\", torch.cuda.device_count(),\"GPU's\")\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:15:50.200278Z",
     "iopub.status.busy": "2025-09-15T07:15:50.200018Z",
     "iopub.status.idle": "2025-09-15T07:22:16.831892Z",
     "shell.execute_reply": "2025-09-15T07:22:16.830909Z",
     "shell.execute_reply.started": "2025-09-15T07:15:50.200260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 977/977, loss =1.9251Epoch:1/20, Train Loss: 1.9251, Train Metric: 0.4352, Valid Metric: 0.4791\n",
      "Batch 977/977, loss =1.5565Epoch:2/20, Train Loss: 1.5565, Train Metric: 0.5280, Valid Metric: 0.5152\n",
      "Batch 977/977, loss =1.4798Epoch:3/20, Train Loss: 1.4798, Train Metric: 0.5465, Valid Metric: 0.5294\n",
      "Batch 977/977, loss =1.4423Epoch:4/20, Train Loss: 1.4423, Train Metric: 0.5564, Valid Metric: 0.5358\n",
      "Batch 977/977, loss =1.4189Epoch:5/20, Train Loss: 1.4189, Train Metric: 0.5622, Valid Metric: 0.5447\n",
      "Batch 977/977, loss =1.4019Epoch:6/20, Train Loss: 1.4019, Train Metric: 0.5658, Valid Metric: 0.5473\n",
      "Batch 977/977, loss =1.3897Epoch:7/20, Train Loss: 1.3897, Train Metric: 0.5691, Valid Metric: 0.5481\n",
      "Batch 977/977, loss =1.3807Epoch:8/20, Train Loss: 1.3807, Train Metric: 0.5711, Valid Metric: 0.5510\n",
      "Batch 977/977, loss =1.3730Epoch:9/20, Train Loss: 1.3730, Train Metric: 0.5734, Valid Metric: 0.5536\n",
      "Batch 977/977, loss =1.3665Epoch:10/20, Train Loss: 1.3665, Train Metric: 0.5748, Valid Metric: 0.5550\n",
      "Batch 977/977, loss =1.3620Epoch:11/20, Train Loss: 1.3620, Train Metric: 0.5757, Valid Metric: 0.5531\n",
      "Batch 977/977, loss =1.3577Epoch:12/20, Train Loss: 1.3577, Train Metric: 0.5766, Valid Metric: 0.5548\n",
      "Batch 977/977, loss =1.3534Epoch:13/20, Train Loss: 1.3534, Train Metric: 0.5776, Valid Metric: 0.5554\n",
      "Batch 977/977, loss =1.3501Epoch:14/20, Train Loss: 1.3501, Train Metric: 0.5783, Valid Metric: 0.5557\n",
      "Batch 977/977, loss =1.3484Epoch:15/20, Train Loss: 1.3484, Train Metric: 0.5785, Valid Metric: 0.5566\n",
      "Batch 977/977, loss =1.3451Epoch:16/20, Train Loss: 1.3451, Train Metric: 0.5793, Valid Metric: 0.5588\n",
      "Batch 977/977, loss =1.3432Epoch:17/20, Train Loss: 1.3432, Train Metric: 0.5797, Valid Metric: 0.5617\n",
      "Batch 977/977, loss =1.3408Epoch:18/20, Train Loss: 1.3408, Train Metric: 0.5807, Valid Metric: 0.5622\n",
      "Batch 977/977, loss =1.3386Epoch:19/20, Train Loss: 1.3386, Train Metric: 0.5808, Valid Metric: 0.5608\n",
      "Batch 977/977, loss =1.3374Epoch:20/20, Train Loss: 1.3374, Train Metric: 0.5814, Valid Metric: 0.5605\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "xentropy = nn.CrossEntropyLoss()\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\",num_classes=len(vocab)).to(device)\n",
    "optimizer = torch.optim.NAdam(model.parameters())\n",
    "\n",
    "history = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical Shape Journey\n",
    "\n",
    "# X (Input) Path:\n",
    "\n",
    "#→ windowing → [1115294, 56] \n",
    "#→ batching → [1024, 56]\n",
    "#→ embed() → [1024, 56, 10]\n",
    "#→ GRU() → [1024, 56, 128] \n",
    "#→ [:, -1, :] → [1024, 128]\n",
    "#→ Linear() → [1024, 39]\n",
    "\n",
    "# y (Target) Path:\n",
    "\n",
    "# → windowing → [1115294, 1] \n",
    "# → squeeze(-1) → [1115294]\n",
    "# → batching → [1024]\n",
    "# → CrossEntropyLoss with [1024, 39]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:25:22.932888Z",
     "iopub.status.busy": "2025-09-15T07:25:22.932592Z",
     "iopub.status.idle": "2025-09-15T07:25:22.939568Z",
     "shell.execute_reply": "2025-09-15T07:25:22.938995Z",
     "shell.execute_reply.started": "2025-09-15T07:25:22.932867Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"my_shakespeare_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:29:51.497204Z",
     "iopub.status.busy": "2025-09-15T07:29:51.496485Z",
     "iopub.status.idle": "2025-09-15T07:29:51.502328Z",
     "shell.execute_reply": "2025-09-15T07:29:51.501566Z",
     "shell.execute_reply.started": "2025-09-15T07:29:51.497179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"To be or not to b\"\n",
    "encoded_text = encode_text(text).unsqueeze(dim=0).to(device)\n",
    "encoded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:34:18.003509Z",
     "iopub.status.busy": "2025-09-15T07:34:18.003267Z",
     "iopub.status.idle": "2025-09-15T07:34:18.011117Z",
     "shell.execute_reply": "2025-09-15T07:34:18.010347Z",
     "shell.execute_reply.started": "2025-09-15T07:34:18.003493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_logits = model(encoded_text)\n",
    "    predicted_char_id = y_logits[0].argmax().item()\n",
    "    predicted_char = id_to_char[predicted_char_id]\n",
    "predicted_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T07:44:44.637804Z",
     "iopub.status.busy": "2025-09-15T07:44:44.637498Z",
     "iopub.status.idle": "2025-09-15T07:44:44.642444Z",
     "shell.execute_reply": "2025-09-15T07:44:44.641699Z",
     "shell.execute_reply.started": "2025-09-15T07:44:44.637780Z"
    }
   },
   "outputs": [],
   "source": [
    "def next_char(model, text, temperature=0.7):\n",
    "    model.eval()\n",
    "    encoded_text = encode_text(text).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        y_logits = model(encoded_text)\n",
    "        y_probas = torch.softmax(y_logits/temperature, dim=-1)\n",
    "        predicted_char_id = torch.multinomial(y_probas,num_samples=1).item()     \n",
    "        return id_to_char[predicted_char_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:00:09.399239Z",
     "iopub.status.busy": "2025-09-15T08:00:09.398993Z",
     "iopub.status.idle": "2025-09-15T08:00:09.403758Z",
     "shell.execute_reply": "2025-09-15T08:00:09.403136Z",
     "shell.execute_reply.started": "2025-09-15T08:00:09.399222Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def generate_text(model, text, n_chars=100,temperature=0.7):\n",
    "    print(text, end='', flush=True)\n",
    "    for _ in range(n_chars):\n",
    "        char = next_char(model, text, temperature)\n",
    "        text += char\n",
    "        print(char, end='', flush=True)\n",
    "        time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T08:00:10.813302Z",
     "iopub.status.busy": "2025-09-15T08:00:10.812701Z",
     "iopub.status.idle": "2025-09-15T08:00:17.402941Z",
     "shell.execute_reply": "2025-09-15T08:00:17.402338Z",
     "shell.execute_reply.started": "2025-09-15T08:00:10.813278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be\n",
      "makes this good to mercy of the head?\n",
      "\n",
      "edward:\n",
      "and shall not be rest that i may should not every fear.\n",
      "\n",
      "somerset:\n",
      "why is the jight thou speaks to his land, when faults at crown,\n",
      "or i cannot said most less than here and honour'd to the will,\n",
      "i do a patience which we at his capulet.\n",
      "\n",
      "romeo:\n",
      "why, that's grief to my conscience with her, and short me;\n",
      "and in the people of far of his assus;\n",
      "who shall i can see thy earth will i think, i come and but your world-drown\n",
      "and destroyed as never say our cauNone\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model,\"To be or not to b\",n_chars=500 ))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
