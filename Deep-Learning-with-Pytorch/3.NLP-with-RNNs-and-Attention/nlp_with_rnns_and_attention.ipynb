{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Generating Fake Shakespeare Text","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.706368Z","iopub.execute_input":"2025-09-16T07:38:40.707174Z","iopub.status.idle":"2025-09-16T07:38:40.710676Z","shell.execute_reply.started":"2025-09-16T07:38:40.707147Z","shell.execute_reply":"2025-09-16T07:38:40.709867Z"},"trusted":true},"outputs":[],"execution_count":117},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(torch.cuda.device_count())\n    device = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.712122Z","iopub.execute_input":"2025-09-16T07:38:40.712389Z","iopub.status.idle":"2025-09-16T07:38:40.732104Z","shell.execute_reply.started":"2025-09-16T07:38:40.712366Z","shell.execute_reply":"2025-09-16T07:38:40.731458Z"},"trusted":true},"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"},{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":118},{"cell_type":"code","source":"from pathlib import Path\nimport urllib.request\n\ndef download_shakespeare_text():\n    path = Path(\"datasets/shakespeare/shakespeare.txt\")\n    if not path.is_file():\n        path.parent.mkdir(parents=True, exist_ok=True)\n        url = \"https://homl.info/shakespeare\"\n        urllib.request.urlretrieve(url, path)\n    return path.read_text()\nshakespeare_text = download_shakespeare_text()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.732954Z","iopub.execute_input":"2025-09-16T07:38:40.733635Z","iopub.status.idle":"2025-09-16T07:38:40.750220Z","shell.execute_reply.started":"2025-09-16T07:38:40.733609Z","shell.execute_reply":"2025-09-16T07:38:40.749706Z"},"trusted":true},"outputs":[],"execution_count":119},{"cell_type":"code","source":"print(shakespeare_text[:100])","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.790260Z","iopub.execute_input":"2025-09-16T07:38:40.791035Z","iopub.status.idle":"2025-09-16T07:38:40.795158Z","shell.execute_reply.started":"2025-09-16T07:38:40.791016Z","shell.execute_reply":"2025-09-16T07:38:40.794210Z"},"trusted":true},"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"vocab = sorted(set(shakespeare_text.lower()))\n''.join(vocab)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.796349Z","iopub.execute_input":"2025-09-16T07:38:40.796635Z","iopub.status.idle":"2025-09-16T07:38:40.823013Z","shell.execute_reply.started":"2025-09-16T07:38:40.796619Z","shell.execute_reply":"2025-09-16T07:38:40.822258Z"},"trusted":true},"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""},"metadata":{}}],"execution_count":121},{"cell_type":"code","source":"char_to_id = {char:idx for idx, char in enumerate(vocab)}\nid_to_char = {idx:char for idx, char in enumerate(vocab)}","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.823817Z","iopub.execute_input":"2025-09-16T07:38:40.824059Z","iopub.status.idle":"2025-09-16T07:38:40.840296Z","shell.execute_reply.started":"2025-09-16T07:38:40.824031Z","shell.execute_reply":"2025-09-16T07:38:40.839788Z"},"trusted":true},"outputs":[],"execution_count":122},{"cell_type":"code","source":"char_to_id[\"a\"]","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.841618Z","iopub.execute_input":"2025-09-16T07:38:40.841849Z","iopub.status.idle":"2025-09-16T07:38:40.859616Z","shell.execute_reply.started":"2025-09-16T07:38:40.841831Z","shell.execute_reply":"2025-09-16T07:38:40.858847Z"},"trusted":true},"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"13"},"metadata":{}}],"execution_count":123},{"cell_type":"code","source":"id_to_char[13]","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.860379Z","iopub.execute_input":"2025-09-16T07:38:40.860629Z","iopub.status.idle":"2025-09-16T07:38:40.877444Z","shell.execute_reply.started":"2025-09-16T07:38:40.860608Z","shell.execute_reply":"2025-09-16T07:38:40.876659Z"},"trusted":true},"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"'a'"},"metadata":{}}],"execution_count":124},{"cell_type":"code","source":"def encode_text(text):\n    return torch.tensor([char_to_id[char] for char in text.lower()])\ndef decode_text(char_ids):\n    return ''.join([id_to_char[char_id.item()] for char_id in char_ids])","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.879038Z","iopub.execute_input":"2025-09-16T07:38:40.879676Z","iopub.status.idle":"2025-09-16T07:38:40.895404Z","shell.execute_reply.started":"2025-09-16T07:38:40.879652Z","shell.execute_reply":"2025-09-16T07:38:40.894595Z"},"trusted":true},"outputs":[],"execution_count":125},{"cell_type":"code","source":"encoded = encode_text(\"hello world\")\nencoded\n","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.896247Z","iopub.execute_input":"2025-09-16T07:38:40.896946Z","iopub.status.idle":"2025-09-16T07:38:40.914834Z","shell.execute_reply.started":"2025-09-16T07:38:40.896923Z","shell.execute_reply":"2025-09-16T07:38:40.914222Z"},"trusted":true},"outputs":[{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"tensor([20, 17, 24, 24, 27,  1, 35, 27, 30, 24, 16])"},"metadata":{}}],"execution_count":126},{"cell_type":"code","source":"decode_text(encoded)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.915562Z","iopub.execute_input":"2025-09-16T07:38:40.915808Z","iopub.status.idle":"2025-09-16T07:38:40.931272Z","shell.execute_reply.started":"2025-09-16T07:38:40.915786Z","shell.execute_reply":"2025-09-16T07:38:40.930694Z"},"trusted":true},"outputs":[{"execution_count":127,"output_type":"execute_result","data":{"text/plain":"'hello world'"},"metadata":{}}],"execution_count":127},{"cell_type":"code","source":"class TimeSeriesDatasetBuilder:\n    def __init__(self, series, window_length=56):\n        self.encoded_text = encode_text(series)\n        self.window_length = window_length\n   \n    def create_X_y(self):\n        X, y =[],[]\n        for i in range(len(self.encoded_text) - self.window_length):\n            window = self.encoded_text[i:i+self.window_length]\n            future = self.encoded_text[i+1:i+self.window_length+1]\n            X.append(window)\n            y.append(future)\n        return np.array(X),np.array(y)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.931890Z","iopub.execute_input":"2025-09-16T07:38:40.932050Z","iopub.status.idle":"2025-09-16T07:38:40.946173Z","shell.execute_reply.started":"2025-09-16T07:38:40.932036Z","shell.execute_reply":"2025-09-16T07:38:40.945451Z"},"trusted":true},"outputs":[],"execution_count":128},{"cell_type":"code","source":"to_be_dataset = TimeSeriesDatasetBuilder(series=\"to be or not to be\", window_length=10)\nX,y = to_be_dataset.create_X_y()\nfor i in range(len(X)):\n    decoded_x = decode_text(X[i])\n    decoded_y = decode_text(y[i])\n    print(f\"x : {decoded_x}\")\n    print(f\"y : {decoded_y}\")","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.947972Z","iopub.execute_input":"2025-09-16T07:38:40.948213Z","iopub.status.idle":"2025-09-16T07:38:40.972115Z","shell.execute_reply.started":"2025-09-16T07:38:40.948199Z","shell.execute_reply":"2025-09-16T07:38:40.971384Z"},"trusted":true},"outputs":[{"name":"stdout","text":"x : to be or n\ny : o be or no\nx : o be or no\ny :  be or not\nx :  be or not\ny : be or not \nx : be or not \ny : e or not t\nx : e or not t\ny :  or not to\nx :  or not to\ny : or not to \nx : or not to \ny : r not to b\nx : r not to b\ny :  not to be\n","output_type":"stream"}],"execution_count":129},{"cell_type":"code","source":"window_length = 56\nbatch_size = 1024 \nbuilder = TimeSeriesDatasetBuilder(shakespeare_text,window_length)\nX, y = builder.create_X_y()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:38:40.973042Z","iopub.execute_input":"2025-09-16T07:38:40.973221Z","iopub.status.idle":"2025-09-16T07:39:01.732338Z","shell.execute_reply.started":"2025-09-16T07:38:40.973207Z","shell.execute_reply":"2025-09-16T07:39:01.731529Z"},"trusted":true},"outputs":[],"execution_count":130},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:39:01.733290Z","iopub.execute_input":"2025-09-16T07:39:01.733532Z","iopub.status.idle":"2025-09-16T07:39:01.738201Z","shell.execute_reply.started":"2025-09-16T07:39:01.733513Z","shell.execute_reply":"2025-09-16T07:39:01.737549Z"}},"outputs":[{"execution_count":131,"output_type":"execute_result","data":{"text/plain":"(1115338, 56)"},"metadata":{}}],"execution_count":131},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:39:01.738921Z","iopub.execute_input":"2025-09-16T07:39:01.739222Z","iopub.status.idle":"2025-09-16T07:39:01.758809Z","shell.execute_reply.started":"2025-09-16T07:39:01.739203Z","shell.execute_reply":"2025-09-16T07:39:01.758075Z"}},"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"(1115338, 56)"},"metadata":{}}],"execution_count":132},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\nX_tensor = torch.tensor(X, dtype = torch.long)\ny_tensor = torch.tensor(y, dtype = torch.long)\n\ntrain_set = TensorDataset(X_tensor[:1_000_000], y_tensor[:1_000_000])\nvalid_set = TensorDataset(X_tensor[1_000_000:1_060_000],y_tensor[1_000_000:1_060_000])\ntest_set = TensorDataset(X_tensor[1_060_000:], y_tensor[1_060_000:])\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, \n                          num_workers=2, pin_memory=True)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size,\n                         num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_set, batch_size=batch_size,\n                        num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:39:01.759437Z","iopub.execute_input":"2025-09-16T07:39:01.759727Z","iopub.status.idle":"2025-09-16T07:39:02.174972Z","shell.execute_reply.started":"2025-09-16T07:39:01.759700Z","shell.execute_reply":"2025-09-16T07:39:02.174312Z"},"trusted":true},"outputs":[],"execution_count":133},{"cell_type":"code","source":"import torchmetrics\n\ndef evaluate_tm(model, data_loader, metric):\n    model.eval()\n    metric.reset()\n    with torch.no_grad():\n        for X_batch, y_batch in data_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_pred = model(X_batch)\n            metric.update(y_pred, y_batch)\n    return metric.compute()\n\ndef train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs, patience=2,\n         factor=0.5,epoch_callback=None):\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"max\", patience=patience, factor=factor\n    )\n    history = {\"train_losses\":[],\"train_metrics\":[],\"valid_metrics\":[]}\n    for epoch in range(n_epochs):\n        total_loss = 0\n        metric.reset()\n        model.train()\n        if epoch_callback is not None:\n            epoch_callback(model,epoch)\n        for idx,( X_batch, y_batch) in enumerate(train_loader):\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_pred = model(X_batch)\n            loss = criterion(y_pred, y_batch)\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            metric.update(y_pred, y_batch)\n            print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n            print(f\", loss ={total_loss/(idx+1 ):.4f} \", end=\"\")\n        mean_loss = total_loss / len(train_loader)\n        history[\"train_losses\"].append(mean_loss)\n        history[\"train_metrics\"].append(metric.compute().item())\n        val_metric = evaluate_tm(model, valid_loader, metric).item()\n        history[\"valid_metrics\"].append(val_metric)\n        scheduler.step(val_metric)\n        print(f\"Epoch:{epoch+1}/{n_epochs}, \"\n             f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n             f\"Train Metric: {history['train_metrics'][-1]:.4f}, \"\n             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}\")\n    return history","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:39:02.177695Z","iopub.execute_input":"2025-09-16T07:39:02.177919Z","iopub.status.idle":"2025-09-16T07:39:02.186145Z","shell.execute_reply.started":"2025-09-16T07:39:02.177902Z","shell.execute_reply":"2025-09-16T07:39:02.185395Z"},"trusted":true},"outputs":[],"execution_count":134},{"cell_type":"code","source":"class ShakespeareModel(nn.Module):\n    def __init__(self, vocab_size, n_hidden=128, n_layers=2, embed_size=20, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.gru = nn.GRU(embed_size, n_hidden, num_layers=n_layers,\n                         batch_first=True, dropout=dropout)\n        self.output = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, X):\n        embeddings = self.embed(X)\n        outputs, _states = self.gru(embeddings)\n        return self.output(outputs).permute(0,2,1)\nmodel = ShakespeareModel(len(vocab)).to(device)\nif torch.cuda.device_count()>1:\n    print(\"Using\", torch.cuda.device_count(),\"GPU's\")\n    model = nn.DataParallel(model)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:39:02.187039Z","iopub.execute_input":"2025-09-16T07:39:02.187268Z","iopub.status.idle":"2025-09-16T07:39:02.214071Z","shell.execute_reply.started":"2025-09-16T07:39:02.187244Z","shell.execute_reply":"2025-09-16T07:39:02.213447Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using 2 GPU's\n","output_type":"stream"},{"execution_count":135,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): ShakespeareModel(\n    (embed): Embedding(39, 20)\n    (gru): GRU(20, 128, num_layers=2, batch_first=True, dropout=0.1)\n    (output): Linear(in_features=128, out_features=39, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":135},{"cell_type":"code","source":"n_epochs = 20\nxentropy = nn.CrossEntropyLoss()\naccuracy = torchmetrics.Accuracy(task=\"multiclass\",num_classes=len(vocab)).to(device)\noptimizer = torch.optim.NAdam(model.parameters())\n\nhistory = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader, n_epochs)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:39:02.214694Z","iopub.execute_input":"2025-09-16T07:39:02.214881Z","iopub.status.idle":"2025-09-16T07:46:06.517549Z","shell.execute_reply.started":"2025-09-16T07:39:02.214867Z","shell.execute_reply":"2025-09-16T07:46:06.516777Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Batch 977/977, loss =1.7116 Epoch:1/20, Train Loss: 1.7116, Train Metric: 0.4857, Valid Metric: 0.5305\nBatch 977/977, loss =1.4045 Epoch:2/20, Train Loss: 1.4045, Train Metric: 0.5626, Valid Metric: 0.5457\nBatch 977/977, loss =1.3578 Epoch:3/20, Train Loss: 1.3578, Train Metric: 0.5743, Valid Metric: 0.5510\nBatch 977/977, loss =1.3354 Epoch:4/20, Train Loss: 1.3354, Train Metric: 0.5800, Valid Metric: 0.5506\nBatch 977/977, loss =1.3225 Epoch:5/20, Train Loss: 1.3225, Train Metric: 0.5832, Valid Metric: 0.5515\nBatch 977/977, loss =1.3136 Epoch:6/20, Train Loss: 1.3136, Train Metric: 0.5856, Valid Metric: 0.5525\nBatch 977/977, loss =1.3075 Epoch:7/20, Train Loss: 1.3075, Train Metric: 0.5872, Valid Metric: 0.5521\nBatch 977/977, loss =1.3029 Epoch:8/20, Train Loss: 1.3029, Train Metric: 0.5885, Valid Metric: 0.5519\nBatch 977/977, loss =1.2993 Epoch:9/20, Train Loss: 1.2993, Train Metric: 0.5895, Valid Metric: 0.5534\nBatch 977/977, loss =1.2965 Epoch:10/20, Train Loss: 1.2965, Train Metric: 0.5903, Valid Metric: 0.5516\nBatch 977/977, loss =1.2941 Epoch:11/20, Train Loss: 1.2941, Train Metric: 0.5909, Valid Metric: 0.5537\nBatch 977/977, loss =1.2923 Epoch:12/20, Train Loss: 1.2923, Train Metric: 0.5914, Valid Metric: 0.5533\nBatch 977/977, loss =1.2908 Epoch:13/20, Train Loss: 1.2908, Train Metric: 0.5918, Valid Metric: 0.5532\nBatch 977/977, loss =1.2894 Epoch:14/20, Train Loss: 1.2894, Train Metric: 0.5922, Valid Metric: 0.5529\nBatch 977/977, loss =1.2830 Epoch:15/20, Train Loss: 1.2830, Train Metric: 0.5941, Valid Metric: 0.5538\nBatch 977/977, loss =1.2820 Epoch:16/20, Train Loss: 1.2820, Train Metric: 0.5944, Valid Metric: 0.5540\nBatch 977/977, loss =1.2813 Epoch:17/20, Train Loss: 1.2813, Train Metric: 0.5945, Valid Metric: 0.5527\nBatch 977/977, loss =1.2807 Epoch:18/20, Train Loss: 1.2807, Train Metric: 0.5947, Valid Metric: 0.5539\nBatch 977/977, loss =1.2802 Epoch:19/20, Train Loss: 1.2802, Train Metric: 0.5948, Valid Metric: 0.5529\nBatch 977/977, loss =1.2766 Epoch:20/20, Train Loss: 1.2766, Train Metric: 0.5959, Valid Metric: 0.5527\n","output_type":"stream"}],"execution_count":136},{"cell_type":"code","source":"torch.save(model.state_dict(), \"my_shakespeare_model.pt\")","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:46:06.523923Z","iopub.execute_input":"2025-09-16T07:46:06.524143Z","iopub.status.idle":"2025-09-16T07:46:06.547595Z","shell.execute_reply.started":"2025-09-16T07:46:06.524118Z","shell.execute_reply":"2025-09-16T07:46:06.546832Z"},"trusted":true},"outputs":[],"execution_count":138},{"cell_type":"code","source":"text = \"To be or not to b\"\nencoded_text = encode_text(text).unsqueeze(dim=0).to(device)\nencoded_text.shape","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:46:06.548412Z","iopub.execute_input":"2025-09-16T07:46:06.548669Z","iopub.status.idle":"2025-09-16T07:46:06.563107Z","shell.execute_reply.started":"2025-09-16T07:46:06.548646Z","shell.execute_reply":"2025-09-16T07:46:06.562601Z"},"trusted":true},"outputs":[{"execution_count":139,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 17])"},"metadata":{}}],"execution_count":139},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    y_logits = model(encoded_text)\n    predicted_char_id = y_logits[0,:,-1].argmax().item()\n    predicted_char = id_to_char[predicted_char_id]\npredicted_char","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:46:06.563856Z","iopub.execute_input":"2025-09-16T07:46:06.564111Z","iopub.status.idle":"2025-09-16T07:46:06.584154Z","shell.execute_reply.started":"2025-09-16T07:46:06.564088Z","shell.execute_reply":"2025-09-16T07:46:06.583528Z"},"trusted":true},"outputs":[{"execution_count":140,"output_type":"execute_result","data":{"text/plain":"'e'"},"metadata":{}}],"execution_count":140},{"cell_type":"code","source":"def next_char(model, text, temperature=0.7):\n    model.eval()\n    encoded_text = encode_text(text).unsqueeze(0).to(device)\n    with torch.no_grad():\n        y_logits = model(encoded_text)\n        y_probas = torch.softmax(y_logits[0,:,-1]/temperature, dim=-1)\n        predicted_char_id = torch.multinomial(y_probas,num_samples=1).item()     \n        return id_to_char[predicted_char_id]\n\n","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:46:06.584949Z","iopub.execute_input":"2025-09-16T07:46:06.585505Z","iopub.status.idle":"2025-09-16T07:46:06.597290Z","shell.execute_reply.started":"2025-09-16T07:46:06.585464Z","shell.execute_reply":"2025-09-16T07:46:06.596662Z"},"trusted":true},"outputs":[],"execution_count":141},{"cell_type":"code","source":"import time\ndef generate_text(model, text, n_chars=100,temperature=0.7):\n    print(text, end='', flush=True)\n    for _ in range(n_chars):\n        char = next_char(model, text, temperature)\n        text += char\n        print(char, end='', flush=True)\n        time.sleep(0.01)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:46:06.597990Z","iopub.execute_input":"2025-09-16T07:46:06.598528Z","iopub.status.idle":"2025-09-16T07:46:06.622066Z","shell.execute_reply.started":"2025-09-16T07:46:06.598505Z","shell.execute_reply":"2025-09-16T07:46:06.621525Z"},"trusted":true},"outputs":[],"execution_count":142},{"cell_type":"code","source":"print(generate_text(model,\"To be or not to b\",n_chars=500 ))","metadata":{"execution":{"iopub.status.busy":"2025-09-16T07:46:06.622704Z","iopub.execute_input":"2025-09-16T07:46:06.622886Z","iopub.status.idle":"2025-09-16T07:46:13.238805Z","shell.execute_reply.started":"2025-09-16T07:46:06.622872Z","shell.execute_reply":"2025-09-16T07:46:13.238013Z"},"trusted":true},"outputs":[{"name":"stdout","text":"To be or not to be so?\n\nclarence:\npeace! great entertunes that he is tom the time.\n\nking richard iii:\nay, stand your losses did take a foul of the ground\nthe prince thou hast subder'd that you shall be whom here come\nthat they have suit of all this thoughts, sir,\nand give groad for a good fairly will not prove me to come:\ndown their sovereign, for my great a prophet of the throw your beggar of mine event\nboth with him home, but i be heart.\n\nisabella:\nare you comes our kingdom in henry,\nthat he hath lambs, that wNone\n","output_type":"stream"}],"execution_count":143},{"cell_type":"markdown","source":"## Statefull RNN","metadata":{}},{"cell_type":"code","source":"class DatasetBuilderStateful:\n    def __init__(self, series,batch_size, window_length=56):\n        self.encoded_text = encode_text(series)\n        self.window_length = window_length\n        self.batch_size = batch_size\n\n        #total number of full windows\n        self.n_consecutive_windows = (len(self.encoded_text) - 1) // self.window_length\n        #windows per stream\n        self.n_windows_per_stream = self.n_consecutive_windows // self.batch_size\n        #spacing between streams\n        self.spacing = self.n_windows_per_stream * self.window_length\n        #Total samples\n        self.length = self.n_windows_per_stream * self.batch_size\n                \n    def create_X_y(self):\n        X, y =[],[]\n        for i in range(self.length):\n            slot = i % self.batch_size  # stream index\n            window_no_in_slot = i // self.batch_size\n\n            start = slot * self.spacing + window_no_in_slot * self.window_length\n            end = start + self.window_length\n            window = self.encoded_text[start:end]\n            future = self.encoded_text[start+1:end+1]\n            X.append(window)\n            y.append(future)\n        return np.array(X),np.array(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:46:13.239745Z","iopub.execute_input":"2025-09-16T07:46:13.240009Z","iopub.status.idle":"2025-09-16T07:46:13.246262Z","shell.execute_reply.started":"2025-09-16T07:46:13.239987Z","shell.execute_reply":"2025-09-16T07:46:13.245540Z"}},"outputs":[],"execution_count":144},{"cell_type":"code","source":"batch_size =1024\n\ntext_len = len(shakespeare_text)\n\ntrain_text = shakespeare_text[:int(0.9 * text_len)]\nvalid_text = shakespeare_text[int(0.9 * text_len):int(text_len)]\ntrain_builder = DatasetBuilderStateful(train_text, batch_size=batch_size)\nvalid_builder = DatasetBuilderStateful(valid_text, batch_size=batch_size)\n\nX_train, y_train = train_builder.create_X_y()\nX_valid, y_valid = valid_builder.create_X_y()\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.long)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_valid_tensor = torch.tensor(X_valid, dtype=torch.long)\ny_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n\ntrain_set = TensorDataset(X_train_tensor, y_train_tensor)\nvalid_set = TensorDataset(X_valid_tensor, y_valid_tensor)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size,\n                         num_workers=2,pin_memory=True,drop_last=True)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size,\n                         num_workers=2,pin_memory=True,drop_last=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:46:13.247015Z","iopub.execute_input":"2025-09-16T07:46:13.247188Z","iopub.status.idle":"2025-09-16T07:46:13.731539Z","shell.execute_reply.started":"2025-09-16T07:46:13.247174Z","shell.execute_reply":"2025-09-16T07:46:13.730927Z"}},"outputs":[],"execution_count":145},{"cell_type":"code","source":"class StatefulShakespeareModel(nn.Module):\n    def __init__(self, vocab_size, n_layers=2, embed_size=20, n_hidden=128, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.gru = nn.GRU(embed_size, n_hidden, num_layers=n_layers,batch_first=True,\n                         dropout=dropout)\n        self.output = nn.Linear(n_hidden,vocab_size)\n        self.hidden_states = None\n\n    def forward(self,X):\n        embeddings = self.embed(X)\n        outputs, hidden_states = self.gru(embeddings,self.hidden_states)\n        self.hidden_states = hidden_states.detach()\n        return self.output(outputs).permute(0,2,1)        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:46:13.732258Z","iopub.execute_input":"2025-09-16T07:46:13.732464Z","iopub.status.idle":"2025-09-16T07:46:13.738143Z","shell.execute_reply.started":"2025-09-16T07:46:13.732432Z","shell.execute_reply":"2025-09-16T07:46:13.737547Z"}},"outputs":[],"execution_count":146},{"cell_type":"code","source":"stateful_model = StatefulShakespeareModel(len(vocab)).to(device)\nn_epochs = 20\n\ndef reset_hidden_state(model,epoch):\n    model.hidden_states = None\n\nxentropy = nn.CrossEntropyLoss()\naccuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(vocab)).to(device)\noptimizer = torch.optim.NAdam(stateful_model.parameters())\n\nhistory = train(stateful_model, optimizer, xentropy, accuracy, train_loader, \n                valid_loader,n_epochs, epoch_callback=reset_hidden_state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:46:13.740881Z","iopub.execute_input":"2025-09-16T07:46:13.741143Z","iopub.status.idle":"2025-09-16T07:46:33.025404Z","shell.execute_reply.started":"2025-09-16T07:46:13.741116Z","shell.execute_reply":"2025-09-16T07:46:33.024680Z"}},"outputs":[{"name":"stdout","text":"Batch 17/17, loss =3.1882 Epoch:1/20, Train Loss: 3.1882, Train Metric: 0.1431, Valid Metric: 0.1607\nBatch 17/17, loss =2.8370 Epoch:2/20, Train Loss: 2.8370, Train Metric: 0.2108, Valid Metric: 0.2317\nBatch 17/17, loss =2.5781 Epoch:3/20, Train Loss: 2.5781, Train Metric: 0.2677, Valid Metric: 0.2864\nBatch 17/17, loss =2.3991 Epoch:4/20, Train Loss: 2.3991, Train Metric: 0.3127, Valid Metric: 0.3218\nBatch 17/17, loss =2.2745 Epoch:5/20, Train Loss: 2.2745, Train Metric: 0.3461, Valid Metric: 0.3476\nBatch 17/17, loss =2.1738 Epoch:6/20, Train Loss: 2.1738, Train Metric: 0.3686, Valid Metric: 0.3602\nBatch 17/17, loss =2.0922 Epoch:7/20, Train Loss: 2.0922, Train Metric: 0.3861, Valid Metric: 0.3791\nBatch 17/17, loss =2.0249 Epoch:8/20, Train Loss: 2.0249, Train Metric: 0.4023, Valid Metric: 0.3920\nBatch 17/17, loss =1.9575 Epoch:9/20, Train Loss: 1.9575, Train Metric: 0.4205, Valid Metric: 0.4039\nBatch 17/17, loss =1.9137 Epoch:10/20, Train Loss: 1.9137, Train Metric: 0.4320, Valid Metric: 0.4125\nBatch 17/17, loss =1.8603 Epoch:11/20, Train Loss: 1.8603, Train Metric: 0.4462, Valid Metric: 0.4212\nBatch 17/17, loss =1.8241 Epoch:12/20, Train Loss: 1.8241, Train Metric: 0.4558, Valid Metric: 0.4270\nBatch 17/17, loss =1.7880 Epoch:13/20, Train Loss: 1.7880, Train Metric: 0.4656, Valid Metric: 0.4310\nBatch 17/17, loss =1.7550 Epoch:14/20, Train Loss: 1.7550, Train Metric: 0.4751, Valid Metric: 0.4383\nBatch 17/17, loss =1.7243 Epoch:15/20, Train Loss: 1.7243, Train Metric: 0.4829, Valid Metric: 0.4391\nBatch 17/17, loss =1.6996 Epoch:16/20, Train Loss: 1.6996, Train Metric: 0.4898, Valid Metric: 0.4449\nBatch 17/17, loss =1.6805 Epoch:17/20, Train Loss: 1.6805, Train Metric: 0.4941, Valid Metric: 0.4505\nBatch 17/17, loss =1.6511 Epoch:18/20, Train Loss: 1.6511, Train Metric: 0.5025, Valid Metric: 0.4531\nBatch 17/17, loss =1.6321 Epoch:19/20, Train Loss: 1.6321, Train Metric: 0.5073, Valid Metric: 0.4543\nBatch 17/17, loss =1.6170 Epoch:20/20, Train Loss: 1.6170, Train Metric: 0.5114, Valid Metric: 0.4589\n","output_type":"stream"}],"execution_count":147},{"cell_type":"code","source":"\ndef next_char(model, text, temperature):\n    model.eval()\n    encoded_text = encode_text(text).unsqueeze(0).to(device)\n    with torch.no_grad():\n        y_logits = model(encoded_text)\n        y_probas = torch.softmax(y_logits[0,:,-1]/temperature, dim=-1)\n        predicted_char_id = torch.multinomial(y_probas,num_samples=1).item()     \n        return id_to_char[predicted_char_id]\n\n\ndef generate_text(model, text, n_chars=100,temperature=0.9): \n    print(text, end='', flush=True)\n    model.hidden_states = None\n    for _ in range(n_chars):\n        char = next_char(model, text, temperature)\n        text += char\n        print(char, end='', flush=True)\n        time.sleep(0.01)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:48:44.259644Z","iopub.execute_input":"2025-09-16T07:48:44.260013Z","iopub.status.idle":"2025-09-16T07:48:44.266415Z","shell.execute_reply.started":"2025-09-16T07:48:44.259990Z","shell.execute_reply":"2025-09-16T07:48:44.265452Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"print(generate_text(stateful_model,\"To be or not to b\",n_chars=500 ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:48:46.534707Z","iopub.execute_input":"2025-09-16T07:48:46.535306Z","iopub.status.idle":"2025-09-16T07:48:52.902010Z","shell.execute_reply.started":"2025-09-16T07:48:46.535281Z","shell.execute_reply":"2025-09-16T07:48:52.901231Z"}},"outputs":[{"name":"stdout","text":"To be or not to be what.\ni'll proseding\nof being it is was live.\n\naulow:\nheaven thee the graves, be heaven to\nhe, where reprothan be to make us never that a time.\nsir, i can you, and i fall wast i worth fare troth, my secust betul, to not make accomber'd, sis they lies if refore and warwick?\nbut a lay's man the pitis.\n\nabear:\nwhither up, steep my soundess were thy pobpatest that, choldisholt. and wamather dase. attends as it what i'll grare a murders-on!\n\nlady:\nsay you, marrier more\nof what rise moot thine, and None\n","output_type":"stream"}],"execution_count":151}]}