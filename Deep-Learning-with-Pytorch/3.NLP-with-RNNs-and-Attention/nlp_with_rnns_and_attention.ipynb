{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:11.751067Z","iopub.execute_input":"2025-09-15T07:15:11.751348Z","iopub.status.idle":"2025-09-15T07:15:19.303117Z","shell.execute_reply.started":"2025-09-15T07:15:11.751321Z","shell.execute_reply":"2025-09-15T07:15:19.302445Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(torch.cuda.device_count())\n    device = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:19.304364Z","iopub.execute_input":"2025-09-15T07:15:19.304728Z","iopub.status.idle":"2025-09-15T07:15:19.420746Z","shell.execute_reply.started":"2025-09-15T07:15:19.304700Z","shell.execute_reply":"2025-09-15T07:15:19.420201Z"}},"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from pathlib import Path\nimport urllib.request\n\ndef download_shakespeare_text():\n    path = Path(\"datasets/shakespeare/shakespeare.txt\")\n    if not path.is_file():\n        path.parent.mkdir(parents=True, exist_ok=True)\n        url = \"https://homl.info/shakespeare\"\n        urllib.request.urlretrieve(url, path)\n    return path.read_text()\nshakespeare_text = download_shakespeare_text()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:19.421324Z","iopub.execute_input":"2025-09-15T07:15:19.421513Z","iopub.status.idle":"2025-09-15T07:15:20.167995Z","shell.execute_reply.started":"2025-09-15T07:15:19.421497Z","shell.execute_reply":"2025-09-15T07:15:20.167133Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(shakespeare_text[:100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:20.169757Z","iopub.execute_input":"2025-09-15T07:15:20.169995Z","iopub.status.idle":"2025-09-15T07:15:20.174082Z","shell.execute_reply.started":"2025-09-15T07:15:20.169977Z","shell.execute_reply":"2025-09-15T07:15:20.173274Z"}},"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"vocab = sorted(set(shakespeare_text.lower()))\n''.join(vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:20.175034Z","iopub.execute_input":"2025-09-15T07:15:20.175290Z","iopub.status.idle":"2025-09-15T07:15:20.202637Z","shell.execute_reply.started":"2025-09-15T07:15:20.175273Z","shell.execute_reply":"2025-09-15T07:15:20.202037Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"char_to_id = {char:idx for idx, char in enumerate(vocab)}\nid_to_char = {idx:char for idx, char in enumerate(vocab)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:20.203270Z","iopub.execute_input":"2025-09-15T07:15:20.203504Z","iopub.status.idle":"2025-09-15T07:15:20.218136Z","shell.execute_reply.started":"2025-09-15T07:15:20.203483Z","shell.execute_reply":"2025-09-15T07:15:20.217570Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"char_to_id[\"a\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:20.218723Z","iopub.execute_input":"2025-09-15T07:15:20.218958Z","iopub.status.idle":"2025-09-15T07:15:20.237547Z","shell.execute_reply.started":"2025-09-15T07:15:20.218942Z","shell.execute_reply":"2025-09-15T07:15:20.237023Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"13"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"id_to_char[13]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:20.238179Z","iopub.execute_input":"2025-09-15T07:15:20.238484Z","iopub.status.idle":"2025-09-15T07:15:20.254853Z","shell.execute_reply.started":"2025-09-15T07:15:20.238459Z","shell.execute_reply":"2025-09-15T07:15:20.254253Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'a'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def encode_text(text):\n    return torch.tensor([char_to_id[char] for char in text.lower()])\ndef decode_text(char_ids):\n    return ''.join([id_to_char[char_id.item()] for char_id in char_ids])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:20.255417Z","iopub.execute_input":"2025-09-15T07:15:20.255608Z","iopub.status.idle":"2025-09-15T07:15:20.269192Z","shell.execute_reply.started":"2025-09-15T07:15:20.255594Z","shell.execute_reply":"2025-09-15T07:15:20.268476Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"encoded = encode_text(\"hello world\")\nencoded\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:20.271250Z","iopub.execute_input":"2025-09-15T07:15:20.271929Z","iopub.status.idle":"2025-09-15T07:15:20.339180Z","shell.execute_reply.started":"2025-09-15T07:15:20.271909Z","shell.execute_reply":"2025-09-15T07:15:20.338624Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([20, 17, 24, 24, 27,  1, 35, 27, 30, 24, 16])"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"decode_text(encoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:20.339794Z","iopub.execute_input":"2025-09-15T07:15:20.340053Z","iopub.status.idle":"2025-09-15T07:15:20.345278Z","shell.execute_reply.started":"2025-09-15T07:15:20.340037Z","shell.execute_reply":"2025-09-15T07:15:20.344655Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'hello world'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"class TimeSeriesDatasetBuilder:\n    def __init__(self, series, window_length=56, horizon = 1):\n        self.encoded_text = encode_text(series)\n        self.window_length = window_length\n        self.horizon = horizon\n    def create_X_y(self):\n        X, y =[],[]\n        for i in range(len(self.encoded_text) - self.window_length):\n            window = self.encoded_text[i:i+self.window_length]\n            future = self.encoded_text[i+self.window_length:i+self.window_length+self.horizon]\n            X.append(window)\n            y.append(future)\n        return np.array(X),np.array(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:20.345982Z","iopub.execute_input":"2025-09-15T07:15:20.346243Z","iopub.status.idle":"2025-09-15T07:15:20.360841Z","shell.execute_reply.started":"2025-09-15T07:15:20.346218Z","shell.execute_reply":"2025-09-15T07:15:20.360357Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"to_be_dataset = TimeSeriesDatasetBuilder(series=\"to be or not to be\", window_length=10)\nX,y = to_be_dataset.create_X_y()\nfor i in range(len(X)):\n    decoded_x = decode_text(X[i])\n    decoded_y = decode_text(y[i])\n    print(f\"x : {decoded_x}\")\n    print(f\"y : {decoded_y}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:20.361376Z","iopub.execute_input":"2025-09-15T07:15:20.361537Z","iopub.status.idle":"2025-09-15T07:15:20.392549Z","shell.execute_reply.started":"2025-09-15T07:15:20.361516Z","shell.execute_reply":"2025-09-15T07:15:20.391981Z"}},"outputs":[{"name":"stdout","text":"x : to be or n\ny : o\nx : o be or no\ny : t\nx :  be or not\ny :  \nx : be or not \ny : t\nx : e or not t\ny : o\nx :  or not to\ny :  \nx : or not to \ny : b\nx : r not to b\ny : e\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"window_length = 56\nbatch_size = 1024 \nbuilder = TimeSeriesDatasetBuilder(shakespeare_text,window_length)\nX, y = builder.create_X_y()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:20.393287Z","iopub.execute_input":"2025-09-15T07:15:20.393475Z","iopub.status.idle":"2025-09-15T07:15:39.503775Z","shell.execute_reply.started":"2025-09-15T07:15:20.393461Z","shell.execute_reply":"2025-09-15T07:15:39.503020Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:39.504642Z","iopub.execute_input":"2025-09-15T07:15:39.504917Z","iopub.status.idle":"2025-09-15T07:15:39.510409Z","shell.execute_reply.started":"2025-09-15T07:15:39.504895Z","shell.execute_reply":"2025-09-15T07:15:39.509680Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array([[18, 21, 30, ...,  1, 31, 28],\n       [21, 30, 31, ..., 31, 28, 17],\n       [30, 31, 32, ..., 28, 17, 13],\n       ...,\n       [27, 30, 32, ..., 23, 21, 26],\n       [30, 32, 33, ..., 21, 26, 19],\n       [32, 33, 26, ..., 26, 19,  8]])"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\nX_tensor = torch.tensor(X, dtype = torch.long)\ny_tensor = torch.tensor(y, dtype = torch.long).squeeze(-1)\n\ntrain_set = TensorDataset(X_tensor[:1_000_000], y_tensor[:1_000_000])\nvalid_set = TensorDataset(X_tensor[1_000_000:1_060_000],y_tensor[1_000_000:1_060_000])\ntest_set = TensorDataset(X_tensor[1_060_000:], y_tensor[1_060_000:])\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, \n                          num_workers=2, pin_memory=True)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size,\n                         num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_set, batch_size=batch_size,\n                        num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:39.511164Z","iopub.execute_input":"2025-09-15T07:15:39.511367Z","iopub.status.idle":"2025-09-15T07:15:39.596363Z","shell.execute_reply.started":"2025-09-15T07:15:39.511351Z","shell.execute_reply":"2025-09-15T07:15:39.595768Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torchmetrics\n\ndef evaluate_tm(model, data_loader, metric):\n    model.eval()\n    metric.reset()\n    with torch.no_grad():\n        for X_batch, y_batch in data_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_pred = model(X_batch)\n            metric.update(y_pred, y_batch)\n    return metric.compute()\n\ndef train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs, patience=2,\n         factor=0.5):\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"max\", patience=patience, factor=factor\n    )\n    history = {\"train_losses\":[],\"train_metrics\":[],\"valid_metrics\":[]}\n    for epoch in range(n_epochs):\n        total_loss = 0\n        metric.reset()\n        model.train()\n        for idx,( X_batch, y_batch) in enumerate(train_loader):\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_pred = model(X_batch)\n            loss = criterion(y_pred, y_batch)\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            metric.update(y_pred, y_batch)\n            print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n            print(f\", loss ={total_loss/(idx+1 ):.4f}\", end=\"\")\n        mean_loss = total_loss / len(train_loader)\n        history[\"train_losses\"].append(mean_loss)\n        history[\"train_metrics\"].append(metric.compute().item())\n        val_metric = evaluate_tm(model, valid_loader, metric).item()\n        history[\"valid_metrics\"].append(val_metric)\n        scheduler.step(val_metric)\n        print(f\"Epoch:{epoch+1}/{n_epochs}, \"\n             f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n             f\"Train Metric: {history['train_metrics'][-1]:.4f}, \"\n             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}\")\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:39.597033Z","iopub.execute_input":"2025-09-15T07:15:39.597236Z","iopub.status.idle":"2025-09-15T07:15:49.903196Z","shell.execute_reply.started":"2025-09-15T07:15:39.597219Z","shell.execute_reply":"2025-09-15T07:15:49.902557Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class ShakespeareModel(nn.Module):\n    def __init__(self, vocab_size, n_hidden=128, n_layers=2, embed_size=10, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.gru = nn.GRU(embed_size, n_hidden, num_layers=n_layers,\n                         batch_first=True, dropout=dropout)\n        self.output = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, X):\n        embeddings = self.embed(X)\n        outputs, _states = self.gru(embeddings)\n        return self.output(outputs[:, -1, :])\nmodel = ShakespeareModel(len(vocab)).to(device)\nif torch.cuda.device_count()>1:\n    print(\"Using\", torch.cuda.device_count(),\"GPU's\")\n    model = nn.DataParallel(model)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:49.903900Z","iopub.execute_input":"2025-09-15T07:15:49.904213Z","iopub.status.idle":"2025-09-15T07:15:50.199250Z","shell.execute_reply.started":"2025-09-15T07:15:49.904197Z","shell.execute_reply":"2025-09-15T07:15:50.198547Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPU's\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): ShakespeareModel(\n    (embed): Embedding(39, 10)\n    (gru): GRU(10, 128, num_layers=2, batch_first=True, dropout=0.1)\n    (output): Linear(in_features=128, out_features=39, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"n_epochs = 20\nxentropy = nn.CrossEntropyLoss()\naccuracy = torchmetrics.Accuracy(task=\"multiclass\",num_classes=len(vocab)).to(device)\noptimizer = torch.optim.NAdam(model.parameters())\n\nhistory = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader, n_epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:15:50.200018Z","iopub.execute_input":"2025-09-15T07:15:50.200278Z","iopub.status.idle":"2025-09-15T07:22:16.831892Z","shell.execute_reply.started":"2025-09-15T07:15:50.200260Z","shell.execute_reply":"2025-09-15T07:22:16.830909Z"}},"outputs":[{"name":"stdout","text":"Batch 977/977, loss =1.9251Epoch:1/20, Train Loss: 1.9251, Train Metric: 0.4352, Valid Metric: 0.4791\nBatch 977/977, loss =1.5565Epoch:2/20, Train Loss: 1.5565, Train Metric: 0.5280, Valid Metric: 0.5152\nBatch 977/977, loss =1.4798Epoch:3/20, Train Loss: 1.4798, Train Metric: 0.5465, Valid Metric: 0.5294\nBatch 977/977, loss =1.4423Epoch:4/20, Train Loss: 1.4423, Train Metric: 0.5564, Valid Metric: 0.5358\nBatch 977/977, loss =1.4189Epoch:5/20, Train Loss: 1.4189, Train Metric: 0.5622, Valid Metric: 0.5447\nBatch 977/977, loss =1.4019Epoch:6/20, Train Loss: 1.4019, Train Metric: 0.5658, Valid Metric: 0.5473\nBatch 977/977, loss =1.3897Epoch:7/20, Train Loss: 1.3897, Train Metric: 0.5691, Valid Metric: 0.5481\nBatch 977/977, loss =1.3807Epoch:8/20, Train Loss: 1.3807, Train Metric: 0.5711, Valid Metric: 0.5510\nBatch 977/977, loss =1.3730Epoch:9/20, Train Loss: 1.3730, Train Metric: 0.5734, Valid Metric: 0.5536\nBatch 977/977, loss =1.3665Epoch:10/20, Train Loss: 1.3665, Train Metric: 0.5748, Valid Metric: 0.5550\nBatch 977/977, loss =1.3620Epoch:11/20, Train Loss: 1.3620, Train Metric: 0.5757, Valid Metric: 0.5531\nBatch 977/977, loss =1.3577Epoch:12/20, Train Loss: 1.3577, Train Metric: 0.5766, Valid Metric: 0.5548\nBatch 977/977, loss =1.3534Epoch:13/20, Train Loss: 1.3534, Train Metric: 0.5776, Valid Metric: 0.5554\nBatch 977/977, loss =1.3501Epoch:14/20, Train Loss: 1.3501, Train Metric: 0.5783, Valid Metric: 0.5557\nBatch 977/977, loss =1.3484Epoch:15/20, Train Loss: 1.3484, Train Metric: 0.5785, Valid Metric: 0.5566\nBatch 977/977, loss =1.3451Epoch:16/20, Train Loss: 1.3451, Train Metric: 0.5793, Valid Metric: 0.5588\nBatch 977/977, loss =1.3432Epoch:17/20, Train Loss: 1.3432, Train Metric: 0.5797, Valid Metric: 0.5617\nBatch 977/977, loss =1.3408Epoch:18/20, Train Loss: 1.3408, Train Metric: 0.5807, Valid Metric: 0.5622\nBatch 977/977, loss =1.3386Epoch:19/20, Train Loss: 1.3386, Train Metric: 0.5808, Valid Metric: 0.5608\nBatch 977/977, loss =1.3374Epoch:20/20, Train Loss: 1.3374, Train Metric: 0.5814, Valid Metric: 0.5605\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"torch.save(model.state_dict(), \"my_shakespeare_model.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:25:22.932592Z","iopub.execute_input":"2025-09-15T07:25:22.932888Z","iopub.status.idle":"2025-09-15T07:25:22.939568Z","shell.execute_reply.started":"2025-09-15T07:25:22.932867Z","shell.execute_reply":"2025-09-15T07:25:22.938995Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"text = \"To be or not to b\"\nencoded_text = encode_text(text).unsqueeze(dim=0).to(device)\nencoded_text.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:29:51.496485Z","iopub.execute_input":"2025-09-15T07:29:51.497204Z","iopub.status.idle":"2025-09-15T07:29:51.502328Z","shell.execute_reply.started":"2025-09-15T07:29:51.497179Z","shell.execute_reply":"2025-09-15T07:29:51.501566Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 17])"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    y_logits = model(encoded_text)\n    predicted_char_id = y_logits[0].argmax().item()\n    predicted_char = id_to_char[predicted_char_id]\npredicted_char","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:34:18.003267Z","iopub.execute_input":"2025-09-15T07:34:18.003509Z","iopub.status.idle":"2025-09-15T07:34:18.011117Z","shell.execute_reply.started":"2025-09-15T07:34:18.003493Z","shell.execute_reply":"2025-09-15T07:34:18.010347Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"'e'"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"def next_char(model, text, temperature=0.7):\n    model.eval()\n    encoded_text = encode_text(text).unsqueeze(0).to(device)\n    with torch.no_grad():\n        y_logits = model(encoded_text)\n        y_probas = torch.softmax(y_logits/temperature, dim=-1)\n        predicted_char_id = torch.multinomial(y_probas,num_samples=1).item()     \n        return id_to_char[predicted_char_id]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:44:44.637498Z","iopub.execute_input":"2025-09-15T07:44:44.637804Z","iopub.status.idle":"2025-09-15T07:44:44.642444Z","shell.execute_reply.started":"2025-09-15T07:44:44.637780Z","shell.execute_reply":"2025-09-15T07:44:44.641699Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import time\ndef generate_text(model, text, n_chars=100,temperature=0.7):\n    print(text, end='', flush=True)\n    for _ in range(n_chars):\n        char = next_char(model, text, temperature)\n        text += char\n        print(char, end='', flush=True)\n        time.sleep(0.01)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T08:00:09.398993Z","iopub.execute_input":"2025-09-15T08:00:09.399239Z","iopub.status.idle":"2025-09-15T08:00:09.403758Z","shell.execute_reply.started":"2025-09-15T08:00:09.399222Z","shell.execute_reply":"2025-09-15T08:00:09.403136Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"print(generate_text(model,\"To be or not to b\",n_chars=500 ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T08:00:10.812701Z","iopub.execute_input":"2025-09-15T08:00:10.813302Z","iopub.status.idle":"2025-09-15T08:00:17.402941Z","shell.execute_reply.started":"2025-09-15T08:00:10.813278Z","shell.execute_reply":"2025-09-15T08:00:17.402338Z"}},"outputs":[{"name":"stdout","text":"To be or not to be\nmakes this good to mercy of the head?\n\nedward:\nand shall not be rest that i may should not every fear.\n\nsomerset:\nwhy is the jight thou speaks to his land, when faults at crown,\nor i cannot said most less than here and honour'd to the will,\ni do a patience which we at his capulet.\n\nromeo:\nwhy, that's grief to my conscience with her, and short me;\nand in the people of far of his assus;\nwho shall i can see thy earth will i think, i come and but your world-drown\nand destroyed as never say our cauNone\n","output_type":"stream"}],"execution_count":79}]}