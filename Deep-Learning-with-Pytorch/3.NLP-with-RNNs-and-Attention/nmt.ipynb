{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13114139,"sourceType":"datasetVersion","datasetId":8307286}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport numpy as np\nimport pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:30.463968Z","iopub.execute_input":"2025-09-20T08:05:30.464613Z","iopub.status.idle":"2025-09-20T08:05:34.651339Z","shell.execute_reply.started":"2025-09-20T08:05:30.464578Z","shell.execute_reply":"2025-09-20T08:05:34.650753Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = \"cuda\"\n    print(torch.cuda.device_count())\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:34.652503Z","iopub.execute_input":"2025-09-20T08:05:34.652893Z","iopub.status.idle":"2025-09-20T08:05:34.739296Z","shell.execute_reply.started":"2025-09-20T08:05:34.652872Z","shell.execute_reply":"2025-09-20T08:05:34.738599Z"}},"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"data = []\nfilename = \"/kaggle/input/eng-spa/spa.txt\"\nwith open(filename, \"r\") as f:\n    for line in f:\n        data.append(line.strip())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:34.740201Z","iopub.execute_input":"2025-09-20T08:05:34.740461Z","iopub.status.idle":"2025-09-20T08:05:34.937448Z","shell.execute_reply.started":"2025-09-20T08:05:34.740437Z","shell.execute_reply":"2025-09-20T08:05:34.936865Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"cleaned_data = [line.replace(\"¡\", \"\").replace(\"¿\", \"\") for line in data]\npairs = [line.split(\"\\t\") for line in cleaned_data]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:34.939242Z","iopub.execute_input":"2025-09-20T08:05:34.939742Z","iopub.status.idle":"2025-09-20T08:05:35.141719Z","shell.execute_reply.started":"2025-09-20T08:05:34.939721Z","shell.execute_reply":"2025-09-20T08:05:35.140871Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"np.random.shuffle(pairs)\neng_sentences, es_sentences = zip(*pairs)\nfor i in range(3):\n    print(eng_sentences[i], \"==>\" ,es_sentences[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:35.142570Z","iopub.execute_input":"2025-09-20T08:05:35.142803Z","iopub.status.idle":"2025-09-20T08:05:35.540508Z","shell.execute_reply.started":"2025-09-20T08:05:35.142784Z","shell.execute_reply":"2025-09-20T08:05:35.539916Z"}},"outputs":[{"name":"stdout","text":"We are not short of oil in this country. ==> No estamos escasos de petróleo en este país.\nIsn't that excessive? ==> No es eso excesivo?\nI thought we would have breakfast together. ==> Pensé que desayunaríamos juntos.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:35.541261Z","iopub.execute_input":"2025-09-20T08:05:35.541507Z","iopub.status.idle":"2025-09-20T08:05:40.363766Z","shell.execute_reply.started":"2025-09-20T08:05:35.541477Z","shell.execute_reply":"2025-09-20T08:05:40.363127Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2a47f8fd6f346868d9b2d70a410f18d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75a26fd0009d457fbdaae5d259cabd48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92d5cdeeba594f578d065ba711b0d900"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d592d6104c54bbdb85143bb466f2c4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a02c9145744f44b9922c42d2e34d4ad5"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"max_len = 50\ndef encode_with_gpt2(sentence, add_sos_and_eos=False):\n    \n    texts = [f\"<s> {s} </s>\" if add_sos_and_eos else s for s in sentence]\n    encodings = tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_len,\n        return_tensors=\"pt\"\n    )\n    return encodings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:40.364487Z","iopub.execute_input":"2025-09-20T08:05:40.364986Z","iopub.status.idle":"2025-09-20T08:05:40.369488Z","shell.execute_reply.started":"2025-09-20T08:05:40.364959Z","shell.execute_reply":"2025-09-20T08:05:40.368807Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass TranslationDataset(Dataset):\n    def __init__(self, src_sentences, tgt_sentences, tokenizer, max_len=500):\n        self.src_sentences = src_sentences\n        self.tgt_sentences = tgt_sentences\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.src_sentences)\n\n    def __getitem__(self,idx):\n        src = self.src_sentences[idx]\n        tgt = f\"<s> {self.tgt_sentences[idx]} </s>\"\n\n        src_enc = self.tokenizer(\n            src,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\")\n        tgt_enc = self.tokenizer(\n            tgt,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\")\n\n        decoder_input_ids = tgt_enc[\"input_ids\"][:,:-1].squeeze(0)\n        labels = tgt_enc[\"input_ids\"][:,1:].squeeze(0)\n\n        return {\n            \"encoder_input_ids\":src_enc[\"input_ids\"].squeeze(0),\n            \"encoder_attention_mask\":src_enc[\"attention_mask\"].squeeze(0),\n            \"decoder_input_ids\":decoder_input_ids,\n            \"labels\":labels\n        } ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:40.370527Z","iopub.execute_input":"2025-09-20T08:05:40.370858Z","iopub.status.idle":"2025-09-20T08:05:40.397499Z","shell.execute_reply.started":"2025-09-20T08:05:40.370831Z","shell.execute_reply":"2025-09-20T08:05:40.396860Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass Encoder(nn.Module):\n    def __init__(self, pretrained_embed, n_hidden=2, hidden_size=64,dropout=0.2):\n        super().__init__()\n        weights = pretrained_embed.weight.data\n        self.embed = nn.Embedding.from_pretrained(weights,freeze=True)\n        embed_size = weights.shape[-1]\n        self.gru = nn.GRU(embed_size, hidden_size, num_layers=n_hidden,\n                         batch_first=True, dropout=dropout, bidirectional=True)\n        self.n_hidden = n_hidden\n\n    def forward(self, input_ids, attention_mask):\n        embeddings = self.embed(input_ids)\n        lengths = attention_mask.sum(dim=1)\n        packed = pack_padded_sequence(embeddings,\n                                     lengths = lengths.cpu(),\n                                     batch_first=True,\n                                     enforce_sorted=False)\n        outputs, hidden = self.gru(packed)\n        outputs, _ = pad_packed_sequence(outputs, batch_first=True, total_length=input_ids.size(1))\n        #########################\n        batch_size = hidden.size(1)\n        hidden = hidden.view(self.n_hidden, 2, batch_size, -1)\n        hidden = torch.cat([hidden[:, 0, :, :], hidden[:, 1, :, :]], dim=2)\n       \n       \n        return outputs, hidden    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:40.398185Z","iopub.execute_input":"2025-09-20T08:05:40.398377Z","iopub.status.idle":"2025-09-20T08:05:40.416709Z","shell.execute_reply.started":"2025-09-20T08:05:40.398362Z","shell.execute_reply":"2025-09-20T08:05:40.415997Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, pretrained_embed, n_hidden=2, hidden_size=64):\n        super().__init__()\n        weights = pretrained_embed.weight.data\n        self.embed = nn.Embedding.from_pretrained(weights,freeze=True)\n        embed_size = weights.shape[-1]\n        self.gru = nn.GRU(embed_size, hidden_size*2,num_layers=n_hidden,\n                          batch_first=True)\n        self.output = nn.Linear(hidden_size*2, weights.shape[0])\n\n    def forward(self, input_ids, hidden):\n        embeddings = self.embed(input_ids)\n        outputs, hidden = self.gru(embeddings, hidden)\n        logits = self.output(outputs)\n        return logits, hidden       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:40.418772Z","iopub.execute_input":"2025-09-20T08:05:40.419196Z","iopub.status.idle":"2025-09-20T08:05:40.435248Z","shell.execute_reply.started":"2025-09-20T08:05:40.419178Z","shell.execute_reply":"2025-09-20T08:05:40.434593Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()   \n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src_ids, src_mask, tgt_ids):\n        enc_outputs, enc_hidden = self.encoder(src_ids, src_mask)\n        logits, _ = self.decoder(tgt_ids, enc_hidden)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:40.436044Z","iopub.execute_input":"2025-09-20T08:05:40.436284Z","iopub.status.idle":"2025-09-20T08:05:40.451348Z","shell.execute_reply.started":"2025-09-20T08:05:40.436260Z","shell.execute_reply":"2025-09-20T08:05:40.450761Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import transformers\n\ngpt_model = transformers.AutoModel.from_pretrained(\"gpt2\")\nvocab_size = gpt_model.get_input_embeddings().weight.data.shape[0]\nvocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:05:40.452023Z","iopub.execute_input":"2025-09-20T08:05:40.452268Z","iopub.status.idle":"2025-09-20T08:06:04.328816Z","shell.execute_reply.started":"2025-09-20T08:05:40.452238Z","shell.execute_reply":"2025-09-20T08:06:04.328013Z"}},"outputs":[{"name":"stderr","text":"2025-09-20 08:05:50.896436: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758355551.089754      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758355551.141254      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b2b975780f439c9f50dca1161799a6"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"50257"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\neng_train, eng_valid, es_train, es_valid = train_test_split(\n    eng_sentences, es_sentences, test_size = 0.20\n)\n\nbatch_size = 32\n\ntrain_dataset = TranslationDataset(eng_train, es_train, tokenizer)\nvalid_dataset = TranslationDataset(eng_valid, es_valid, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:06:04.329716Z","iopub.execute_input":"2025-09-20T08:06:04.330291Z","iopub.status.idle":"2025-09-20T08:06:04.410828Z","shell.execute_reply.started":"2025-09-20T08:06:04.330271Z","shell.execute_reply":"2025-09-20T08:06:04.410207Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torchmetrics\n\ndef evaluate_tm(model, data_loader, metric, vocab_size):\n    model.eval()\n    metric.reset()\n    with torch.no_grad():\n        for batch in data_loader:\n            src_ids = batch[\"encoder_input_ids\"].to(device)\n            src_mask = batch[\"encoder_attention_mask\"].to(device)\n            tgt_ids = batch[\"decoder_input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            y_pred = model(src_ids, src_mask, tgt_ids)\n            metric.update(y_pred.view(-1,vocab_size), labels.view(-1))\n    return metric.compute()\n            \ndef train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs, vocab_size):\n    history = {\"train_losses\":[],\"train_metrics\":[],\"valid_metrics\":[]}\n    for epoch in range(n_epochs):\n        total_loss = 0\n        metric.reset()\n        model.train()\n        for idx, batch in enumerate(train_loader):\n            src_ids = batch[\"encoder_input_ids\"].to(device)\n            src_mask = batch[\"encoder_attention_mask\"].to(device)\n            tgt_ids = batch[\"decoder_input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            y_pred = model(src_ids, src_mask, tgt_ids)\n            loss = criterion(y_pred.view(-1,vocab_size), labels.view(-1))\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            metric.update(y_pred.view(-1,vocab_size), labels.view(-1))\n            print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n            print(f\", loss ={total_loss/(idx+1 ):.4f} \", end=\"\")\n        mean_loss = total_loss / len(train_loader)\n        history[\"train_losses\"].append(mean_loss)\n        history[\"train_metrics\"].append(metric.compute().item())\n        val_metric = evaluate_tm(model, valid_loader, metric,vocab_size).item()\n        history[\"valid_metrics\"].append(val_metric)\n        print(f\"Epoch:{epoch+1}/{n_epochs}, \"\n             f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n             f\"Train Metric: {history['train_metrics'][-1]:.4f}%, \"\n             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}%\")\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:06:04.411538Z","iopub.execute_input":"2025-09-20T08:06:04.411774Z","iopub.status.idle":"2025-09-20T08:06:05.131483Z","shell.execute_reply.started":"2025-09-20T08:06:04.411757Z","shell.execute_reply":"2025-09-20T08:06:05.130895Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"encoder = Encoder(gpt_model.get_input_embeddings())\ndecoder = Decoder(gpt_model.get_input_embeddings())\nnmt_model = Seq2Seq(encoder, decoder).to(device)\n\noptimizer = torch.optim.NAdam(nmt_model.parameters())\nxentropy = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\naccuracy = torchmetrics.Accuracy(task=\"multiclass\",num_classes = vocab_size).to(device)\nn_epochs=20\n\nhistory = train(nmt_model, optimizer, xentropy, accuracy, train_loader, valid_loader, n_epochs, vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T08:06:05.138499Z","iopub.execute_input":"2025-09-20T08:06:05.138769Z"}},"outputs":[{"name":"stdout","text":"Batch 2975/2975, loss =2.4291 Epoch:1/20, Train Loss: 2.4291, Train Metric: 0.0196%, Valid Metric: 0.0224%\nBatch 388/2975, loss =1.7225 ","output_type":"stream"}],"execution_count":null}]}