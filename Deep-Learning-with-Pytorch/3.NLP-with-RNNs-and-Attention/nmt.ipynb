{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13114139,"sourceType":"datasetVersion","datasetId":8307286}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport numpy as np\nimport pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:40.444663Z","iopub.execute_input":"2025-09-20T07:19:40.445258Z","iopub.status.idle":"2025-09-20T07:19:40.448948Z","shell.execute_reply.started":"2025-09-20T07:19:40.445238Z","shell.execute_reply":"2025-09-20T07:19:40.448137Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = \"cuda\"\n    print(torch.cuda.device_count())\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:40.450158Z","iopub.execute_input":"2025-09-20T07:19:40.450773Z","iopub.status.idle":"2025-09-20T07:19:40.466104Z","shell.execute_reply.started":"2025-09-20T07:19:40.450750Z","shell.execute_reply":"2025-09-20T07:19:40.465363Z"}},"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"data = []\nfilename = \"/kaggle/input/eng-spa/spa.txt\"\nwith open(filename, \"r\") as f:\n    for line in f:\n        data.append(line.strip())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:40.466917Z","iopub.execute_input":"2025-09-20T07:19:40.467628Z","iopub.status.idle":"2025-09-20T07:19:40.527729Z","shell.execute_reply.started":"2025-09-20T07:19:40.467604Z","shell.execute_reply":"2025-09-20T07:19:40.527120Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"cleaned_data = [line.replace(\"¡\", \"\").replace(\"¿\", \"\") for line in data]\npairs = [line.split(\"\\t\") for line in cleaned_data]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:40.528872Z","iopub.execute_input":"2025-09-20T07:19:40.529121Z","iopub.status.idle":"2025-09-20T07:19:41.211597Z","shell.execute_reply.started":"2025-09-20T07:19:40.529104Z","shell.execute_reply":"2025-09-20T07:19:41.211026Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"np.random.shuffle(pairs)\neng_sentences, es_sentences = zip(*pairs)\nfor i in range(3):\n    print(eng_sentences[i], \"==>\" ,es_sentences[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:41.212240Z","iopub.execute_input":"2025-09-20T07:19:41.212446Z","iopub.status.idle":"2025-09-20T07:19:41.306446Z","shell.execute_reply.started":"2025-09-20T07:19:41.212409Z","shell.execute_reply":"2025-09-20T07:19:41.305731Z"}},"outputs":[{"name":"stdout","text":"You've still got thirty minutes. ==> Todavía tienes treinta minutos.\nShe'll be having dinner with him at this time tomorrow. ==> Ella cenará con él a esta hora mañana.\nI like your article. ==> Me gusta tu artículo.\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:41.307184Z","iopub.execute_input":"2025-09-20T07:19:41.307377Z","iopub.status.idle":"2025-09-20T07:19:41.697721Z","shell.execute_reply.started":"2025-09-20T07:19:41.307354Z","shell.execute_reply":"2025-09-20T07:19:41.697134Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"max_len = 100\ndef encode_with_gpt2(sentence, add_sos_and_eos=False):\n    \n    texts = [f\"<s> {s} </s>\" if add_sos_and_eos else s for s in sentence]\n    encodings = tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_len,\n        return_tensors=\"pt\"\n    )\n    return encodings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:41.698459Z","iopub.execute_input":"2025-09-20T07:19:41.698667Z","iopub.status.idle":"2025-09-20T07:19:41.703964Z","shell.execute_reply.started":"2025-09-20T07:19:41.698649Z","shell.execute_reply":"2025-09-20T07:19:41.703318Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass TranslationDataset(Dataset):\n    def __init__(self, src_sentences, tgt_sentences, tokenizer, max_len=500):\n        self.src_sentences = src_sentences\n        self.tgt_sentences = tgt_sentences\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.src_sentences)\n\n    def __getitem__(self,idx):\n        src = self.src_sentences[idx]\n        tgt = f\"<s> {self.tgt_sentences[idx]} </s>\"\n\n        src_enc = self.tokenizer(\n            src,\n            padding=True,\n            truncation=True,\n            max_length=max_len,\n            return_tensors=\"pt\")\n        tgt_enc = self.tokenizer(\n            tgt,\n            padding=True,\n            truncation=True,\n            max_length=max_len,\n            return_tensors=\"pt\")\n\n        decoder_input_ids = tgt_enc[\"input_ids\"][:,:-1].squeeze(0)\n        labels = tgt_enc[\"input_ids\"][:,1:].squeeze(0)\n\n        return {\n            \"encoder_input_ids\":src_enc[\"input_ids\"].squeeze(0),\n            \"encoder_attention_mask\":src_enc[\"attention_mask\"].squeeze(0),\n            \"decoder_input_ids\":decoder_input_ids,\n            \"labels\":labels\n        } ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:41.704617Z","iopub.execute_input":"2025-09-20T07:19:41.704846Z","iopub.status.idle":"2025-09-20T07:19:41.718725Z","shell.execute_reply.started":"2025-09-20T07:19:41.704830Z","shell.execute_reply":"2025-09-20T07:19:41.718054Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass Encoder(nn.Module):\n    def __init__(self, pretrained_embed, n_hidden=2, hidden_size=128,dropout=0.2):\n        super().__init__()\n        weights = pretrained_embed.weight.data\n        self.embed = nn.Embedding.from_pretrained(weights,freeze=True)\n        embed_size = weights.shape[-1]\n        self.gru = nn.GRU(embed_size, hidden_size, num_layers=n_hidden,\n                         batch_first=True, dropout=dropout, bidirectional=True)\n\n    def forward(self, input_ids, attention_mask):\n        embeddings = self.embed(input_ids)\n        lengths = attention_mask.sum(dim=1)\n        packed = pack_padded_sequence(embeddings,\n                                     lengths = lengths.cpu(),\n                                     batch_first=True,\n                                     enforce_sorted=False)\n        outputs, hidden = self.gru(packed)\n        outputs, _ = pad_packed_sequence(outputs, batch_first=True, total_length=input_ids.size(1))\n        return outputs, hidden    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:41.720494Z","iopub.execute_input":"2025-09-20T07:19:41.720736Z","iopub.status.idle":"2025-09-20T07:19:41.737833Z","shell.execute_reply.started":"2025-09-20T07:19:41.720723Z","shell.execute_reply":"2025-09-20T07:19:41.737148Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, pretrained_embed, n_hidden=2, hidden_size=128):\n        super().__init__()\n        weights = pretrained_embed.weight.data\n        self.embed = nn.Embedding.from_pretrained(weights,freeze=True)\n        embed_size = weights.shape[-1]\n        self.gru = nn.GRU(embed_size, hidden_size*2,num_layers=n_hidden,\n                          batch_first=True)\n        self.output = nn.Linear(hidden_size*2, weights.shape[0])\n\n    def forward(self, input_ids, hidden):\n        embeddings = self.embed(input_ids)\n        outputs, hidden = self.gru(embeddings, hidden)\n        logits = self.output(outputs)\n        return logits, hidden       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:41.738473Z","iopub.execute_input":"2025-09-20T07:19:41.738701Z","iopub.status.idle":"2025-09-20T07:19:41.754034Z","shell.execute_reply.started":"2025-09-20T07:19:41.738680Z","shell.execute_reply":"2025-09-20T07:19:41.753464Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()   \n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src_ids, src_mask, tgt_ids):\n        enc_outputs, enc_hidden = self.encoder(src_ids, src_mask)\n        logits, _ = self.decoder(tgt_ids, enc_hidden)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:41.754742Z","iopub.execute_input":"2025-09-20T07:19:41.754994Z","iopub.status.idle":"2025-09-20T07:19:41.773791Z","shell.execute_reply.started":"2025-09-20T07:19:41.754968Z","shell.execute_reply":"2025-09-20T07:19:41.772923Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import transformers\n\ngpt_model = transformers.AutoModel.from_pretrained(\"gpt2\")\nvocab_size = gpt_model.get_input_embeddings().weight.data.shape[0]\nvocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:41.774701Z","iopub.execute_input":"2025-09-20T07:19:41.775019Z","iopub.status.idle":"2025-09-20T07:19:41.958559Z","shell.execute_reply.started":"2025-09-20T07:19:41.774998Z","shell.execute_reply":"2025-09-20T07:19:41.957940Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"50257"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\neng_train, eng_valid, es_train, es_valid = train_test_split(\n    eng_sentences, es_sentences, test_size = 0.20\n)\n\nbatch_size = 128\n\ntrain_dataset = TranslationDataset(eng_train, es_train, tokenizer)\nvalid_dataset = TranslationDataset(eng_valid, es_valid, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:41.959240Z","iopub.execute_input":"2025-09-20T07:19:41.959504Z","iopub.status.idle":"2025-09-20T07:19:42.023204Z","shell.execute_reply.started":"2025-09-20T07:19:41.959477Z","shell.execute_reply":"2025-09-20T07:19:42.022716Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"import torchmetrics\n\ndef evaluate_tm(model, data_loader, metric):\n    model.eval()\n    metric.reset()\n    with torch.no_grad():\n        for batch in data_loader:\n            src_ids = batch[\"encoder_input_ids\"].to(device)\n            src_mask = batch[\"encoder_attention_mask\"].to(device)\n            tgt_ids = batch[\"decoder_input_ids\"].to(device)\n            labels = batch[\"labels\"]\n\n            y_pred = model(src_ids, src_mask, tgt_ids)\n            metric.update(y_pred.view(-1,vocab_size), labels.view(-1))\n    return metric.compute\n            \ndef train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs):\n    history = {\"train_losses\":[],\"train_metrics\":[],\"valid_metrics\":[]}\n    for epoch in range(n_epochs):\n        total_loss = 0\n        metric.reset()\n        model.train()\n        for idx, batch in enumerate(train_loader):\n            src_ids = batch[\"encoder_input_ids\"].to(device)\n            src_mask = batch[\"encoder_attention_mask\"].to(device)\n            tgt_ids = batch[\"decoder_input_ids\"].to(device)\n            labels = batch[\"labels\"]\n\n            y_pred = model(src_ids, src_mask, tgt_ids)\n            loss = criterion(y_pred.view(-1,vocab_size), labels.view(-1))\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            optimzer.zero_grad()\n            metric.update(y_pred.view(-1,vocab_size), labels.view(-1))\n            print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n            print(f\", loss ={total_loss/(idx+1 ):.4f} \", end=\"\")\n        mean_loss = total_loss / len(train_loader)\n        history[\"train_losses\"].append(mean_loss)\n        history[\"train_metrics\"].append(metric.compute().item())\n        val_metric = evaluate_tm(model, valid_loader, metric).item()\n        history[\"valid_metrics\"].append(val_metric)\n        print(f\"Epoch:{epoch+1}/{n_epochs}, \"\n             f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n             f\"Train Metric: {history['train_metrics'][-1]:.4f}%, \"\n             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}%\")\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:42.023778Z","iopub.execute_input":"2025-09-20T07:19:42.023989Z","iopub.status.idle":"2025-09-20T07:19:42.039287Z","shell.execute_reply.started":"2025-09-20T07:19:42.023964Z","shell.execute_reply":"2025-09-20T07:19:42.038613Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"encoder = Encoder(gpt_model.get_input_embeddings())\ndecoder = Decoder(gpt_model.get_input_embeddings())\n\nnmt_model = Seq2Seq(encoder, decoder).to(device)\n\noptimizer = torch.optim.NAdam(nmt_model.parameters())\nxentropy = nn.CrossEntropyLoss(ignore_index=0)\naccuracy = torchmetrics.Accuracy(task=\"multiclass\",num_classes = vocab_size)\nn_epochs=20\n\nhistory = train(nmt_model, optimizer, xentropy, accuracy, train_loader, valid_loader, n_epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:19:42.040026Z","iopub.execute_input":"2025-09-20T07:19:42.040688Z","iopub.status.idle":"2025-09-20T07:19:42.431827Z","shell.execute_reply.started":"2025-09-20T07:19:42.040662Z","shell.execute_reply":"2025-09-20T07:19:42.430826Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1810762714.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/2404609150.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0msrc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 clone.update(\n\u001b[0;32m--> 171\u001b[0;31m                     {\n\u001b[0m\u001b[1;32m    172\u001b[0m                         key: collate(\n\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 clone.update(\n\u001b[1;32m    171\u001b[0m                     {\n\u001b[0;32m--> 172\u001b[0;31m                         key: collate(\n\u001b[0m\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [9] at entry 0 and [8] at entry 1"],"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [9] at entry 0 and [8] at entry 1","output_type":"error"}],"execution_count":49}]}