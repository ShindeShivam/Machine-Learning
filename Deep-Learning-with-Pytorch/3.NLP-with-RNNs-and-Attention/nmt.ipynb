{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13114139,"sourceType":"datasetVersion","datasetId":8307286}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.008507Z","iopub.execute_input":"2025-09-21T03:03:25.008775Z","iopub.status.idle":"2025-09-21T03:03:25.013193Z","shell.execute_reply.started":"2025-09-21T03:03:25.008755Z","shell.execute_reply":"2025-09-21T03:03:25.012374Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = \"cuda\"\n    print(torch.cuda.device_count())\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.014527Z","iopub.execute_input":"2025-09-21T03:03:25.015234Z","iopub.status.idle":"2025-09-21T03:03:25.031900Z","shell.execute_reply.started":"2025-09-21T03:03:25.015214Z","shell.execute_reply":"2025-09-21T03:03:25.031065Z"}},"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"data = []\nfilename = \"/kaggle/input/eng-spa/spa.txt\"\nwith open(filename, \"r\") as f:\n    for line in f:\n        data.append(line.strip())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.033153Z","iopub.execute_input":"2025-09-21T03:03:25.033375Z","iopub.status.idle":"2025-09-21T03:03:25.097658Z","shell.execute_reply.started":"2025-09-21T03:03:25.033358Z","shell.execute_reply":"2025-09-21T03:03:25.096808Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"cleaned_data = [line.replace(\"¡\", \"\").replace(\"¿\", \"\") for line in data]\npairs = [line.split(\"\\t\") for line in cleaned_data]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.098493Z","iopub.execute_input":"2025-09-21T03:03:25.098708Z","iopub.status.idle":"2025-09-21T03:03:25.269560Z","shell.execute_reply.started":"2025-09-21T03:03:25.098691Z","shell.execute_reply":"2025-09-21T03:03:25.268746Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"np.random.shuffle(pairs)\neng_sentences, es_sentences = zip(*pairs)\nfor i in range(3):\n    print(eng_sentences[i], \"==>\" ,es_sentences[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.270929Z","iopub.execute_input":"2025-09-21T03:03:25.271182Z","iopub.status.idle":"2025-09-21T03:03:25.408639Z","shell.execute_reply.started":"2025-09-21T03:03:25.271165Z","shell.execute_reply":"2025-09-21T03:03:25.407871Z"}},"outputs":[{"name":"stdout","text":"Turn off the gas. ==> Cierre el gas.\nFifty families live in this tiny village. ==> Cincuenta familias viven en este pequeño pueblo.\nYou surprised everybody. ==> Los sorprendiste a todos.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\nvocab_size = tokenizer.vocab_size\nvocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.409427Z","iopub.execute_input":"2025-09-21T03:03:25.409710Z","iopub.status.idle":"2025-09-21T03:03:25.820217Z","shell.execute_reply.started":"2025-09-21T03:03:25.409681Z","shell.execute_reply":"2025-09-21T03:03:25.819539Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:177: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"65001"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# max_len = 50\n# def encode_with_pretrained2(sentence, add_sos_and_eos=False):\n    \n#     texts = [f\"<s> {s} </s>\" if add_sos_and_eos else s for s in sentence]\n#     encodings = tokenizer(\n#         texts,\n#         padding=True,\n#         truncation=True,\n#         max_length=max_len,\n#         return_tensors=\"pt\"\n#     )\n#     return encodings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.821031Z","iopub.execute_input":"2025-09-21T03:03:25.821523Z","iopub.status.idle":"2025-09-21T03:03:25.825759Z","shell.execute_reply.started":"2025-09-21T03:03:25.821504Z","shell.execute_reply":"2025-09-21T03:03:25.825111Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass TranslationDataset(Dataset):\n    def __init__(self, src_sentences, tgt_sentences, tokenizer, max_len=50):\n        self.src_sentences = src_sentences\n        self.tgt_sentences = tgt_sentences\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.src_sentences)\n\n    def __getitem__(self,idx):\n        src = self.src_sentences[idx]\n        tgt = f\"<s> {self.tgt_sentences[idx]} </s>\"\n\n        src_enc = self.tokenizer(\n            src,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\")\n        tgt_enc = self.tokenizer(\n            tgt,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\")\n\n        decoder_input_ids = tgt_enc[\"input_ids\"][:,:-1].squeeze(0)\n        labels = tgt_enc[\"input_ids\"][:,1:].squeeze(0)\n\n        return {\n            \"encoder_input_ids\":src_enc[\"input_ids\"].squeeze(0),\n            \"encoder_attention_mask\":src_enc[\"attention_mask\"].squeeze(0),\n            \"decoder_input_ids\":decoder_input_ids,\n            \"labels\":labels\n        } ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.826629Z","iopub.execute_input":"2025-09-21T03:03:25.826912Z","iopub.status.idle":"2025-09-21T03:03:25.846448Z","shell.execute_reply.started":"2025-09-21T03:03:25.826890Z","shell.execute_reply":"2025-09-21T03:03:25.845538Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass Encoder(nn.Module):\n    def __init__(self, pretrained_embed, n_hidden=2, hidden_size=64,dropout=0.2):\n        super().__init__()\n        weights = pretrained_embed.weight.data\n        self.embed = nn.Embedding.from_pretrained(weights,freeze=True)\n        embed_size = weights.shape[-1]\n        self.gru = nn.GRU(embed_size, hidden_size, num_layers=n_hidden,\n                         batch_first=True, dropout=dropout, bidirectional=True)\n        self.n_hidden = n_hidden\n\n    def forward(self, input_ids, attention_mask):\n        embeddings = self.embed(input_ids)\n        lengths = attention_mask.sum(dim=1)\n        packed = pack_padded_sequence(embeddings,\n                                     lengths = lengths.cpu(),\n                                     batch_first=True,\n                                     enforce_sorted=False)\n        outputs, hidden = self.gru(packed)\n        outputs, _ = pad_packed_sequence(outputs, batch_first=True, total_length=input_ids.size(1))\n        #########################\n        batch_size = hidden.size(1)\n        hidden = hidden.view(self.n_hidden, 2, batch_size, -1)\n        hidden = torch.cat([hidden[:, 0, :, :], hidden[:, 1, :, :]], dim=2)\n       \n       \n        return outputs, hidden    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.847319Z","iopub.execute_input":"2025-09-21T03:03:25.847691Z","iopub.status.idle":"2025-09-21T03:03:25.868993Z","shell.execute_reply.started":"2025-09-21T03:03:25.847666Z","shell.execute_reply":"2025-09-21T03:03:25.868193Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, pretrained_embed, n_hidden=2, hidden_size=64):\n        super().__init__()\n        weights = pretrained_embed.weight.data\n        self.embed = nn.Embedding.from_pretrained(weights,freeze=True)\n        embed_size = weights.shape[-1]\n        self.gru = nn.GRU(embed_size, hidden_size*2,num_layers=n_hidden,\n                          batch_first=True)\n        self.output = nn.Linear(hidden_size*2, weights.shape[0])\n\n    def forward(self, input_ids, hidden):\n        embeddings = self.embed(input_ids)\n        outputs, hidden = self.gru(embeddings, hidden)\n        logits = self.output(outputs)\n        return logits, hidden       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.869898Z","iopub.execute_input":"2025-09-21T03:03:25.870466Z","iopub.status.idle":"2025-09-21T03:03:25.889760Z","shell.execute_reply.started":"2025-09-21T03:03:25.870446Z","shell.execute_reply":"2025-09-21T03:03:25.888845Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()   \n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src_ids, src_mask, tgt_ids):\n        enc_outputs, enc_hidden = self.encoder(src_ids, src_mask)\n        logits, _ = self.decoder(tgt_ids, enc_hidden)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.892353Z","iopub.execute_input":"2025-09-21T03:03:25.892596Z","iopub.status.idle":"2025-09-21T03:03:25.906453Z","shell.execute_reply.started":"2025-09-21T03:03:25.892577Z","shell.execute_reply":"2025-09-21T03:03:25.905654Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import transformers\n\npretrained_model = AutoModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:25.907300Z","iopub.execute_input":"2025-09-21T03:03:25.907587Z","iopub.status.idle":"2025-09-21T03:03:26.591475Z","shell.execute_reply.started":"2025-09-21T03:03:25.907565Z","shell.execute_reply":"2025-09-21T03:03:26.590642Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\neng_train, eng_valid, es_train, es_valid = train_test_split(\n    eng_sentences, es_sentences, test_size = 0.20\n)\n\nbatch_size = 32\n\ntrain_dataset = TranslationDataset(eng_train, es_train, tokenizer)\nvalid_dataset = TranslationDataset(eng_valid, es_valid, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:26.592468Z","iopub.execute_input":"2025-09-21T03:03:26.592744Z","iopub.status.idle":"2025-09-21T03:03:26.665978Z","shell.execute_reply.started":"2025-09-21T03:03:26.592721Z","shell.execute_reply":"2025-09-21T03:03:26.665145Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import torchmetrics\n\ndef evaluate_tm(model, data_loader, metric, vocab_size):\n    model.eval()\n    metric.reset()\n    with torch.no_grad():\n        for batch in data_loader:\n            src_ids = batch[\"encoder_input_ids\"].to(device)\n            src_mask = batch[\"encoder_attention_mask\"].to(device)\n            tgt_ids = batch[\"decoder_input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            y_pred = model(src_ids, src_mask, tgt_ids)\n            metric.update(y_pred.view(-1,vocab_size), labels.view(-1))\n    return metric.compute()\n            \ndef train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs, vocab_size):\n    history = {\"train_losses\":[],\"train_metrics\":[],\"valid_metrics\":[]}\n    for epoch in range(n_epochs):\n        total_loss = 0\n        metric.reset()\n        model.train()\n        for idx, batch in enumerate(train_loader):\n            src_ids = batch[\"encoder_input_ids\"].to(device)\n            src_mask = batch[\"encoder_attention_mask\"].to(device)\n            tgt_ids = batch[\"decoder_input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            y_pred = model(src_ids, src_mask, tgt_ids)\n            loss = criterion(y_pred.view(-1,vocab_size), labels.view(-1))\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            metric.update(y_pred.view(-1,vocab_size), labels.view(-1))\n            print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n            print(f\", loss ={total_loss/(idx+1 ):.4f} \", end=\"\")\n        mean_loss = total_loss / len(train_loader)\n        history[\"train_losses\"].append(mean_loss)\n        history[\"train_metrics\"].append(metric.compute().item())\n        val_metric = evaluate_tm(model, valid_loader, metric,vocab_size).item()\n        history[\"valid_metrics\"].append(val_metric)\n        print(f\"Epoch:{epoch+1}/{n_epochs}, \"\n             f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n             f\"Train Metric: {history['train_metrics'][-1]:.4f}%, \"\n             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}%\")\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:26.666985Z","iopub.execute_input":"2025-09-21T03:03:26.670097Z","iopub.status.idle":"2025-09-21T03:03:26.696859Z","shell.execute_reply.started":"2025-09-21T03:03:26.670065Z","shell.execute_reply":"2025-09-21T03:03:26.696078Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"encoder = Encoder(pretrained_model.get_input_embeddings())\ndecoder = Decoder(pretrained_model.get_input_embeddings())\nnmt_model = Seq2Seq(encoder, decoder).to(device)\n\noptimizer = torch.optim.NAdam(nmt_model.parameters())\nxentropy = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\naccuracy = torchmetrics.Accuracy(task=\"multiclass\",num_classes = vocab_size).to(device)\nn_epochs=20\n\nhistory = train(nmt_model, optimizer, xentropy, accuracy, train_loader, valid_loader, n_epochs, vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:03:26.697770Z","iopub.execute_input":"2025-09-21T03:03:26.698132Z","iopub.status.idle":"2025-09-21T03:42:15.674060Z","shell.execute_reply.started":"2025-09-21T03:03:26.698107Z","shell.execute_reply":"2025-09-21T03:42:15.673352Z"}},"outputs":[{"name":"stdout","text":"Batch 2975/2975, loss =2.8201 Epoch:1/20, Train Loss: 2.8201, Train Metric: 0.1802%, Valid Metric: 0.2120%\nBatch 2975/2975, loss =1.8774 Epoch:2/20, Train Loss: 1.8774, Train Metric: 0.2266%, Valid Metric: 0.2314%\nBatch 2975/2975, loss =1.6286 Epoch:3/20, Train Loss: 1.6286, Train Metric: 0.2407%, Valid Metric: 0.2418%\nBatch 2975/2975, loss =1.4884 Epoch:4/20, Train Loss: 1.4884, Train Metric: 0.2488%, Valid Metric: 0.2474%\nBatch 2975/2975, loss =1.3950 Epoch:5/20, Train Loss: 1.3950, Train Metric: 0.2543%, Valid Metric: 0.2505%\nBatch 2975/2975, loss =1.3252 Epoch:6/20, Train Loss: 1.3252, Train Metric: 0.2587%, Valid Metric: 0.2543%\nBatch 2975/2975, loss =1.2721 Epoch:7/20, Train Loss: 1.2721, Train Metric: 0.2620%, Valid Metric: 0.2570%\nBatch 2975/2975, loss =1.2286 Epoch:8/20, Train Loss: 1.2286, Train Metric: 0.2648%, Valid Metric: 0.2589%\nBatch 2975/2975, loss =1.1936 Epoch:9/20, Train Loss: 1.1936, Train Metric: 0.2670%, Valid Metric: 0.2586%\nBatch 2975/2975, loss =1.1626 Epoch:10/20, Train Loss: 1.1626, Train Metric: 0.2689%, Valid Metric: 0.2618%\nBatch 2975/2975, loss =1.1362 Epoch:11/20, Train Loss: 1.1362, Train Metric: 0.2706%, Valid Metric: 0.2620%\nBatch 2975/2975, loss =1.1136 Epoch:12/20, Train Loss: 1.1136, Train Metric: 0.2720%, Valid Metric: 0.2636%\nBatch 2975/2975, loss =1.0929 Epoch:13/20, Train Loss: 1.0929, Train Metric: 0.2734%, Valid Metric: 0.2629%\nBatch 2975/2975, loss =1.0751 Epoch:14/20, Train Loss: 1.0751, Train Metric: 0.2745%, Valid Metric: 0.2625%\nBatch 2975/2975, loss =1.0585 Epoch:15/20, Train Loss: 1.0585, Train Metric: 0.2756%, Valid Metric: 0.2647%\nBatch 2975/2975, loss =1.0428 Epoch:16/20, Train Loss: 1.0428, Train Metric: 0.2766%, Valid Metric: 0.2657%\nBatch 2975/2975, loss =1.0292 Epoch:17/20, Train Loss: 1.0292, Train Metric: 0.2775%, Valid Metric: 0.2657%\nBatch 2975/2975, loss =1.0166 Epoch:18/20, Train Loss: 1.0166, Train Metric: 0.2782%, Valid Metric: 0.2659%\nBatch 2975/2975, loss =1.0059 Epoch:19/20, Train Loss: 1.0059, Train Metric: 0.2790%, Valid Metric: 0.2660%\nBatch 2975/2975, loss =0.9949 Epoch:20/20, Train Loss: 0.9949, Train Metric: 0.2797%, Valid Metric: 0.2662%\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import torch\nimport re\n\ndef translate(model, sentence, tokenizer, device=\"cpu\", max_len=30):\n    \"\"\"Simple, short translation function that works\"\"\"\n    model.eval()\n    with torch.no_grad():\n        # Encode input\n        src = tokenizer(sentence, return_tensors=\"pt\", padding=True, max_length=max_len)\n        src_ids = src[\"input_ids\"].to(device)\n        src_mask = src[\"attention_mask\"].to(device)\n        \n        # Get encoder output\n        _, hidden = model.encoder(src_ids, src_mask)\n        \n        # Start with a Spanish word\n        try:\n            start_token = tokenizer.encode(\"Hola\", add_special_tokens=False)[0]\n        except:\n            start_token = tokenizer.pad_token_id\n            \n        decoder_input = torch.tensor([[start_token]], device=device)\n        tokens = []\n        \n        # Generate tokens\n        for _ in range(15):\n            logits, hidden = model.decoder(decoder_input, hidden)\n            \n            # Get top 5 tokens and filter bad ones\n            top_tokens = torch.topk(logits[:, -1, :], 5)\n            \n            # Pick first good token\n            next_token = None\n            for token_id in top_tokens.indices[0]:\n                token_id = token_id.item()\n                if token_id not in [0, 1, 65000]:  # Skip EOS, UNK, PAD\n                    next_token = token_id\n                    break\n            \n            if next_token is None:\n                break\n                \n            tokens.append(next_token)\n            decoder_input = torch.cat([decoder_input, torch.tensor([[next_token]], device=device)], dim=1)\n            \n            # Stop at punctuation\n            decoded = tokenizer.decode([next_token])\n            if any(p in decoded for p in ['.', '!', '?']):\n                break\n        \n        # Clean and return\n        if tokens:\n            result = tokenizer.decode(tokens, skip_special_tokens=True)\n            result = re.sub(r'[<>▁]', ' ', result)  # Remove artifacts\n            result = re.sub(r'\\s+', ' ', result).strip()  # Fix spaces\n            return result.capitalize()\n        \n        return \"No translation\"\n\n# Quick test function\ndef test_model():\n    \"\"\"Test your model quickly\"\"\"\n    sentences = [\"Hello\", \"Thank you\", \"Good morning\", \"How are you?\", \"I love you\"]\n    \n    print(\"=== QUICK TRANSLATION TEST ===\")\n    for sent in sentences:\n        result = translate(nmt_model, sent, tokenizer, device)\n        print(f\"'{sent}' -> '{result}'\")\n\n# Ready to use!\nprint(\"Simple translator ready!\")\nprint(\"Usage: translate(nmt_model, 'Hello world', tokenizer, device)\")\nprint(\"Or run: test_model()\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:50:53.338281Z","iopub.execute_input":"2025-09-21T03:50:53.338961Z","iopub.status.idle":"2025-09-21T03:50:53.348373Z","shell.execute_reply.started":"2025-09-21T03:50:53.338938Z","shell.execute_reply":"2025-09-21T03:50:53.347589Z"}},"outputs":[{"name":"stdout","text":"Simple translator ready!\nUsage: translate(nmt_model, 'Hello world', tokenizer, device)\nOr run: test_model()\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# Single translation\nresult = translate(nmt_model, \"Hello world\", tokenizer, device)\nprint(result)\n\n# Test multiple sentences\ntest_model()\n\n# Interactive use\nsentence = input(\"English: \")\ntranslation = translate(nmt_model, sentence, tokenizer, device)\nprint(f\"Spanish: {translation}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:51:14.022903Z","iopub.execute_input":"2025-09-21T03:51:14.023514Z","iopub.status.idle":"2025-09-21T03:51:48.938738Z","shell.execute_reply.started":"2025-09-21T03:51:14.023489Z","shell.execute_reply":"2025-09-21T03:51:48.938130Z"}},"outputs":[{"name":"stdout","text":"S él.\n=== QUICK TRANSLATION TEST ===\n'Hello' -> 'Recomías.'\n'Thank you' -> 'Ses.'\n'Good morning' -> 'Palmiera.'\n'How are you?' -> 'Tú.'\n'I love you' -> 'S queso.'\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"English:  i like soccer\n"},{"name":"stdout","text":"Spanish: S blanco.\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Test the improved functions\ntest_fixed_translations()\n\n# Or try individual translations:\nprint(\"QUICK TESTS:\")\nsentences = [\"Hello\", \"Thank you\", \"Good morning\", \"I like soccer\"]\nfor sent in sentences:\n    result = simple_clean_translate(nmt_model, sent, tokenizer, device)\n    print(f\"'{sent}' -> '{result}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T03:52:57.893334Z","iopub.execute_input":"2025-09-21T03:52:57.894006Z","iopub.status.idle":"2025-09-21T03:52:58.154109Z","shell.execute_reply.started":"2025-09-21T03:52:57.893982Z","shell.execute_reply":"2025-09-21T03:52:58.153569Z"}},"outputs":[{"name":"stdout","text":"=== TESTING IMPROVED TRANSLATIONS ===\nMethod 1: Properly Fixed | Method 2: Token Filtering\n============================================================\nEnglish: Hello\n  Fixed:    'Holice fue.'\n  Filtered: 'Recomías.'\n\nEnglish: Good morning\n  Fixed:    'Yo marca hasta una trampa.'\n  Filtered: 'Palmiera.'\n\nEnglish: Thank you\n  Fixed:    'Esras personas.'\n  Filtered: 'Ses.'\n\nEnglish: How are you?\n  Fixed:    'Hol tú.'\n  Filtered: 'S listás.'\n\nEnglish: I like soccer\n  Fixed:    'El Yo está barriendo.'\n  Filtered: 'S Cuégi.'\n\nEnglish: What is your name?\n  Fixed:    'El ha escrito.'\n  Filtered: 'S Cuábacas?'\n\nEnglish: I am fine\n  Fixed:    'Hols Me fueron.'\n  Filtered: 'S» jugue.'\n\nEnglish: Goodbye\n  Fixed:    'Hol favor.'\n  Filtered: 'Mente.'\n\nQUICK TESTS:\n'Hello' -> 'Hol recomías.'\n'Thank you' -> 'Holses.'\n'Good morning' -> 'Hol palmiera.'\n'I like soccer' -> 'Hols> él.'\n","output_type":"stream"}],"execution_count":50}]}