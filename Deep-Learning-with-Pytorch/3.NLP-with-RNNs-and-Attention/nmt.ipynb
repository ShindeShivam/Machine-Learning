{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13114139,"sourceType":"datasetVersion","datasetId":8307286}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport numpy as np\nimport pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:22.114191Z","iopub.execute_input":"2025-09-20T07:39:22.114534Z","iopub.status.idle":"2025-09-20T07:39:22.118605Z","shell.execute_reply.started":"2025-09-20T07:39:22.114510Z","shell.execute_reply":"2025-09-20T07:39:22.117870Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = \"cuda\"\n    print(torch.cuda.device_count())\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:22.119718Z","iopub.execute_input":"2025-09-20T07:39:22.120087Z","iopub.status.idle":"2025-09-20T07:39:22.139030Z","shell.execute_reply.started":"2025-09-20T07:39:22.120071Z","shell.execute_reply":"2025-09-20T07:39:22.138323Z"}},"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"},{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":119},{"cell_type":"code","source":"data = []\nfilename = \"/kaggle/input/eng-spa/spa.txt\"\nwith open(filename, \"r\") as f:\n    for line in f:\n        data.append(line.strip())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:22.139821Z","iopub.execute_input":"2025-09-20T07:39:22.140007Z","iopub.status.idle":"2025-09-20T07:39:22.195359Z","shell.execute_reply.started":"2025-09-20T07:39:22.139988Z","shell.execute_reply":"2025-09-20T07:39:22.194888Z"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":"cleaned_data = [line.replace(\"¡\", \"\").replace(\"¿\", \"\") for line in data]\npairs = [line.split(\"\\t\") for line in cleaned_data]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:22.196584Z","iopub.execute_input":"2025-09-20T07:39:22.196770Z","iopub.status.idle":"2025-09-20T07:39:22.347891Z","shell.execute_reply.started":"2025-09-20T07:39:22.196756Z","shell.execute_reply":"2025-09-20T07:39:22.347319Z"}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"np.random.shuffle(pairs)\neng_sentences, es_sentences = zip(*pairs)\nfor i in range(3):\n    print(eng_sentences[i], \"==>\" ,es_sentences[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:22.348616Z","iopub.execute_input":"2025-09-20T07:39:22.348822Z","iopub.status.idle":"2025-09-20T07:39:22.931179Z","shell.execute_reply.started":"2025-09-20T07:39:22.348806Z","shell.execute_reply":"2025-09-20T07:39:22.930587Z"}},"outputs":[{"name":"stdout","text":"It's impossible not to be fascinated by her beauty. ==> Es imposible no fascinarse por su belleza.\nAre you on Facebook? ==> Tienes Facebook?\nHe had his car stolen in that parking lot. ==> Le robaron el coche en ese aparcamiento.\n","output_type":"stream"}],"execution_count":122},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:22.931971Z","iopub.execute_input":"2025-09-20T07:39:22.932661Z","iopub.status.idle":"2025-09-20T07:39:23.301194Z","shell.execute_reply.started":"2025-09-20T07:39:22.932639Z","shell.execute_reply":"2025-09-20T07:39:23.300660Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"max_len = 100\ndef encode_with_gpt2(sentence, add_sos_and_eos=False):\n    \n    texts = [f\"<s> {s} </s>\" if add_sos_and_eos else s for s in sentence]\n    encodings = tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_len,\n        return_tensors=\"pt\"\n    )\n    return encodings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:23.302048Z","iopub.execute_input":"2025-09-20T07:39:23.302249Z","iopub.status.idle":"2025-09-20T07:39:23.306399Z","shell.execute_reply.started":"2025-09-20T07:39:23.302233Z","shell.execute_reply":"2025-09-20T07:39:23.305759Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass TranslationDataset(Dataset):\n    def __init__(self, src_sentences, tgt_sentences, tokenizer, max_len=500):\n        self.src_sentences = src_sentences\n        self.tgt_sentences = tgt_sentences\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.src_sentences)\n\n    def __getitem__(self,idx):\n        src = self.src_sentences[idx]\n        tgt = f\"<s> {self.tgt_sentences[idx]} </s>\"\n\n        src_enc = self.tokenizer(\n            src,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\")\n        tgt_enc = self.tokenizer(\n            tgt,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\")\n\n        decoder_input_ids = tgt_enc[\"input_ids\"][:,:-1].squeeze(0)\n        labels = tgt_enc[\"input_ids\"][:,1:].squeeze(0)\n\n        return {\n            \"encoder_input_ids\":src_enc[\"input_ids\"].squeeze(0),\n            \"encoder_attention_mask\":src_enc[\"attention_mask\"].squeeze(0),\n            \"decoder_input_ids\":decoder_input_ids,\n            \"labels\":labels\n        } ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:23.307466Z","iopub.execute_input":"2025-09-20T07:39:23.307707Z","iopub.status.idle":"2025-09-20T07:39:23.323974Z","shell.execute_reply.started":"2025-09-20T07:39:23.307677Z","shell.execute_reply":"2025-09-20T07:39:23.323239Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass Encoder(nn.Module):\n    def __init__(self, pretrained_embed, n_hidden=2, hidden_size=128,dropout=0.2):\n        super().__init__()\n        weights = pretrained_embed.weight.data\n        self.embed = nn.Embedding.from_pretrained(weights,freeze=True)\n        embed_size = weights.shape[-1]\n        self.gru = nn.GRU(embed_size, hidden_size, num_layers=n_hidden,\n                         batch_first=True, dropout=dropout, bidirectional=True)\n        self.n_hidden = n_hidden\n\n    def forward(self, input_ids, attention_mask):\n        embeddings = self.embed(input_ids)\n        lengths = attention_mask.sum(dim=1)\n        packed = pack_padded_sequence(embeddings,\n                                     lengths = lengths.cpu(),\n                                     batch_first=True,\n                                     enforce_sorted=False)\n        outputs, hidden = self.gru(packed)\n        outputs, _ = pad_packed_sequence(outputs, batch_first=True, total_length=input_ids.size(1))\n        #########################\n        batch_size = hidden.size(1)\n        hidden = hidden.view(self.n_hidden, 2, batch_size, -1)\n        hidden = torch.cat([hidden[:, 0, :, :], hidden[:, 1, :, :]], dim=2)\n       \n       \n        return outputs, hidden    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:23.325755Z","iopub.execute_input":"2025-09-20T07:39:23.326163Z","iopub.status.idle":"2025-09-20T07:39:23.345279Z","shell.execute_reply.started":"2025-09-20T07:39:23.326148Z","shell.execute_reply":"2025-09-20T07:39:23.344708Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, pretrained_embed, n_hidden=2, hidden_size=128):\n        super().__init__()\n        weights = pretrained_embed.weight.data\n        self.embed = nn.Embedding.from_pretrained(weights,freeze=True)\n        embed_size = weights.shape[-1]\n        self.gru = nn.GRU(embed_size, hidden_size*2,num_layers=n_hidden,\n                          batch_first=True)\n        self.output = nn.Linear(hidden_size*2, weights.shape[0])\n\n    def forward(self, input_ids, hidden):\n        embeddings = self.embed(input_ids)\n        outputs, hidden = self.gru(embeddings, hidden)\n        logits = self.output(outputs)\n        return logits, hidden       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:23.345918Z","iopub.execute_input":"2025-09-20T07:39:23.346073Z","iopub.status.idle":"2025-09-20T07:39:23.362988Z","shell.execute_reply.started":"2025-09-20T07:39:23.346060Z","shell.execute_reply":"2025-09-20T07:39:23.362464Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()   \n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src_ids, src_mask, tgt_ids):\n        enc_outputs, enc_hidden = self.encoder(src_ids, src_mask)\n        logits, _ = self.decoder(tgt_ids, enc_hidden)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:23.363670Z","iopub.execute_input":"2025-09-20T07:39:23.363840Z","iopub.status.idle":"2025-09-20T07:39:23.378897Z","shell.execute_reply.started":"2025-09-20T07:39:23.363827Z","shell.execute_reply":"2025-09-20T07:39:23.378333Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"import transformers\n\ngpt_model = transformers.AutoModel.from_pretrained(\"gpt2\")\nvocab_size = gpt_model.get_input_embeddings().weight.data.shape[0]\nvocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:23.379613Z","iopub.execute_input":"2025-09-20T07:39:23.379848Z","iopub.status.idle":"2025-09-20T07:39:23.568363Z","shell.execute_reply.started":"2025-09-20T07:39:23.379829Z","shell.execute_reply":"2025-09-20T07:39:23.567685Z"}},"outputs":[{"execution_count":129,"output_type":"execute_result","data":{"text/plain":"50257"},"metadata":{}}],"execution_count":129},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\neng_train, eng_valid, es_train, es_valid = train_test_split(\n    eng_sentences, es_sentences, test_size = 0.20\n)\n\nbatch_size = 32\n\ntrain_dataset = TranslationDataset(eng_train, es_train, tokenizer)\nvalid_dataset = TranslationDataset(eng_valid, es_valid, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:23.569028Z","iopub.execute_input":"2025-09-20T07:39:23.569212Z","iopub.status.idle":"2025-09-20T07:39:23.632299Z","shell.execute_reply.started":"2025-09-20T07:39:23.569196Z","shell.execute_reply":"2025-09-20T07:39:23.631809Z"}},"outputs":[],"execution_count":130},{"cell_type":"code","source":"import torchmetrics\n\ndef evaluate_tm(model, data_loader, metric, vocab_size):\n    model.eval()\n    metric.reset()\n    with torch.no_grad():\n        for batch in data_loader:\n            src_ids = batch[\"encoder_input_ids\"].to(device)\n            src_mask = batch[\"encoder_attention_mask\"].to(device)\n            tgt_ids = batch[\"decoder_input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            y_pred = model(src_ids, src_mask, tgt_ids)\n            metric.update(y_pred.view(-1,vocab_size), labels.view(-1))\n    return metric.compute\n            \ndef train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs, vocab_size):\n    history = {\"train_losses\":[],\"train_metrics\":[],\"valid_metrics\":[]}\n    for epoch in range(n_epochs):\n        total_loss = 0\n        metric.reset()\n        model.train()\n        for idx, batch in enumerate(train_loader):\n            src_ids = batch[\"encoder_input_ids\"].to(device)\n            src_mask = batch[\"encoder_attention_mask\"].to(device)\n            tgt_ids = batch[\"decoder_input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            y_pred = model(src_ids, src_mask, tgt_ids)\n            loss = criterion(y_pred.view(-1,vocab_size), labels.view(-1))\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            metric.update(y_pred.view(-1,vocab_size), labels.view(-1))\n            print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n            print(f\", loss ={total_loss/(idx+1 ):.4f} \", end=\"\")\n        mean_loss = total_loss / len(train_loader)\n        history[\"train_losses\"].append(mean_loss)\n        history[\"train_metrics\"].append(metric.compute().item())\n        val_metric = evaluate_tm(model, valid_loader, metric).item()\n        history[\"valid_metrics\"].append(val_metric)\n        print(f\"Epoch:{epoch+1}/{n_epochs}, \"\n             f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n             f\"Train Metric: {history['train_metrics'][-1]:.4f}%, \"\n             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}%\")\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:23.632971Z","iopub.execute_input":"2025-09-20T07:39:23.633137Z","iopub.status.idle":"2025-09-20T07:39:23.648719Z","shell.execute_reply.started":"2025-09-20T07:39:23.633123Z","shell.execute_reply":"2025-09-20T07:39:23.648159Z"}},"outputs":[],"execution_count":131},{"cell_type":"code","source":"encoder = Encoder(gpt_model.get_input_embeddings())\ndecoder = Decoder(gpt_model.get_input_embeddings())\n\nnmt_model = Seq2Seq(encoder, decoder).to(device)\n\noptimizer = torch.optim.NAdam(nmt_model.parameters())\nxentropy = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\naccuracy = torchmetrics.Accuracy(task=\"multiclass\",num_classes = vocab_size).to(device)\nn_epochs=20\n\nhistory = train(nmt_model, optimizer, xentropy, accuracy, train_loader, valid_loader, n_epochs, vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T07:39:23.649343Z","iopub.execute_input":"2025-09-20T07:39:23.649537Z","iopub.status.idle":"2025-09-20T07:39:24.056767Z","shell.execute_reply.started":"2025-09-20T07:39:23.649521Z","shell.execute_reply":"2025-09-20T07:39:24.055589Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1401571680.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/1955920520.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs, vocab_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/78868229.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_ids, src_mask, tgt_ids)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0menc_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/940545107.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, hidden)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.99 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.53 GiB is free. Process 2944 has 14.36 GiB memory in use. Of the allocated memory 13.52 GiB is allocated by PyTorch, and 549.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.99 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.53 GiB is free. Process 2944 has 14.36 GiB memory in use. Of the allocated memory 13.52 GiB is allocated by PyTorch, and 549.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":132}]}