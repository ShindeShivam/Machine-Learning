{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Generating Fake Shakespeare Text","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.319338Z","iopub.execute_input":"2025-09-16T06:50:36.320067Z","iopub.status.idle":"2025-09-16T06:50:36.323812Z","shell.execute_reply.started":"2025-09-16T06:50:36.320038Z","shell.execute_reply":"2025-09-16T06:50:36.323030Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(torch.cuda.device_count())\n    device = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.325151Z","iopub.execute_input":"2025-09-16T06:50:36.325537Z","iopub.status.idle":"2025-09-16T06:50:36.345937Z","shell.execute_reply.started":"2025-09-16T06:50:36.325512Z","shell.execute_reply":"2025-09-16T06:50:36.345085Z"},"trusted":true},"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from pathlib import Path\nimport urllib.request\n\ndef download_shakespeare_text():\n    path = Path(\"datasets/shakespeare/shakespeare.txt\")\n    if not path.is_file():\n        path.parent.mkdir(parents=True, exist_ok=True)\n        url = \"https://homl.info/shakespeare\"\n        urllib.request.urlretrieve(url, path)\n    return path.read_text()\nshakespeare_text = download_shakespeare_text()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.347189Z","iopub.execute_input":"2025-09-16T06:50:36.347816Z","iopub.status.idle":"2025-09-16T06:50:36.362974Z","shell.execute_reply.started":"2025-09-16T06:50:36.347794Z","shell.execute_reply":"2025-09-16T06:50:36.362458Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"print(shakespeare_text[:100])","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.363682Z","iopub.execute_input":"2025-09-16T06:50:36.363939Z","iopub.status.idle":"2025-09-16T06:50:36.379546Z","shell.execute_reply.started":"2025-09-16T06:50:36.363924Z","shell.execute_reply":"2025-09-16T06:50:36.378850Z"},"trusted":true},"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"vocab = sorted(set(shakespeare_text.lower()))\n''.join(vocab)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.381104Z","iopub.execute_input":"2025-09-16T06:50:36.381598Z","iopub.status.idle":"2025-09-16T06:50:36.408725Z","shell.execute_reply.started":"2025-09-16T06:50:36.381571Z","shell.execute_reply":"2025-09-16T06:50:36.408039Z"},"trusted":true},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"char_to_id = {char:idx for idx, char in enumerate(vocab)}\nid_to_char = {idx:char for idx, char in enumerate(vocab)}","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.409328Z","iopub.execute_input":"2025-09-16T06:50:36.409511Z","iopub.status.idle":"2025-09-16T06:50:36.425268Z","shell.execute_reply.started":"2025-09-16T06:50:36.409475Z","shell.execute_reply":"2025-09-16T06:50:36.424546Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"char_to_id[\"a\"]","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.425991Z","iopub.execute_input":"2025-09-16T06:50:36.426193Z","iopub.status.idle":"2025-09-16T06:50:36.442930Z","shell.execute_reply.started":"2025-09-16T06:50:36.426178Z","shell.execute_reply":"2025-09-16T06:50:36.442331Z"},"trusted":true},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"13"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"id_to_char[13]","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.443809Z","iopub.execute_input":"2025-09-16T06:50:36.444052Z","iopub.status.idle":"2025-09-16T06:50:36.460883Z","shell.execute_reply.started":"2025-09-16T06:50:36.444027Z","shell.execute_reply":"2025-09-16T06:50:36.460150Z"},"trusted":true},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'a'"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"def encode_text(text):\n    return torch.tensor([char_to_id[char] for char in text.lower()])\ndef decode_text(char_ids):\n    return ''.join([id_to_char[char_id.item()] for char_id in char_ids])","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.462288Z","iopub.execute_input":"2025-09-16T06:50:36.462725Z","iopub.status.idle":"2025-09-16T06:50:36.479355Z","shell.execute_reply.started":"2025-09-16T06:50:36.462708Z","shell.execute_reply":"2025-09-16T06:50:36.478657Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"encoded = encode_text(\"hello world\")\nencoded\n","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.480020Z","iopub.execute_input":"2025-09-16T06:50:36.480190Z","iopub.status.idle":"2025-09-16T06:50:36.502118Z","shell.execute_reply.started":"2025-09-16T06:50:36.480177Z","shell.execute_reply":"2025-09-16T06:50:36.501362Z"},"trusted":true},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor([20, 17, 24, 24, 27,  1, 35, 27, 30, 24, 16])"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"decode_text(encoded)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.503687Z","iopub.execute_input":"2025-09-16T06:50:36.504311Z","iopub.status.idle":"2025-09-16T06:50:36.517917Z","shell.execute_reply.started":"2025-09-16T06:50:36.504291Z","shell.execute_reply":"2025-09-16T06:50:36.517256Z"},"trusted":true},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'hello world'"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"class TimeSeriesDatasetBuilder:\n    def __init__(self, series, window_length=56):\n        self.encoded_text = encode_text(series)\n        self.window_length = window_length\n   \n    def create_X_y(self):\n        X, y =[],[]\n        for i in range(len(self.encoded_text) - self.window_length):\n            window = self.encoded_text[i:i+self.window_length]\n            future = self.encoded_text[i+1:i+self.window_length+1]\n            X.append(window)\n            y.append(future)\n        return np.array(X),np.array(y)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.518611Z","iopub.execute_input":"2025-09-16T06:50:36.518812Z","iopub.status.idle":"2025-09-16T06:50:36.533071Z","shell.execute_reply.started":"2025-09-16T06:50:36.518796Z","shell.execute_reply":"2025-09-16T06:50:36.532392Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"to_be_dataset = TimeSeriesDatasetBuilder(series=\"to be or not to be\", window_length=10)\nX,y = to_be_dataset.create_X_y()\nfor i in range(len(X)):\n    decoded_x = decode_text(X[i])\n    decoded_y = decode_text(y[i])\n    print(f\"x : {decoded_x}\")\n    print(f\"y : {decoded_y}\")","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.533894Z","iopub.execute_input":"2025-09-16T06:50:36.534108Z","iopub.status.idle":"2025-09-16T06:50:36.554209Z","shell.execute_reply.started":"2025-09-16T06:50:36.534085Z","shell.execute_reply":"2025-09-16T06:50:36.553467Z"},"trusted":true},"outputs":[{"name":"stdout","text":"x : to be or n\ny : o be or no\nx : o be or no\ny :  be or not\nx :  be or not\ny : be or not \nx : be or not \ny : e or not t\nx : e or not t\ny :  or not to\nx :  or not to\ny : or not to \nx : or not to \ny : r not to b\nx : r not to b\ny :  not to be\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"window_length = 56\nbatch_size = 1024 \nbuilder = TimeSeriesDatasetBuilder(shakespeare_text,window_length)\nX, y = builder.create_X_y()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:36.554960Z","iopub.execute_input":"2025-09-16T06:50:36.555190Z","iopub.status.idle":"2025-09-16T06:50:58.095152Z","shell.execute_reply.started":"2025-09-16T06:50:36.555168Z","shell.execute_reply":"2025-09-16T06:50:58.094248Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T06:50:58.096174Z","iopub.execute_input":"2025-09-16T06:50:58.096721Z","iopub.status.idle":"2025-09-16T06:50:58.101198Z","shell.execute_reply.started":"2025-09-16T06:50:58.096695Z","shell.execute_reply":"2025-09-16T06:50:58.100468Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"(1115338, 56)"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T06:50:58.104787Z","iopub.execute_input":"2025-09-16T06:50:58.104981Z","iopub.status.idle":"2025-09-16T06:50:58.120354Z","shell.execute_reply.started":"2025-09-16T06:50:58.104967Z","shell.execute_reply":"2025-09-16T06:50:58.119869Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"(1115338, 56)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\nX_tensor = torch.tensor(X, dtype = torch.long)\ny_tensor = torch.tensor(y, dtype = torch.long).squeeze(-1)\n\ntrain_set = TensorDataset(X_tensor[:1_000_000], y_tensor[:1_000_000])\nvalid_set = TensorDataset(X_tensor[1_000_000:1_060_000],y_tensor[1_000_000:1_060_000])\ntest_set = TensorDataset(X_tensor[1_060_000:], y_tensor[1_060_000:])\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, \n                          num_workers=2, pin_memory=True)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size,\n                         num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_set, batch_size=batch_size,\n                        num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:58.120967Z","iopub.execute_input":"2025-09-16T06:50:58.121189Z","iopub.status.idle":"2025-09-16T06:50:58.238467Z","shell.execute_reply.started":"2025-09-16T06:50:58.121166Z","shell.execute_reply":"2025-09-16T06:50:58.237697Z"},"trusted":true},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import torchmetrics\n\ndef evaluate_tm(model, data_loader, metric):\n    model.eval()\n    metric.reset()\n    with torch.no_grad():\n        for X_batch, y_batch in data_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_pred = model(X_batch)\n            metric.update(y_pred, y_batch)\n    return metric.compute()\n\ndef train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs, patience=2,\n         factor=0.5,epoch_callback=None):\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"max\", patience=patience, factor=factor\n    )\n    history = {\"train_losses\":[],\"train_metrics\":[],\"valid_metrics\":[]}\n    for epoch in range(n_epochs):\n        total_loss = 0\n        metric.reset()\n        model.train()\n        if epoch_callback is not None:\n            epoch_callback(model,epoch)\n        for idx,( X_batch, y_batch) in enumerate(train_loader):\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_pred = model(X_batch)\n            loss = criterion(y_pred, y_batch)\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            metric.update(y_pred, y_batch)\n            print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n            print(f\", loss ={total_loss/(idx+1 ):.4f} \", end=\"\")\n        mean_loss = total_loss / len(train_loader)\n        history[\"train_losses\"].append(mean_loss)\n        history[\"train_metrics\"].append(metric.compute().item())\n        val_metric = evaluate_tm(model, valid_loader, metric).item()\n        history[\"valid_metrics\"].append(val_metric)\n        scheduler.step(val_metric)\n        print(f\"Epoch:{epoch+1}/{n_epochs}, \"\n             f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n             f\"Train Metric: {history['train_metrics'][-1]:.4f}, \"\n             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}\")\n    return history","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:58.239293Z","iopub.execute_input":"2025-09-16T06:50:58.239512Z","iopub.status.idle":"2025-09-16T06:50:58.247987Z","shell.execute_reply.started":"2025-09-16T06:50:58.239485Z","shell.execute_reply":"2025-09-16T06:50:58.247429Z"},"trusted":true},"outputs":[],"execution_count":38},{"cell_type":"code","source":"class ShakespeareModel(nn.Module):\n    def __init__(self, vocab_size, n_hidden=128, n_layers=2, embed_size=20, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.gru = nn.GRU(embed_size, n_hidden, num_layers=n_layers,\n                         batch_first=True, dropout=dropout)\n        self.output = nn.Linear(n_hidden, vocab_size)\n        \n    def forward(self, X):\n        embeddings = self.embed(X)\n        outputs, _states = self.gru(embeddings)\n        return self.output(outputs).permute(0,2,1)\nmodel = ShakespeareModel(len(vocab)).to(device)\nif torch.cuda.device_count()>1:\n    print(\"Using\", torch.cuda.device_count(),\"GPU's\")\n    model = nn.DataParallel(model)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:58.249174Z","iopub.execute_input":"2025-09-16T06:50:58.249516Z","iopub.status.idle":"2025-09-16T06:50:58.278240Z","shell.execute_reply.started":"2025-09-16T06:50:58.249475Z","shell.execute_reply":"2025-09-16T06:50:58.277566Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using 2 GPU's\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): ShakespeareModel(\n    (embed): Embedding(39, 20)\n    (gru): GRU(20, 128, num_layers=2, batch_first=True, dropout=0.1)\n    (output): Linear(in_features=128, out_features=39, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"n_epochs = 20\nxentropy = nn.CrossEntropyLoss()\naccuracy = torchmetrics.Accuracy(task=\"multiclass\",num_classes=len(vocab)).to(device)\noptimizer = torch.optim.NAdam(model.parameters())\n\nhistory = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader, n_epochs)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:50:58.278903Z","iopub.execute_input":"2025-09-16T06:50:58.279142Z","iopub.status.idle":"2025-09-16T06:58:07.001031Z","shell.execute_reply.started":"2025-09-16T06:50:58.279117Z","shell.execute_reply":"2025-09-16T06:58:07.000050Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Batch 977/977, loss =1.7122Epoch:1/20, Train Loss: 1.7122, Train Metric: 0.4861, Valid Metric: 0.5340\nBatch 977/977, loss =1.4012Epoch:2/20, Train Loss: 1.4012, Train Metric: 0.5633, Valid Metric: 0.5495\nBatch 977/977, loss =1.3568Epoch:3/20, Train Loss: 1.3568, Train Metric: 0.5742, Valid Metric: 0.5529\nBatch 977/977, loss =1.3353Epoch:4/20, Train Loss: 1.3353, Train Metric: 0.5795, Valid Metric: 0.5543\nBatch 977/977, loss =1.3225Epoch:5/20, Train Loss: 1.3225, Train Metric: 0.5827, Valid Metric: 0.5530\nBatch 977/977, loss =1.3141Epoch:6/20, Train Loss: 1.3141, Train Metric: 0.5849, Valid Metric: 0.5520\nBatch 977/977, loss =1.3082Epoch:7/20, Train Loss: 1.3082, Train Metric: 0.5865, Valid Metric: 0.5530\nBatch 977/977, loss =1.2995Epoch:8/20, Train Loss: 1.2995, Train Metric: 0.5889, Valid Metric: 0.5535\nBatch 977/977, loss =1.2971Epoch:9/20, Train Loss: 1.2971, Train Metric: 0.5896, Valid Metric: 0.5533\nBatch 977/977, loss =1.2951Epoch:10/20, Train Loss: 1.2951, Train Metric: 0.5902, Valid Metric: 0.5528\nBatch 977/977, loss =1.2910Epoch:11/20, Train Loss: 1.2910, Train Metric: 0.5913, Valid Metric: 0.5524\nBatch 977/977, loss =1.2901Epoch:12/20, Train Loss: 1.2901, Train Metric: 0.5916, Valid Metric: 0.5525\nBatch 977/977, loss =1.2892Epoch:13/20, Train Loss: 1.2892, Train Metric: 0.5918, Valid Metric: 0.5520\nBatch 977/977, loss =1.2871Epoch:14/20, Train Loss: 1.2871, Train Metric: 0.5925, Valid Metric: 0.5526\nBatch 977/977, loss =1.2865Epoch:15/20, Train Loss: 1.2865, Train Metric: 0.5926, Valid Metric: 0.5525\nBatch 977/977, loss =1.2861Epoch:16/20, Train Loss: 1.2861, Train Metric: 0.5927, Valid Metric: 0.5523\nBatch 977/977, loss =1.2851Epoch:17/20, Train Loss: 1.2851, Train Metric: 0.5931, Valid Metric: 0.5525\nBatch 977/977, loss =1.2849Epoch:18/20, Train Loss: 1.2849, Train Metric: 0.5931, Valid Metric: 0.5527\nBatch 977/977, loss =1.2846Epoch:19/20, Train Loss: 1.2846, Train Metric: 0.5932, Valid Metric: 0.5528\nBatch 977/977, loss =1.2841Epoch:20/20, Train Loss: 1.2841, Train Metric: 0.5933, Valid Metric: 0.5527\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# Critical Shape Journey\n\n# X (Input) Path:\n\n#→ windowing → [1115294, 56] \n#→ batching → [1024, 56]\n#→ embed() → [1024, 56, 10]\n#→ GRU() → [1024, 56, 128] \n#→ [:, -1, :] → [1024, 128]\n#→ Linear() → [1024, 39]\n\n# y (Target) Path:\n\n# → windowing → [1115294, 1] \n# → squeeze(-1) → [1115294]\n# → batching → [1024]\n# → CrossEntropyLoss with [1024, 39]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T06:58:07.002120Z","iopub.execute_input":"2025-09-16T06:58:07.002342Z","iopub.status.idle":"2025-09-16T06:58:07.006584Z","shell.execute_reply.started":"2025-09-16T06:58:07.002320Z","shell.execute_reply":"2025-09-16T06:58:07.005798Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"torch.save(model.state_dict(), \"my_shakespeare_model.pt\")","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:58:07.007433Z","iopub.execute_input":"2025-09-16T06:58:07.007964Z","iopub.status.idle":"2025-09-16T06:58:07.032310Z","shell.execute_reply.started":"2025-09-16T06:58:07.007936Z","shell.execute_reply":"2025-09-16T06:58:07.031730Z"},"trusted":true},"outputs":[],"execution_count":42},{"cell_type":"code","source":"text = \"To be or not to b\"\nencoded_text = encode_text(text).unsqueeze(dim=0).to(device)\nencoded_text.shape","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:58:07.033051Z","iopub.execute_input":"2025-09-16T06:58:07.033279Z","iopub.status.idle":"2025-09-16T06:58:07.047866Z","shell.execute_reply.started":"2025-09-16T06:58:07.033261Z","shell.execute_reply":"2025-09-16T06:58:07.047257Z"},"trusted":true},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 17])"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    y_logits = model(encoded_text)\n    predicted_char_id = y_logits[0,:,-1].argmax().item()\n    predicted_char = id_to_char[predicted_char_id]\npredicted_char","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:58:07.048659Z","iopub.execute_input":"2025-09-16T06:58:07.048852Z","iopub.status.idle":"2025-09-16T06:58:07.145351Z","shell.execute_reply.started":"2025-09-16T06:58:07.048830Z","shell.execute_reply":"2025-09-16T06:58:07.144232Z"},"trusted":true},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"'e'"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"def next_char(model, text, temperature=0.7):\n    model.eval()\n    encoded_text = encode_text(text).unsqueeze(0).to(device)\n    with torch.no_grad():\n        y_logits = model(encoded_text)\n        y_probas = torch.softmax(y_logits[0,:,-1]/temperature, dim=-1)\n        predicted_char_id = torch.multinomial(y_probas,num_samples=1).item()     \n        return id_to_char[predicted_char_id]\n\n","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:58:07.146250Z","iopub.execute_input":"2025-09-16T06:58:07.146557Z","iopub.status.idle":"2025-09-16T06:58:07.152408Z","shell.execute_reply.started":"2025-09-16T06:58:07.146527Z","shell.execute_reply":"2025-09-16T06:58:07.151532Z"},"trusted":true},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import time\ndef generate_text(model, text, n_chars=100,temperature=0.7):\n    print(text, end='', flush=True)\n    for _ in range(n_chars):\n        char = next_char(model, text, temperature)\n        text += char\n        print(char, end='', flush=True)\n        time.sleep(0.01)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:58:07.154064Z","iopub.execute_input":"2025-09-16T06:58:07.154293Z","iopub.status.idle":"2025-09-16T06:58:07.178617Z","shell.execute_reply.started":"2025-09-16T06:58:07.154277Z","shell.execute_reply":"2025-09-16T06:58:07.177556Z"},"trusted":true},"outputs":[],"execution_count":46},{"cell_type":"code","source":"print(generate_text(model,\"To be or not to b\",n_chars=500 ))","metadata":{"execution":{"iopub.status.busy":"2025-09-16T06:58:07.179589Z","iopub.execute_input":"2025-09-16T06:58:07.180057Z","iopub.status.idle":"2025-09-16T06:58:14.225469Z","shell.execute_reply.started":"2025-09-16T06:58:07.180027Z","shell.execute_reply":"2025-09-16T06:58:14.224715Z"},"trusted":true},"outputs":[{"name":"stdout","text":"To be or not to be such as shall be so.\n\nescalus:\nno more of thy gates, our company both in good crown?\n\nking richard iii:\ni shall be show thee to me, and then my state to be a\ncatesby with protectors of my traitor\nwhere he shall be make me to pardon me.\n\nwarwick:\nwhy, then what say you well mock your\nsons and from her father's uncle, i thank'd with the laurence:\nshe has the leans of the sun, what all, sir, if you should think\nthe cause that they shall be to seek the roar upon the thing\nand the report to englandNone\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## Statefull RNN","metadata":{}},{"cell_type":"code","source":"class DatasetBuilderStateful:\n    def __init__(self, series,batch_size, window_length=56):\n        self.encoded_text = encode_text(series)\n        self.window_length = window_length\n        self.batch_size = batch_size\n\n        #total number of full windows\n        self.n_consecutive_windows = (len(self.encoded_text) - 1) // self.window_length\n        #windows per stream\n        self.n_windows_per_stream = self.n_consecutive_windows // self.batch_size\n        #spacing between streams\n        self.spacing = self.n_windows_per_stream * self.window_length\n        #Total samples\n        self.length = self.n_windows_per_stream * self.batch_size\n                \n    def create_X_y(self):\n        X, y =[],[]\n        for i in range(self.length):\n            slot = i % self.batch_size  # stream index\n            window_no_in_slot = i // self.batch_size\n\n            start = slot * self.spacing + window_no_in_slot * self.window_length\n            end = start + self.window_length\n            window = self.encoded_text[start:end]\n            future = self.encoded_text[start+1:end+1]\n            X.append(window)\n            y.append(future)\n        return np.array(X),np.array(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:29:14.950930Z","iopub.execute_input":"2025-09-16T07:29:14.951208Z","iopub.status.idle":"2025-09-16T07:29:14.957641Z","shell.execute_reply.started":"2025-09-16T07:29:14.951187Z","shell.execute_reply":"2025-09-16T07:29:14.956892Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"batch_size =1024\n\ntext_len = len(shakespeare_text)\n\ntrain_text = shakespeare_text[:int(0.9 * text_len)]\nvalid_text = shakespeare_text[int(0.9 * text_len):int(text_len)]\ntrain_builder = DatasetBuilderStateful(train_text, batch_size=batch_size)\nvalid_builder = DatasetBuilderStateful(valid_text, batch_size=batch_size)\n\nX_train, y_train = train_builder.create_X_y()\nX_valid, y_valid = valid_builder.create_X_y()\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.long)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_valid_tensor = torch.tensor(X_valid, dtype=torch.long)\ny_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n\ntrain_set = TensorDataset(X_train_tensor, y_train_tensor)\nvalid_set = TensorDataset(X_valid_tensor, y_valid_tensor)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size,\n                         num_workers=2,pin_memory=True,drop_last=True)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size,\n                         num_workers=2,pin_memory=True,drop_last=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:29:17.830016Z","iopub.execute_input":"2025-09-16T07:29:17.830346Z","iopub.status.idle":"2025-09-16T07:29:18.287626Z","shell.execute_reply.started":"2025-09-16T07:29:17.830325Z","shell.execute_reply":"2025-09-16T07:29:18.287015Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"class StatefulShakespeareModel(nn.Module):\n    def __init__(self, vocab_size, n_layers=2, embed_size=20, n_hidden=128, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.gru = nn.GRU(embed_size, n_hidden, num_layers=n_layers,batch_first=True,\n                         dropout=dropout)\n        self.output = nn.Linear(n_hidden,vocab_size)\n        self.hidden_states = None\n\n    def forward(self,X):\n        embeddings = self.embed(X)\n        outputs, hidden_states = self.gru(embeddings,self.hidden_states)\n        self.hidden_states = hidden_states.detach()\n        return self.output(outputs).permute(0,2,1)        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:29:21.380049Z","iopub.execute_input":"2025-09-16T07:29:21.380336Z","iopub.status.idle":"2025-09-16T07:29:21.385844Z","shell.execute_reply.started":"2025-09-16T07:29:21.380313Z","shell.execute_reply":"2025-09-16T07:29:21.385186Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"stateful_model = StatefulShakespeareModel(len(vocab)).to(device)\nn_epochs = 20\n\ndef reset_hidden_state(model,epoch):\n    model.hidden_states = None\n\nxentropy = nn.CrossEntropyLoss()\naccuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(vocab)).to(device)\noptimizer = torch.optim.NAdam(stateful_model.parameters())\n\nhistory = train(stateful_model, optimizer, xentropy, accuracy, train_loader, \n                valid_loader,n_epochs, epoch_callback=reset_hidden_state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:29:23.427923Z","iopub.execute_input":"2025-09-16T07:29:23.428231Z","iopub.status.idle":"2025-09-16T07:29:39.900740Z","shell.execute_reply.started":"2025-09-16T07:29:23.428208Z","shell.execute_reply":"2025-09-16T07:29:39.899729Z"}},"outputs":[{"name":"stdout","text":"Batch 17/17, loss =3.1932Epoch:1/20, Train Loss: 3.1932, Train Metric: 0.1435, Valid Metric: 0.1516\nBatch 17/17, loss =2.8296Epoch:2/20, Train Loss: 2.8296, Train Metric: 0.2133, Valid Metric: 0.2669\nBatch 17/17, loss =2.5411Epoch:3/20, Train Loss: 2.5411, Train Metric: 0.2849, Valid Metric: 0.3052\nBatch 17/17, loss =2.3679Epoch:4/20, Train Loss: 2.3679, Train Metric: 0.3231, Valid Metric: 0.3306\nBatch 17/17, loss =2.2548Epoch:5/20, Train Loss: 2.2548, Train Metric: 0.3497, Valid Metric: 0.3543\nBatch 17/17, loss =2.1581Epoch:6/20, Train Loss: 2.1581, Train Metric: 0.3741, Valid Metric: 0.3635\nBatch 17/17, loss =2.0888Epoch:7/20, Train Loss: 2.0888, Train Metric: 0.3899, Valid Metric: 0.3828\nBatch 17/17, loss =2.0179Epoch:8/20, Train Loss: 2.0179, Train Metric: 0.4062, Valid Metric: 0.3921\nBatch 17/17, loss =1.9624Epoch:9/20, Train Loss: 1.9624, Train Metric: 0.4200, Valid Metric: 0.4051\nBatch 17/17, loss =1.9126Epoch:10/20, Train Loss: 1.9126, Train Metric: 0.4327, Valid Metric: 0.4118\nBatch 17/17, loss =1.8657Epoch:11/20, Train Loss: 1.8657, Train Metric: 0.4451, Valid Metric: 0.4190\nBatch 17/17, loss =1.8313Epoch:12/20, Train Loss: 1.8313, Train Metric: 0.4543, Valid Metric: 0.4240\nBatch 17/17, loss =1.7897Epoch:13/20, Train Loss: 1.7897, Train Metric: 0.4658, Valid Metric: 0.4316\nBatch 17/17, loss =1.7567Epoch:14/20, Train Loss: 1.7567, Train Metric: 0.4747, Valid Metric: 0.4351\nBatch 17/17, loss =1.7326Epoch:15/20, Train Loss: 1.7326, Train Metric: 0.4813, Valid Metric: 0.4417\nBatch 17/17, loss =1.7025Epoch:16/20, Train Loss: 1.7025, Train Metric: 0.4890, Valid Metric: 0.4449\nBatch 17/17, loss =1.6783Epoch:17/20, Train Loss: 1.6783, Train Metric: 0.4949, Valid Metric: 0.4468\nBatch 17/17, loss =1.6583Epoch:18/20, Train Loss: 1.6583, Train Metric: 0.5006, Valid Metric: 0.4493\nBatch 17/17, loss =1.6391Epoch:19/20, Train Loss: 1.6391, Train Metric: 0.5055, Valid Metric: 0.4526\nBatch 17/17, loss =1.6192Epoch:20/20, Train Loss: 1.6192, Train Metric: 0.5107, Valid Metric: 0.4557\n","output_type":"stream"}],"execution_count":108},{"cell_type":"code","source":"\ndef next_char(model, text, temperature):\n    model.eval()\n    encoded_text = encode_text(text).unsqueeze(0).to(device)\n    with torch.no_grad():\n        y_logits = model(encoded_text)\n        y_probas = torch.softmax(y_logits[0,:,-1]/temperature, dim=-1)\n        predicted_char_id = torch.multinomial(y_probas,num_samples=1).item()     \n        return id_to_char[predicted_char_id]\n\n\ndef generate_text(model, text, n_chars=100,temperature=0.9):\n    \n    print(text, end='', flush=True)\n    for _ in range(n_chars):\n        model.hidden_states = None\n        char = next_char(model, text, temperature)\n        text += char\n        print(char, end='', flush=True)\n        time.sleep(0.01)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:30:57.619844Z","iopub.execute_input":"2025-09-16T07:30:57.620132Z","iopub.status.idle":"2025-09-16T07:30:57.625849Z","shell.execute_reply.started":"2025-09-16T07:30:57.620109Z","shell.execute_reply":"2025-09-16T07:30:57.625267Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"print(generate_text(stateful_model,\"To be or not to b\",n_chars=500 ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T07:30:59.509525Z","iopub.execute_input":"2025-09-16T07:30:59.509805Z","iopub.status.idle":"2025-09-16T07:31:05.887173Z","shell.execute_reply.started":"2025-09-16T07:30:59.509784Z","shell.execute_reply":"2025-09-16T07:31:05.886531Z"}},"outputs":[{"name":"stdout","text":"To be or not to badn.\n\nrepeart:\nthat baat\nis with on his brows in crying.\n\nbrucus:\nthards not abut the wellow as a man'st!\nthe hill thy cortion whe, flown string with inish;\nmy court, my changred their suntion cases,\noul the houldied, a dalk, 'tis to breinide:\nand say to the hunger cease of the hothing recomest unrattled,\ntake on them now world to sitred comking man they son, the row, ever. way not prows your early. i aptor, duke with him wish\nof burnings have blood off out our saod cably\nnot them: strent disweyNone\n","output_type":"stream"}],"execution_count":116}]}