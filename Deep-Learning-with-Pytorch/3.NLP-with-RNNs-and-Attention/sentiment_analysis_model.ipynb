{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis on IMDB Dataset"
      ],
      "metadata": {
        "id": "DCbX2rcElyDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ixitY6GnURSZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(torch.cuda.device_count())\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ERxr3OLzUW_9",
        "outputId": "6951416a-2fbd-45b4-9f2b-16121ddfefea"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "split = imdb_dataset[\"train\"].train_test_split(train_size = 0.8)\n",
        "imdb_train, imdb_valid = split[\"train\"], split[\"test\"]\n",
        "imdb_test = imdb_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "WWn5z8qAUevR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token"
      ],
      "metadata": {
        "id": "ZjMmnq-fh1kN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, tokenizer=gpt_tokenizer):\n",
        "  review = [review[\"text\"] for review in batch]\n",
        "  labels = [[review[\"label\"]] for review in batch]\n",
        "  encodings = tokenizer(\n",
        "      review,\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=200,\n",
        "      return_tensors=\"pt\")\n",
        "  labels = torch.tensor(labels, dtype=torch.float32)\n",
        "  return encodings, labels"
      ],
      "metadata": {
        "id": "ELUrHFmKVALo"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 256\n",
        "\n",
        "imdb_train_loader = DataLoader(imdb_train, batch_size=batch_size,\n",
        "                               collate_fn=collate_fn, shuffle=True)\n",
        "imdb_valid_loader = DataLoader(imdb_valid, batch_size=batch_size,\n",
        "                               collate_fn=collate_fn)\n",
        "imdb_test_loader = DataLoader(imdb_test, batch_size=batch_size,\n",
        "                               collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "921tEv_eWAb3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class SentimentAnalysisPackedSeqModel(nn.Module):\n",
        "  def __init__(self, vocab_size, n_layers=2, hidden_size=128, embed_size=128,\n",
        "               pad_id=0, dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size,\n",
        "                              padding_idx=pad_id)\n",
        "    self.gru = nn.GRU(embed_size, hidden_size, num_layers=n_layers,\n",
        "                      batch_first=True, dropout=dropout)\n",
        "    self.output = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self,encodings):\n",
        "    embeddings = self.embed(encodings[\"input_ids\"])\n",
        "    lengths = encodings[\"attention_mask\"].sum(dim=1)\n",
        "    packed = pack_padded_sequence(embeddings, lengths=lengths.cpu(),\n",
        "                                    batch_first=True, enforce_sorted=False)\n",
        "    _outputs, hidden_states = self.gru(packed)\n",
        "    return self.output(hidden_states[-1])\n"
      ],
      "metadata": {
        "id": "Dt5bgkk0c9p0"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "\n",
        "def evaluate_tm(model, data_loader, metric):\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            metric.update(y_pred, y_batch)\n",
        "    return metric.compute()\n",
        "\n",
        "def train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs, patience=2,\n",
        "         factor=0.5,epoch_callback=None):\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"max\", patience=patience, factor=factor\n",
        "    )\n",
        "    history = {\"train_losses\":[],\"train_metrics\":[],\"valid_metrics\":[]}\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0\n",
        "        metric.reset()\n",
        "        model.train()\n",
        "        if epoch_callback is not None:\n",
        "            epoch_callback(model,epoch)\n",
        "        for idx,( X_batch, y_batch) in enumerate(train_loader):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            metric.update(y_pred, y_batch)\n",
        "            print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n",
        "            print(f\", loss ={total_loss/(idx+1 ):.4f} \", end=\"\")\n",
        "        mean_loss = total_loss / len(train_loader)\n",
        "        history[\"train_losses\"].append(mean_loss)\n",
        "        history[\"train_metrics\"].append(metric.compute().item())\n",
        "        val_metric = evaluate_tm(model, valid_loader, metric).item()\n",
        "        history[\"valid_metrics\"].append(val_metric)\n",
        "        scheduler.step(val_metric)\n",
        "        print(f\"Epoch:{epoch+1}/{n_epochs}, \"\n",
        "             f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n",
        "             f\"Train Metric: {history['train_metrics'][-1]:.4f}, \"\n",
        "             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "PCVfnGMjgNee"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = gpt_tokenizer.vocab_size\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQP_DbcpeL3u",
        "outputId": "41f08970-c985-4da5-b6f9-135b35586f58"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_model = SentimentAnalysisPackedSeqModel(vocab_size).to(device)\n",
        "\n",
        "n_epochs = 10\n",
        "xentropy = nn.BCEWithLogitsLoss()\n",
        "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
        "optimizer = torch.optim.NAdam(imdb_model.parameters())\n",
        "\n",
        "history = train(imdb_model, optimizer, xentropy, accuracy, imdb_train_loader, imdb_valid_loader, n_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH1ErsAEfNoB",
        "outputId": "d85501a9-adc6-445a-f0c8-2f84f0717ddd"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 79/79, loss =0.6845 Epoch:1/10, Train Loss: 0.6845, Train Metric: 0.5701, Valid Metric: 0.5200\n",
            "Batch 79/79, loss =0.6803 Epoch:2/10, Train Loss: 0.6803, Train Metric: 0.5667, Valid Metric: 0.6000\n",
            "Batch 79/79, loss =0.6029 Epoch:3/10, Train Loss: 0.6029, Train Metric: 0.6695, Valid Metric: 0.7422\n",
            "Batch 79/79, loss =0.4161 Epoch:4/10, Train Loss: 0.4161, Train Metric: 0.8083, Valid Metric: 0.7860\n",
            "Batch 79/79, loss =0.2792 Epoch:5/10, Train Loss: 0.2792, Train Metric: 0.8849, Valid Metric: 0.7946\n",
            "Batch 79/79, loss =0.1940 Epoch:6/10, Train Loss: 0.1940, Train Metric: 0.9262, Valid Metric: 0.7976\n",
            "Batch 79/79, loss =0.1238 Epoch:7/10, Train Loss: 0.1238, Train Metric: 0.9560, Valid Metric: 0.8356\n",
            "Batch 79/79, loss =0.1017 Epoch:8/10, Train Loss: 0.1017, Train Metric: 0.9679, Valid Metric: 0.8370\n",
            "Batch 79/79, loss =0.0400 Epoch:9/10, Train Loss: 0.0400, Train Metric: 0.9895, Valid Metric: 0.8324\n",
            "Batch 79/79, loss =0.0223 Epoch:10/10, Train Loss: 0.0223, Train Metric: 0.9957, Valid Metric: 0.7854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Bidirectional RNN"
      ],
      "metadata": {
        "id": "utdezMn6l-9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalysisBidiModel(nn.Module):\n",
        "  def __init__(self, vocab_size, n_layers=2, hidden_size=128, embed_size=128,\n",
        "               pad_id=0, dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad_id)\n",
        "    self.gru = nn.GRU(embed_size, hidden_size, num_layers=n_layers,\n",
        "                      batch_first=True, dropout=dropout, bidirectional=True)\n",
        "    self.output = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "  def forward(self, encodings):\n",
        "    embeddings = self.embed(encodings[\"input_ids\"])\n",
        "    lengths = encodings[\"attention_mask\"].sum(dim=1)\n",
        "    packed = pack_padded_sequence(embeddings, lengths=lengths.cpu(),\n",
        "                                    batch_first=True, enforce_sorted=False)\n",
        "    _outputs, hidden_states = self.gru(packed)\n",
        "\n",
        "    forward_state = hidden_states[-2] # (batch, hidden_size)\n",
        "    backward_state = hidden_states[-1] # (batch, hidden_size)\n",
        "\n",
        "    final_state = torch.cat((forward_state, backward_state), dim=1)   # (batch, 2*hidden_size)\n",
        "\n",
        "    return self.output(final_state)\n"
      ],
      "metadata": {
        "id": "vQBrk-ktl4il"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_bidi_model = SentimentAnalysisBidiModel(vocab_size).to(device)\n",
        "\n",
        "n_epochs = 10\n",
        "xentropy = nn.BCEWithLogitsLoss()\n",
        "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
        "optimizer = torch.optim.NAdam(imdb_bidi_model.parameters())\n",
        "\n",
        "history = train(imdb_bidi_model, optimizer, xentropy, accuracy, imdb_train_loader, imdb_valid_loader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKM4kjHxoX7y",
        "outputId": "0bd48ba8-696c-494b-f104-9e1e477ca18d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 79/79, loss =0.6549 Epoch:1/10, Train Loss: 0.6549, Train Metric: 0.6074, Valid Metric: 0.6600\n",
            "Batch 79/79, loss =0.5331 Epoch:2/10, Train Loss: 0.5331, Train Metric: 0.7340, Valid Metric: 0.6446\n",
            "Batch 79/79, loss =0.3474 Epoch:3/10, Train Loss: 0.3474, Train Metric: 0.8476, Valid Metric: 0.7918\n",
            "Batch 79/79, loss =0.2190 Epoch:4/10, Train Loss: 0.2190, Train Metric: 0.9123, Valid Metric: 0.8048\n",
            "Batch 79/79, loss =0.1237 Epoch:5/10, Train Loss: 0.1237, Train Metric: 0.9555, Valid Metric: 0.7968\n",
            "Batch 79/79, loss =0.0437 Epoch:6/10, Train Loss: 0.0437, Train Metric: 0.9870, Valid Metric: 0.8256\n",
            "Batch 79/79, loss =0.0124 Epoch:7/10, Train Loss: 0.0124, Train Metric: 0.9975, Valid Metric: 0.8314\n",
            "Batch 79/79, loss =0.0037 Epoch:8/10, Train Loss: 0.0037, Train Metric: 0.9995, Valid Metric: 0.8300\n",
            "Batch 79/79, loss =0.0019 Epoch:9/10, Train Loss: 0.0019, Train Metric: 0.9998, Valid Metric: 0.8304\n",
            "Batch 79/79, loss =0.0007 Epoch:10/10, Train Loss: 0.0007, Train Metric: 1.0000, Valid Metric: 0.8322\n"
          ]
        }
      ]
    }
  ]
}