{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13248595,"sourceType":"datasetVersion","datasetId":8394833},{"sourceId":13255611,"sourceType":"datasetVersion","datasetId":8399703}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport json","metadata":{"id":"YjBNqJTRCgQO","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:41.063043Z","iopub.execute_input":"2025-10-04T02:53:41.063314Z","iopub.status.idle":"2025-10-04T02:53:42.977082Z","shell.execute_reply.started":"2025-10-04T02:53:41.063292Z","shell.execute_reply":"2025-10-04T02:53:42.976516Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"if torch.cuda.is_available():\n  device = 'cuda'\n  print(torch.cuda.device_count())\nelif torch.backends.mps.is_available():\n  device = 'mps'\nelse:\n  device = 'cpu'\nprint(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bqYkafYCndQ","outputId":"3bf45a10-9a57-4e67-be1f-6695fb35623d","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:42.978392Z","iopub.execute_input":"2025-10-04T02:53:42.979005Z","iopub.status.idle":"2025-10-04T02:53:43.059202Z","shell.execute_reply.started":"2025-10-04T02:53:42.978981Z","shell.execute_reply":"2025-10-04T02:53:43.058456Z"}},"outputs":[{"name":"stdout","text":"2\ncuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Transformer Architecture","metadata":{"id":"j7bfqkfGCRSI"}},{"cell_type":"markdown","source":"## Positional encodings","metadata":{"id":"jcFLqG7pCXtc"}},{"cell_type":"code","source":"class PositionalEncodings(nn.Module):\n  def __init__(self, max_len, embed_dim, dropout=0.1):\n    super().__init__()\n    # pos_embed: learnable positional embeddings for all positions up to max_len\n    # Shape = [max_len, embed_dim]\n    # Example: if max_len=500 and embed_dim=512 → [500, 512]\n    self.pos_embed = nn.Parameter(torch.randn(max_len, embed_dim) * 0.02)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, X):\n    \"\"\"\n    X: token embeddings\n    Shape = [batch_size, seq_len, embed_dim]\n\n    self.pos_embed[:X.size(1)]:\n        - X.size(1) = seq_len\n        - So we take the first `seq_len` rows from pos_embed\n        - Shape = [seq_len, embed_dim]\n\n    Broadcasting when adding:\n        - X: [batch_size, seq_len, embed_dim]\n        - pos_embed[:seq_len]: [seq_len, embed_dim]\n        - Automatically broadcast to [1, seq_len, embed_dim] → [batch_size, seq_len, embed_dim]\n\n    Final output:\n        - Shape = [batch_size, seq_len, embed_dim]\n    \"\"\"\n    return self.dropout(X + self.pos_embed[:X.size(1)])\n\n","metadata":{"id":"dYBqTShgAxTQ","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:43.060035Z","iopub.execute_input":"2025-10-04T02:53:43.060268Z","iopub.status.idle":"2025-10-04T02:53:43.065606Z","shell.execute_reply.started":"2025-10-04T02:53:43.060251Z","shell.execute_reply":"2025-10-04T02:53:43.064722Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"max_len = 500\nembed_dim = 512\npos_embedding = PositionalEncodings(max_len, embed_dim)\nembeddings = torch.randn(256, 500, 512)\nembeddings_with_pos = pos_embedding(embeddings)\nembeddings_with_pos.shape\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAVJF2c6DIk-","outputId":"0c26feef-7032-4543-dcb6-50e431005fec","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:43.066802Z","iopub.execute_input":"2025-10-04T02:53:43.067027Z","iopub.status.idle":"2025-10-04T02:53:44.193705Z","shell.execute_reply.started":"2025-10-04T02:53:43.066999Z","shell.execute_reply":"2025-10-04T02:53:44.193014Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"torch.Size([256, 500, 512])"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"a = torch.tensor([1,2,3,4,5])\nb = torch.tensor([6,7,8,9,0])","metadata":{"id":"X4N130rNAJ5b","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.194311Z","iopub.execute_input":"2025-10-04T02:53:44.194533Z","iopub.status.idle":"2025-10-04T02:53:44.198802Z","shell.execute_reply.started":"2025-10-04T02:53:44.194516Z","shell.execute_reply":"2025-10-04T02:53:44.197812Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"c = torch.cat((a,b))\nc\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zHrK7MIXAJXi","outputId":"1aa0de92-675c-4884-d178-a34a23d5e574","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.199575Z","iopub.execute_input":"2025-10-04T02:53:44.199822Z","iopub.status.idle":"2025-10-04T02:53:44.212183Z","shell.execute_reply.started":"2025-10-04T02:53:44.199802Z","shell.execute_reply":"2025-10-04T02:53:44.211614Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 0])"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Multi-Head Attention","metadata":{"id":"itTAvHn8AnNS"}},{"cell_type":"markdown","source":"### How splitting works","metadata":{"id":"AHcbeGIHNj0n"}},{"cell_type":"code","source":"import torch\n\n# Input embeddings: (B, L, E) = (1, 3, 6)\nx = torch.tensor([[[1, 2, 3, 4, 5, 6],    # token 1 embedding\n                   [7, 8, 9, 10, 11, 12],   # token 2 embedding\n                   [13, 14, 15, 16, 17, 18]]])   # token 3 embedding\nprint(\"Input embeddings x:\", x)\nprint(\"Shape:\", x.shape)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"33oL-kQfD_ZL","outputId":"631f1c58-6a93-4c3a-a889-0a9c03d33f47","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.213129Z","iopub.execute_input":"2025-10-04T02:53:44.213455Z","iopub.status.idle":"2025-10-04T02:53:44.222279Z","shell.execute_reply.started":"2025-10-04T02:53:44.213430Z","shell.execute_reply":"2025-10-04T02:53:44.221517Z"}},"outputs":[{"name":"stdout","text":"Input embeddings x: tensor([[[ 1,  2,  3,  4,  5,  6],\n         [ 7,  8,  9, 10, 11, 12],\n         [13, 14, 15, 16, 17, 18]]])\nShape: torch.Size([1, 3, 6])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"B, L, E = x.shape\nH = 2\nD = E // H\nx_heads = x.view(B, L, H, D)  # (B, L, H, D)\nprint(x_heads.shape)\nx_heads","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XR85bL5aL_I9","outputId":"2456b442-638e-476d-8b31-2692c796478a","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.222865Z","iopub.execute_input":"2025-10-04T02:53:44.223109Z","iopub.status.idle":"2025-10-04T02:53:44.237920Z","shell.execute_reply.started":"2025-10-04T02:53:44.223082Z","shell.execute_reply":"2025-10-04T02:53:44.237225Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 3, 2, 3])\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"tensor([[[[ 1,  2,  3],\n          [ 4,  5,  6]],\n\n         [[ 7,  8,  9],\n          [10, 11, 12]],\n\n         [[13, 14, 15],\n          [16, 17, 18]]]])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"X = x_heads.transpose(1,2)  # (B, H, L, D)\nprint(X.shape)\nx_heads","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9UHfx8pdMN2J","outputId":"4ad66dca-6d17-41c6-f12a-ba6a32eaae0c","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.238692Z","iopub.execute_input":"2025-10-04T02:53:44.238944Z","iopub.status.idle":"2025-10-04T02:53:44.247506Z","shell.execute_reply.started":"2025-10-04T02:53:44.238921Z","shell.execute_reply":"2025-10-04T02:53:44.246856Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 2, 3, 3])\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor([[[[ 1,  2,  3],\n          [ 4,  5,  6]],\n\n         [[ 7,  8,  9],\n          [10, 11, 12]],\n\n         [[13, 14, 15],\n          [16, 17, 18]]]])"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### Custom MHA","metadata":{"id":"5GZckFVTSMd1"}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n  def __init__(self, embed_dim, num_heads, dropout=0.1):\n    super().__init__()\n    self.H = num_heads\n    self.D = embed_dim // num_heads\n    self.q_proj = nn.Linear(embed_dim, embed_dim)\n    self.k_proj = nn.Linear(embed_dim, embed_dim)\n    self.v_proj = nn.Linear(embed_dim, embed_dim)\n    self.out_proj = nn.Linear(embed_dim, embed_dim)\n    self.dropout = nn.Dropout(dropout)\n\n  def split_heads(self, X):\n    return X.view(X.size(0), X.size(1), self.H, self.D).transpose(1, 2)\n\n  def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n    q = self.split_heads(self.q_proj(query)) # (B, H, Lq, D)\n    k = self.split_heads(self.k_proj(key))  # (B, H, Lk, D)\n    v = self.split_heads(self.v_proj(value)) # (B, H, Lv, D) with Lv=Lk\n    scores = q @ k.transpose(2, 3) / self.D**0.5   # (B, H, Lq, Lk)\n\n    if attn_mask is not None:\n      scores = scores.masked_fill(attn_mask, -torch.inf)  # (B, H, Lq, Lk)\n    if key_padding_mask is not None:\n      mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # (B, 1, 1, Lk)\n      scores = scores.masked_fill(mask, -torch.inf)  # (B, H, Lq, Lk)\n\n    weights = scores.softmax(dim=-1) # (B, H, Lq, Lk)\n    Z = self.dropout(weights) @ v # (B, H, Lq, D)\n    Z = Z.transpose(1, 2)\n    Z = Z.reshape(Z.size(0), Z.size(1), self.H * self.D)\n    return (self.out_proj(Z), weights)\n","metadata":{"id":"28ndz1RlNr9U","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.250069Z","iopub.execute_input":"2025-10-04T02:53:44.250325Z","iopub.status.idle":"2025-10-04T02:53:44.259140Z","shell.execute_reply.started":"2025-10-04T02:53:44.250310Z","shell.execute_reply":"2025-10-04T02:53:44.258485Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Transformer Encoder Layer","metadata":{"id":"bqH0fx-QeZ1W"}},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n  def __init__(self, dim_model, n_heads, dim_ff=2048, dropout=0.1):\n    super().__init__()\n    self.self_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n    self.linear1 = nn.Linear(dim_model, dim_ff)\n    self.linear2 = nn.Linear(dim_ff, dim_model)\n    self.dropout = nn.Dropout(dropout)\n    self.norm1 = nn.LayerNorm(dim_model)\n    self.norm2 = nn.LayerNorm(dim_model)\n\n  def forward(self, src, src_mask=None, src_key_padding_mask=None):\n    attn, _ = self.self_attn(src, src, src, src_mask, src_key_padding_mask)\n    Z = self.norm1(src + self.dropout(attn))\n    ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n\n    return self.norm2(Z + ff)\n\n","metadata":{"id":"keTxTYYyeczb","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.259978Z","iopub.execute_input":"2025-10-04T02:53:44.260560Z","iopub.status.idle":"2025-10-04T02:53:44.269898Z","shell.execute_reply.started":"2025-10-04T02:53:44.260536Z","shell.execute_reply":"2025-10-04T02:53:44.269210Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Transformer Decoder Layer","metadata":{"id":"Lm2Iwrv8go3m"}},{"cell_type":"code","source":"class TransformerDecoderLayer(nn.Module):\n  def __init__(self, dim_model, n_heads, dim_ff=2048, dropout=0.1):\n    super().__init__()\n    self.self_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n    self.multi_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n    self.linear1 = nn.Linear(dim_model, dim_ff)\n    self.linear2 = nn.Linear(dim_ff, dim_model)\n    self.norm1 = nn.LayerNorm(dim_model)\n    self.norm2 = nn.LayerNorm(dim_model)\n    self.norm3 = nn.LayerNorm(dim_model)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n              tgt_key_padding_mask=None, memory_key_padding_mask=None):\n    attn1, _ = self.self_attn(tgt, tgt, tgt,\n                              attn_mask=tgt_mask,\n                              key_padding_mask=tgt_key_padding_mask)\n    Z = self.norm1(tgt + self.dropout(attn1))\n    attn2, _ = self.multi_attn(Z, memory, memory,\n                               attn_mask=memory_mask,\n                               key_padding_mask=memory_key_padding_mask)\n    Z = self.norm2(Z + self.dropout(attn2))\n    ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n    return self.norm3(Z + ff)","metadata":{"id":"kkuFQcCdgrQl","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.270718Z","iopub.execute_input":"2025-10-04T02:53:44.270908Z","iopub.status.idle":"2025-10-04T02:53:44.283853Z","shell.execute_reply.started":"2025-10-04T02:53:44.270894Z","shell.execute_reply":"2025-10-04T02:53:44.283217Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Transformer Encoder","metadata":{"id":"NpfiGPRHoReW"}},{"cell_type":"code","source":"from copy import deepcopy\n\nclass TransformerEncoder(nn.Module):\n  def __init__(self, encoder_layer, num_layers, norm=None):\n    super().__init__()\n    self.layers = nn.ModuleList([deepcopy(encoder_layer)\n                                   for _ in range(num_layers)])\n    self.norm = norm\n\n  def forward(self, src, src_mask=None, src_key_padding_mask=None):\n    Z = src\n    for layer in self.layers:\n      Z = layer(Z, src_mask, src_key_padding_mask)\n\n    if self.norm is not None:\n      Z = self.norm(Z)\n    return Z","metadata":{"id":"3s498sENoLyJ","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.284569Z","iopub.execute_input":"2025-10-04T02:53:44.284747Z","iopub.status.idle":"2025-10-04T02:53:44.297732Z","shell.execute_reply.started":"2025-10-04T02:53:44.284734Z","shell.execute_reply":"2025-10-04T02:53:44.297105Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Transformer Decoder","metadata":{"id":"3TAxBmhup13s"}},{"cell_type":"code","source":"class TransformerDecoder(nn.Module):\n  def __init__(self, decoder_layer, num_layers, norm=None):\n    super().__init__()\n    self.layers = nn.ModuleList([deepcopy(decoder_layer)\n                                  for _ in range(num_layers)])\n    self.norm = norm\n\n  def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n                    tgt_key_padding_mask=None, memory_key_padding_mask=None):\n    Z = tgt\n    for layer in self.layers:\n      Z = layer(Z, memory, tgt_mask, memory_mask,\n                tgt_key_padding_mask, memory_key_padding_mask)\n\n    if self.norm is not None:\n      Z = self.norm(Z)\n    return Z\n","metadata":{"id":"vSnaMDZAp4JB","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.298487Z","iopub.execute_input":"2025-10-04T02:53:44.298869Z","iopub.status.idle":"2025-10-04T02:53:44.311814Z","shell.execute_reply.started":"2025-10-04T02:53:44.298852Z","shell.execute_reply":"2025-10-04T02:53:44.311137Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Transformer","metadata":{"id":"NRJ8nduVrQ2Q"}},{"cell_type":"code","source":"class Transformer(nn.Module):\n  def __init__(self, d_model=512, n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n               dim_ff = 2048, dropout=0.1):\n    super().__init__()\n\n    encoder_layer = TransformerEncoderLayer(d_model, n_heads, dim_ff, dropout)\n    norm1 = nn.LayerNorm(d_model)\n\n    self.encoder = TransformerEncoder(encoder_layer, n_encoder_layers, norm1)\n\n    decoder_layer = TransformerDecoderLayer(d_model, n_heads, dim_ff, dropout)\n    norm2 = nn.LayerNorm(d_model)\n\n    self.decoder = TransformerDecoder(decoder_layer, n_decoder_layers, norm2)\n\n\n  def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None ,\n                src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n    memory = self.encoder(src, src_mask, src_key_padding_mask)\n    output = self.decoder(tgt, memory, tgt_mask, memory_mask,\n                          tgt_key_padding_mask, memory_key_padding_mask)\n\n    return output\n","metadata":{"id":"LxCo_g8rrQLc","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.312618Z","iopub.execute_input":"2025-10-04T02:53:44.312811Z","iopub.status.idle":"2025-10-04T02:53:44.323033Z","shell.execute_reply.started":"2025-10-04T02:53:44.312797Z","shell.execute_reply":"2025-10-04T02:53:44.322321Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Building English-to-Hinglish Transformer","metadata":{"id":"jZGquWj_yztY"}},{"cell_type":"code","source":"class NmtTransformer(nn.Module):\n  def __init__(self, vocab_size, max_length, embed_dim=512, pad_id=0,\n               num_heads=8, num_layers=6,dim_ff = 2048, dropout=0.1):\n    super().__init__()\n    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n    self.pos_embed = PositionalEncodings(max_length, embed_dim, dropout)\n    self.transformer = Transformer(embed_dim, num_heads, n_encoder_layers=num_layers,\n                                   n_decoder_layers=num_layers,\n                                   dim_ff=dim_ff, dropout=dropout)\n    self.output = nn.Linear(embed_dim, vocab_size)\n\n  def forward(self, pair):\n    src_embeddings = self.pos_embed(self.embed(pair.src_token_ids))\n    tgt_embeddings = self.pos_embed(self.embed(pair.tgt_token_ids))\n    src_pad_mask = ~pair.src_mask.bool()\n    tgt_pad_mask = ~pair.tgt_mask.bool()\n    size = [pair.tgt_token_ids.size(1)] * 2     #line a\n    full_mask = torch.full(size, True, device=tgt_pad_mask.device) #line b\n    causal_mask = torch.triu(full_mask, diagonal=1)  #line c\n    output_decoder = self.transformer(src_embeddings,\n                                      tgt_embeddings,\n                                      tgt_mask=causal_mask,\n                                      src_key_padding_mask=src_pad_mask,\n                                      tgt_key_padding_mask=tgt_pad_mask,\n                                      memory_key_padding_mask=src_pad_mask)\n    return self.output(output_decoder).permute(0, 2, 1)\n","metadata":{"id":"aVAdqcPLyv4c","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.323717Z","iopub.execute_input":"2025-10-04T02:53:44.323900Z","iopub.status.idle":"2025-10-04T02:53:44.339295Z","shell.execute_reply.started":"2025-10-04T02:53:44.323886Z","shell.execute_reply":"2025-10-04T02:53:44.338787Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**How line a, line b, line c works :**\n\n***Example:***\n\nseq_len = 5\n\nsize = [seq_len] * 2 -> [5, 5]\n\nfull_mask = torch.full(size, True)\n\nfull_mask:\n\ntensor( [\n\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True]\n\n        ])\n\ncausal_mask = torch.triu(full_mask, diagonal=1)\n\ncausal_mask:\n\ntensor([\n\n        [False,  True,  True,  True,  True],\n        [False, False,  True,  True,  True],\n        [False, False, False,  True,  True],\n        [False, False, False, False,  True],\n        [False, False, False, False, False]\n        \n        ])\n\n","metadata":{"id":"qpCt5kYH7_ua"}},{"cell_type":"markdown","source":"## Data for NMT","metadata":{"id":"ILJ9fBaM1Xu3"}},{"cell_type":"code","source":"\ndata = []\nfilename = \"/kaggle/input/my-dataset/hinglish_upload_v1.json\"\n\nwith open(filename, \"r\", encoding=\"utf-8\") as f:\n  for line in f:\n    obj = json.loads(line)\n    data.append({\n        \"English\":obj[\"translation\"][\"en\"],\n        \"Hinglish\":obj[\"translation\"][\"hi_ng\"]\n    })\n","metadata":{"id":"BEZkS_l01ctK","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.339983Z","iopub.execute_input":"2025-10-04T02:53:44.340213Z","iopub.status.idle":"2025-10-04T02:53:44.917960Z","shell.execute_reply.started":"2025-10-04T02:53:44.340194Z","shell.execute_reply":"2025-10-04T02:53:44.917287Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\ndf = pd.DataFrame(data)\ndf = df.sample(frac=1).reset_index(drop=True)\n\ndef preprocess_text(text):\n  return str(text).strip()\n\nen_sentence = df[\"English\"].apply(preprocess_text).tolist()\nhing_sentence = df[\"Hinglish\"].apply(preprocess_text).tolist()\n\nfor i in range(3):\n    print(en_sentence[i], \"=>\", hing_sentence[i])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-mk4oV91hOF","outputId":"b9b69d1b-bf8e-444a-e58d-3db2a11b96d5","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:44.918946Z","iopub.execute_input":"2025-10-04T02:53:44.919194Z","iopub.status.idle":"2025-10-04T02:53:45.157561Z","shell.execute_reply.started":"2025-10-04T02:53:44.919155Z","shell.execute_reply":"2025-10-04T02:53:45.156934Z"}},"outputs":[{"name":"stdout","text":"Open Line and tell Billy he ' s my favorite person . => Line ko open kare aur Billy ko bolo ki he ' s my favorite person .\neven zedge wouldn't open on vodafone, porn obviously won't => Vodafone pe to Zedge bhi nahi khulta, porn ghanta khulega!\nSet an alarm at 3 pm on May the 17th => May the 17th at 3pm ko alarm set kare\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import tokenizers\ndef train_eng_hing():\n  for en, hi in zip(en_sentence, hing_sentence):\n    yield en\n    yield hi\nmax_len = 500\nvocab_size = 10_000\n\nnmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"\")\nnmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\nnmt_tokenizer.enable_padding(pad_id=0, pad_token=\"\")\nnmt_tokenizer.enable_truncation(max_length=max_len)\nnmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\nnmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(\n    vocab_size=vocab_size, special_tokens=[\"\", \"\", \"\", \"\"])\nnmt_tokenizer.train_from_iterator(train_eng_hing(), nmt_tokenizer_trainer)\n\n","metadata":{"id":"ZwHf9krj1kKH","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:45.158414Z","iopub.execute_input":"2025-10-04T02:53:45.159100Z","iopub.status.idle":"2025-10-04T02:53:47.714164Z","shell.execute_reply.started":"2025-10-04T02:53:45.159070Z","shell.execute_reply":"2025-10-04T02:53:47.713373Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\nnmt_tokenizer.encode(\"I love football\").ids","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OY-nPzf1luo","outputId":"7184ed5a-f629-4068-c2e9-819a72865b5e","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:47.714858Z","iopub.execute_input":"2025-10-04T02:53:47.715052Z","iopub.status.idle":"2025-10-04T02:53:47.720499Z","shell.execute_reply.started":"2025-10-04T02:53:47.715037Z","shell.execute_reply":"2025-10-04T02:53:47.719560Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[41, 1248, 2274]"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"nmt_tokenizer.encode(\"Muje football pasand hai.\").ids","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"enWuUj8S1nbG","outputId":"92f7941c-5faa-406e-9d98-4b6489670d53","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:47.721663Z","iopub.execute_input":"2025-10-04T02:53:47.721911Z","iopub.status.idle":"2025-10-04T02:53:47.735967Z","shell.execute_reply.started":"2025-10-04T02:53:47.721890Z","shell.execute_reply":"2025-10-04T02:53:47.735334Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[755, 2274, 1849, 346, 14]"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from collections import namedtuple\n\nfields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"]\nclass NMTPair(namedtuple(\"NmtPairBase\", fields)):\n  def to(self,device):\n    return NMTPair(\n        self.src_token_ids.to(device),\n        self.src_mask.to(device),\n        self.tgt_token_ids.to(device),\n        self.tgt_mask.to(device)\n    )\n\n","metadata":{"id":"TWOgmofv1pa7","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:47.736750Z","iopub.execute_input":"2025-10-04T02:53:47.736912Z","iopub.status.idle":"2025-10-04T02:53:47.747800Z","shell.execute_reply.started":"2025-10-04T02:53:47.736899Z","shell.execute_reply":"2025-10-04T02:53:47.747206Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def nmt_collate_fn(batch):\n  src_text = [item[\"English\"] for item in batch]\n  tgt_text = [f\" {item['Hinglish']} \" for item in batch]\n  src_encodings = nmt_tokenizer.encode_batch(src_text)\n  tgt_encodings = nmt_tokenizer.encode_batch(tgt_text)\n  src_token_ids = torch.tensor([enc.ids for enc in src_encodings])\n  tgt_token_ids = torch.tensor([enc.ids for enc in tgt_encodings])\n  src_mask = torch.tensor([enc.attention_mask for enc in src_encodings])\n  tgt_mask = torch.tensor([enc.attention_mask for enc in tgt_encodings])\n  inputs = NMTPair(src_token_ids,\n                  src_mask,\n                  tgt_token_ids[:,:-1],\n                  tgt_mask[:,:-1])\n  labels =tgt_token_ids[:,1:]\n  return inputs, labels","metadata":{"id":"NkUXTPKV1rgk","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:47.748476Z","iopub.execute_input":"2025-10-04T02:53:47.748798Z","iopub.status.idle":"2025-10-04T02:53:47.758755Z","shell.execute_reply.started":"2025-10-04T02:53:47.748774Z","shell.execute_reply":"2025-10-04T02:53:47.758099Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"train_set = df.to_dict(\"records\")[:int(0.8 * len(df))]\nvalid_set = df.to_dict(\"records\")[int(0.8 * len(df)):]\ntrain_set[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ce2v3NWx1tYn","outputId":"75e21e6e-b82c-4774-8901-a6ed9393c2be","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:47.759346Z","iopub.execute_input":"2025-10-04T02:53:47.759547Z","iopub.status.idle":"2025-10-04T02:53:48.602661Z","shell.execute_reply.started":"2025-10-04T02:53:47.759531Z","shell.execute_reply":"2025-10-04T02:53:48.601817Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'English': \"Open Line and tell Billy he ' s my favorite person .\",\n 'Hinglish': \"Line ko open kare aur Billy ko bolo ki he ' s my favorite person .\"}"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 64\ntrain_loader = DataLoader(\n    train_set,\n    batch_size=batch_size,\n    collate_fn=nmt_collate_fn,\n    shuffle=True\n)\nvalid_loader =DataLoader(\n    valid_set,\n    batch_size=batch_size,\n    collate_fn=nmt_collate_fn\n)","metadata":{"id":"KkknleCy1vO2","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:48.603401Z","iopub.execute_input":"2025-10-04T02:53:48.603623Z","iopub.status.idle":"2025-10-04T02:53:48.607927Z","shell.execute_reply.started":"2025-10-04T02:53:48.603607Z","shell.execute_reply":"2025-10-04T02:53:48.607103Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import torchmetrics\n\ndef evaluate_tm(model, data_loader, metric):\n  model.eval()\n  metric.reset()\n  with torch.no_grad():\n    for X_batch, y_batch in data_loader:\n      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n      y_pred = model(X_batch)\n      metric.update(y_pred, y_batch)\n  return metric.compute()\n\ndef train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs,\n          patience=2, factor=0.5):\n  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n      optimizer, mode='max', patience=patience, factor=factor\n  )\n  history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n  for epoch in range(n_epochs):\n    print(f\"Epoch:{epoch+1}/{n_epochs}\")\n    model.train()\n    metric.reset()\n    total_loss = 0\n    for idx, (X_batch, y_batch) in enumerate(train_loader):\n      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n      y_pred = model(X_batch)\n      loss = criterion(y_pred, y_batch)\n      total_loss += loss.item()\n      loss.backward()\n      optimizer.step()\n      optimizer.zero_grad()\n      metric.update(y_pred, y_batch)\n      print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n      print(f\", loss ={total_loss/(idx+1 ):.4f} \", end=\"\")\n    mean_loss = total_loss / len(train_loader)\n    history[\"train_losses\"].append(mean_loss)\n    history[\"train_metrics\"].append(metric.compute().item())\n    val_metric = evaluate_tm(model, valid_loader, metric).item()\n    history[\"valid_metrics\"].append(val_metric)\n    scheduler.step(val_metric)\n    print(f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n             f\"Train Metric: {history['train_metrics'][-1]:.4f}%, \"\n             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}%\")\n  print(\"Training Completed!\")\n  return history\n\n","metadata":{"id":"DJX5JrKi4eut","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:48.608955Z","iopub.execute_input":"2025-10-04T02:53:48.609237Z","iopub.status.idle":"2025-10-04T02:53:56.207285Z","shell.execute_reply.started":"2025-10-04T02:53:48.609210Z","shell.execute_reply":"2025-10-04T02:53:56.206491Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Train the Model","metadata":{"id":"QtfvyX7x9Usq"}},{"cell_type":"code","source":"model = NmtTransformer(vocab_size, max_len, embed_dim=512, pad_id=0, num_heads=4, num_layers=2,\n                       dropout=0.1).to(device)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\nmodel = torch.compile(model, mode='reduce-overhead')\n\nn_epochs = 20\nxentropy = nn.CrossEntropyLoss(ignore_index=0)\naccuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocab_size).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)\n\nhistory = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader,n_epochs)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"voRgEyrT61Jh","outputId":"e077587a-7b93-4020-985a-21741f118734","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T02:53:56.208065Z","iopub.execute_input":"2025-10-04T02:53:56.208416Z","iopub.status.idle":"2025-10-04T04:13:16.392937Z","shell.execute_reply.started":"2025-10-04T02:53:56.208400Z","shell.execute_reply":"2025-10-04T04:13:16.392217Z"}},"outputs":[{"name":"stderr","text":"W1004 02:53:58.500000 131 torch/_logging/_internal.py:1089] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n","output_type":"stream"},{"name":"stdout","text":"Epoch:1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin sys._getframe. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n  torch._dynamo.utils.warn_once(msg)\n","output_type":"stream"},{"name":"stdout","text":"Batch 2364/2364, loss =3.0806 Train Loss: 3.0806, Train Metric: 0.1152%, Valid Metric: 0.1441%\nEpoch:2/20\nBatch 2364/2364, loss =2.0323 Train Loss: 2.0323, Train Metric: 0.1400%, Valid Metric: 0.1554%\nEpoch:3/20\nBatch 2364/2364, loss =1.6702 Train Loss: 1.6702, Train Metric: 0.1511%, Valid Metric: 0.1614%\nEpoch:4/20\nBatch 2364/2364, loss =1.4627 Train Loss: 1.4627, Train Metric: 0.1578%, Valid Metric: 0.1648%\nEpoch:5/20\nBatch 2364/2364, loss =1.3227 Train Loss: 1.3227, Train Metric: 0.1630%, Valid Metric: 0.1680%\nEpoch:6/20\nBatch 2364/2364, loss =1.2165 Train Loss: 1.2165, Train Metric: 0.1661%, Valid Metric: 0.1702%\nEpoch:7/20\nBatch 2364/2364, loss =1.1305 Train Loss: 1.1305, Train Metric: 0.1697%, Valid Metric: 0.1722%\nEpoch:8/20\nBatch 2364/2364, loss =1.0546 Train Loss: 1.0546, Train Metric: 0.1735%, Valid Metric: 0.1740%\nEpoch:9/20\nBatch 2364/2364, loss =0.9972 Train Loss: 0.9972, Train Metric: 0.1766%, Valid Metric: 0.1758%\nEpoch:10/20\nBatch 2364/2364, loss =0.9431 Train Loss: 0.9431, Train Metric: 0.1783%, Valid Metric: 0.1773%\nEpoch:11/20\nBatch 2364/2364, loss =0.8951 Train Loss: 0.8951, Train Metric: 0.1809%, Valid Metric: 0.1781%\nEpoch:12/20\nBatch 2364/2364, loss =0.8541 Train Loss: 0.8541, Train Metric: 0.1824%, Valid Metric: 0.1794%\nEpoch:13/20\nBatch 2364/2364, loss =0.8155 Train Loss: 0.8155, Train Metric: 0.1841%, Valid Metric: 0.1802%\nEpoch:14/20\nBatch 2364/2364, loss =0.7834 Train Loss: 0.7834, Train Metric: 0.1865%, Valid Metric: 0.1809%\nEpoch:15/20\nBatch 2364/2364, loss =0.7548 Train Loss: 0.7548, Train Metric: 0.1860%, Valid Metric: 0.1819%\nEpoch:16/20\nBatch 2364/2364, loss =0.7259 Train Loss: 0.7259, Train Metric: 0.1884%, Valid Metric: 0.1828%\nEpoch:17/20\nBatch 2364/2364, loss =0.7003 Train Loss: 0.7003, Train Metric: 0.1902%, Valid Metric: 0.1831%\nEpoch:18/20\nBatch 2364/2364, loss =0.6765 Train Loss: 0.6765, Train Metric: 0.1893%, Valid Metric: 0.1839%\nEpoch:19/20\nBatch 2364/2364, loss =0.6522 Train Loss: 0.6522, Train Metric: 0.1920%, Valid Metric: 0.1845%\nEpoch:20/20\nBatch 2364/2364, loss =0.6341 Train Loss: 0.6341, Train Metric: 0.1915%, Valid Metric: 0.1851%\nTraining Completed!\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def translate(model, src_text, max_len=20, eos_id=3):\n    tgt_text = \"<bos>\"\n    for idx in range(max_len):\n        batch, _ = nmt_collate_fn([{\"English\":src_text ,\n                                   \"Hinglish\":tgt_text}])\n        batch = batch.to(device)\n        batch = NMTPair(\n            batch.src_token_ids.long(),\n            batch.src_mask,\n            batch.tgt_token_ids.long(),\n            batch.tgt_mask\n        )\n        \n        with torch.no_grad():\n            y_pred = model(batch)\n            y_token_ids = y_pred.argmax(dim=1)\n            next_token_id = y_token_ids[0, idx]\n        \n        if next_token_id == eos_id:\n            break\n\n        next_token = nmt_tokenizer.id_to_token(next_token_id)\n        tgt_text += \" \" + next_token\n    return tgt_text.replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").strip()\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:21:02.284083Z","iopub.execute_input":"2025-10-04T04:21:02.285032Z","iopub.status.idle":"2025-10-04T04:21:02.290611Z","shell.execute_reply.started":"2025-10-04T04:21:02.284992Z","shell.execute_reply":"2025-10-04T04:21:02.289757Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"\ntranslate(model, \"hello\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T04:21:15.217914Z","iopub.execute_input":"2025-10-04T04:21:15.218597Z","iopub.status.idle":"2025-10-04T04:21:23.996910Z","shell.execute_reply.started":"2025-10-04T04:21:15.218565Z","shell.execute_reply":"2025-10-04T04:21:23.996069Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"'L tal namaste ACH LO AC AC HA HA HE HE LA LA MOVIE MOVIE L GA LO PASAND LO'"},"metadata":{}}],"execution_count":40}]}