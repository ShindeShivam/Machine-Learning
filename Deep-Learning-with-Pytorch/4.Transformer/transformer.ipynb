{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YjBNqJTRCgQO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "  device = 'mps'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bqYkafYCndQ",
        "outputId": "817d3ab2-4a67-4419-96cd-3111b4f8c6c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Architecture"
      ],
      "metadata": {
        "id": "j7bfqkfGCRSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional encodings"
      ],
      "metadata": {
        "id": "jcFLqG7pCXtc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dYBqTShgAxTQ"
      },
      "outputs": [],
      "source": [
        "class PositionalEncodings(nn.Module):\n",
        "  def __init__(self, max_len, embed_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "    # pos_embed: learnable positional embeddings for all positions up to max_len\n",
        "    # Shape = [max_len, embed_dim]\n",
        "    # Example: if max_len=500 and embed_dim=512 → [500, 512]\n",
        "    self.pos_embed = nn.Parameter(torch.randn(max_len, embed_dim) * 0.02)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    X: token embeddings\n",
        "    Shape = [batch_size, seq_len, embed_dim]\n",
        "\n",
        "    self.pos_embed[:X.size(1)]:\n",
        "        - X.size(1) = seq_len\n",
        "        - So we take the first `seq_len` rows from pos_embed\n",
        "        - Shape = [seq_len, embed_dim]\n",
        "\n",
        "    Broadcasting when adding:\n",
        "        - X: [batch_size, seq_len, embed_dim]\n",
        "        - pos_embed[:seq_len]: [seq_len, embed_dim]\n",
        "        - Automatically broadcast to [1, seq_len, embed_dim] → [batch_size, seq_len, embed_dim]\n",
        "\n",
        "    Final output:\n",
        "        - Shape = [batch_size, seq_len, embed_dim]\n",
        "    \"\"\"\n",
        "    return self.dropout(X + self.pos_embed[:X.size(1)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 500\n",
        "embed_dim = 512\n",
        "pos_embedding = PositionalEncodings(max_len, embed_dim)\n",
        "embeddings = torch.randn(256, 500, 512)\n",
        "embeddings_with_pos = pos_embedding(embeddings)\n",
        "embeddings_with_pos.shape\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAVJF2c6DIk-",
        "outputId": "d6f019d7-8063-4234-982f-12fd8554c3b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 500, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}