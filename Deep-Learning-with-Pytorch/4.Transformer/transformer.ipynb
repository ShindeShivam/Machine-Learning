{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json"
      ],
      "metadata": {
        "id": "YjBNqJTRCgQO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "  device = 'mps'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bqYkafYCndQ",
        "outputId": "3bf45a10-9a57-4e67-be1f-6695fb35623d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Architecture"
      ],
      "metadata": {
        "id": "j7bfqkfGCRSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional encodings"
      ],
      "metadata": {
        "id": "jcFLqG7pCXtc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dYBqTShgAxTQ"
      },
      "outputs": [],
      "source": [
        "class PositionalEncodings(nn.Module):\n",
        "  def __init__(self, max_len, embed_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "    # pos_embed: learnable positional embeddings for all positions up to max_len\n",
        "    # Shape = [max_len, embed_dim]\n",
        "    # Example: if max_len=500 and embed_dim=512 → [500, 512]\n",
        "    self.pos_embed = nn.Parameter(torch.randn(max_len, embed_dim) * 0.02)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    X: token embeddings\n",
        "    Shape = [batch_size, seq_len, embed_dim]\n",
        "\n",
        "    self.pos_embed[:X.size(1)]:\n",
        "        - X.size(1) = seq_len\n",
        "        - So we take the first `seq_len` rows from pos_embed\n",
        "        - Shape = [seq_len, embed_dim]\n",
        "\n",
        "    Broadcasting when adding:\n",
        "        - X: [batch_size, seq_len, embed_dim]\n",
        "        - pos_embed[:seq_len]: [seq_len, embed_dim]\n",
        "        - Automatically broadcast to [1, seq_len, embed_dim] → [batch_size, seq_len, embed_dim]\n",
        "\n",
        "    Final output:\n",
        "        - Shape = [batch_size, seq_len, embed_dim]\n",
        "    \"\"\"\n",
        "    return self.dropout(X + self.pos_embed[:X.size(1)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 500\n",
        "embed_dim = 512\n",
        "pos_embedding = PositionalEncodings(max_len, embed_dim)\n",
        "embeddings = torch.randn(256, 500, 512)\n",
        "embeddings_with_pos = pos_embedding(embeddings)\n",
        "embeddings_with_pos.shape\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAVJF2c6DIk-",
        "outputId": "0c26feef-7032-4543-dcb6-50e431005fec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 500, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2,3,4,5])\n",
        "b = torch.tensor([6,7,8,9,0])"
      ],
      "metadata": {
        "id": "X4N130rNAJ5b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = torch.cat((a,b))\n",
        "c\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHrK7MIXAJXi",
        "outputId": "1aa0de92-675c-4884-d178-a34a23d5e574"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention"
      ],
      "metadata": {
        "id": "itTAvHn8AnNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How splitting works"
      ],
      "metadata": {
        "id": "AHcbeGIHNj0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Input embeddings: (B, L, E) = (1, 3, 6)\n",
        "x = torch.tensor([[[1, 2, 3, 4, 5, 6],    # token 1 embedding\n",
        "                   [7, 8, 9, 10, 11, 12],   # token 2 embedding\n",
        "                   [13, 14, 15, 16, 17, 18]]])   # token 3 embedding\n",
        "print(\"Input embeddings x:\", x)\n",
        "print(\"Shape:\", x.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33oL-kQfD_ZL",
        "outputId": "631f1c58-6a93-4c3a-a889-0a9c03d33f47"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input embeddings x: tensor([[[ 1,  2,  3,  4,  5,  6],\n",
            "         [ 7,  8,  9, 10, 11, 12],\n",
            "         [13, 14, 15, 16, 17, 18]]])\n",
            "Shape: torch.Size([1, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B, L, E = x.shape\n",
        "H = 2\n",
        "D = E // H\n",
        "x_heads = x.view(B, L, H, D)  # (B, L, H, D)\n",
        "print(x_heads.shape)\n",
        "x_heads"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR85bL5aL_I9",
        "outputId": "2456b442-638e-476d-8b31-2692c796478a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 2, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 1,  2,  3],\n",
              "          [ 4,  5,  6]],\n",
              "\n",
              "         [[ 7,  8,  9],\n",
              "          [10, 11, 12]],\n",
              "\n",
              "         [[13, 14, 15],\n",
              "          [16, 17, 18]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = x_heads.transpose(1,2)  # (B, H, L, D)\n",
        "print(X.shape)\n",
        "x_heads"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UHfx8pdMN2J",
        "outputId": "4ad66dca-6d17-41c6-f12a-ba6a32eaae0c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2, 3, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 1,  2,  3],\n",
              "          [ 4,  5,  6]],\n",
              "\n",
              "         [[ 7,  8,  9],\n",
              "          [10, 11, 12]],\n",
              "\n",
              "         [[13, 14, 15],\n",
              "          [16, 17, 18]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom MHA"
      ],
      "metadata": {
        "id": "5GZckFVTSMd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.H = num_heads\n",
        "    self.D = embed_dim // num_heads\n",
        "    self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "    self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "    self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "    self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def split_heads(self, X):\n",
        "    return X.view(X.size(0), X.size(1), self.H, self.D).transpose(1, 2)\n",
        "\n",
        "  def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
        "    q = self.split_heads(self.q_proj(query)) # (B, H, Lq, D)\n",
        "    k = self.split_heads(self.k_proj(key))  # (B, H, Lk, D)\n",
        "    v = self.split_heads(self.v_proj(value)) # (B, H, Lv, D) with Lv=Lk\n",
        "    scores = q @ k.transpose(2, 3) / self.D**0.5   # (B, H, Lq, Lk)\n",
        "\n",
        "    if attn_mask is not None:\n",
        "      scores = scores.masked_fill(attn_mask, -torch.inf)  # (B, H, Lq, Lk)\n",
        "    if key_padding_mask is not None:\n",
        "      mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # (B, 1, 1, Lk)\n",
        "      scores = scores.masked_fill(mask, -torch.inf)  # (B, H, Lq, Lk)\n",
        "\n",
        "    weights = scores.softmax(dim=-1) # (B, H, Lq, Lk)\n",
        "    Z = self.dropout(weights) @ v # (B, H, Lq, D)\n",
        "    Z = Z.transpose(1, 2)\n",
        "    Z = Z.reshape(Z.size(0), Z.size(1), self.H * self.D)\n",
        "    return (self.out_proj(Z), weights)\n"
      ],
      "metadata": {
        "id": "28ndz1RlNr9U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder Layer"
      ],
      "metadata": {
        "id": "bqH0fx-QeZ1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, dim_model, n_heads, dim_ff=2048, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.self_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n",
        "    self.linear1 = nn.Linear(dim_model, dim_ff)\n",
        "    self.linear2 = nn.Linear(dim_ff, dim_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm1 = nn.LayerNorm(dim_model)\n",
        "    self.norm2 = nn.LayerNorm(dim_model)\n",
        "\n",
        "  def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "    attn, _ = self.self_attn(src, src, src, src_mask, src_key_padding_mask)\n",
        "    Z = self.norm1(src + self.dropout(attn))\n",
        "    ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n",
        "\n",
        "    return self.norm2(Z + ff)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "keTxTYYyeczb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Decoder Layer"
      ],
      "metadata": {
        "id": "Lm2Iwrv8go3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, dim_model, n_heads, dim_ff=2048, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.self_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n",
        "    self.multi_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n",
        "    self.linear1 = nn.Linear(dim_model, dim_ff)\n",
        "    self.linear2 = nn.Linear(dim_ff, dim_model)\n",
        "    self.norm1 = nn.LayerNorm(dim_model)\n",
        "    self.norm2 = nn.LayerNorm(dim_model)\n",
        "    self.norm3 = nn.LayerNorm(dim_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "              tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "    attn1, _ = self.self_attn(tgt, tgt, tgt,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)\n",
        "    Z = self.norm1(tgt + self.dropout(attn1))\n",
        "    attn2, _ = self.multi_attn(Z, memory, memory,\n",
        "                               attn_mask=memory_mask,\n",
        "                               key_padding_mask=memory_key_padding_mask)\n",
        "    Z = self.norm2(Z + self.dropout(attn2))\n",
        "    ff = self.dropout(self.linear1(self.dropout(self.linear2(Z).relu())))\n",
        "    return self.norm3(Z + ff)"
      ],
      "metadata": {
        "id": "kkuFQcCdgrQl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder"
      ],
      "metadata": {
        "id": "NpfiGPRHoReW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([deepcopy(encoder_layer)\n",
        "                                   for _ in range(num_layers)])\n",
        "    self.norm = norm\n",
        "\n",
        "  def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "    Z = src\n",
        "    for layer in self.layers:\n",
        "      Z = layer(Z, src_mask, src_key_padding_mask)\n",
        "\n",
        "    if self.norm is not None:\n",
        "      Z = self.norm(Z)\n",
        "    return Z"
      ],
      "metadata": {
        "id": "3s498sENoLyJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Decoder"
      ],
      "metadata": {
        "id": "3TAxBmhup13s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, decoder_layer, num_layers, norm=None):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([deepcopy(decoder_layer)\n",
        "                                  for _ in range(num_layers)])\n",
        "    self.norm = norm\n",
        "\n",
        "  def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "                    tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "    Z = tgt\n",
        "    for layer in self.layers:\n",
        "      Z = layer(Z, memory, tgt_mask, memory_mask,\n",
        "                tgt_key_padding_mask, memory_key_padding_mask)\n",
        "\n",
        "    if self.norm is not None:\n",
        "      Z = self.norm(Z)\n",
        "    return Z\n",
        ""
      ],
      "metadata": {
        "id": "vSnaMDZAp4JB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "NRJ8nduVrQ2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, d_model=512, n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n",
        "               dim_ff = 2048, dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    encoder_layer = TransformerEncoderLayer(d_model, n_heads, dim_ff, dropout)\n",
        "    norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.encoder = TransformerEncoder(encoder_layer, n_encoder_layers, norm1)\n",
        "\n",
        "    decoder_layer = TransformerDecoderLayer(d_model, n_heads, dim_ff, dropout)\n",
        "    norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.decoder = TransformerDecoder(decoder_layer, n_decoder_layers, norm2)\n",
        "\n",
        "\n",
        "  def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None ,\n",
        "                src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "    memory = self.encoder(src, src_mask, src_key_padding_mask)\n",
        "    output = self.decoder(tgt, memory, tgt_mask, memory_mask,\n",
        "                          tgt_key_padding_mask, memory_key_padding_mask)\n",
        "\n",
        "    return output\n",
        ""
      ],
      "metadata": {
        "id": "LxCo_g8rrQLc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building English-to-Hinglish Transformer"
      ],
      "metadata": {
        "id": "jZGquWj_yztY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NmtTransformer(nn.Module):\n",
        "  def __init__(self, vocab_size, max_length, embed_dim=512, pad_id=0,\n",
        "               num_heads=8, num_layers=6,dim_ff = 2048, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
        "    self.pos_embed = PositionalEncodings(max_length, embed_dim, dropout)\n",
        "    self.transformer = Transformer(embed_dim, num_heads, n_encoder_layers=num_layers,\n",
        "                                   n_decoder_layers=num_layers,\n",
        "                                   dim_ff=dim_ff, dropout=dropout)\n",
        "    self.output = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "  def forward(self, pair):\n",
        "    src_embeddings = self.pos_embed(self.embed(pair.src_token_ids))\n",
        "    tgt_embeddings = self.pos_embed(self.embed(pair.tgt_token_ids))\n",
        "    src_pad_mask = ~pair.src_mask.bool()\n",
        "    tgt_pad_mask = ~pair.tgt_mask.bool()\n",
        "    size = [pair.tgt_token_ids.size(1)] * 2     #line a\n",
        "    full_mask = torch.full(size, True, device=tgt_pad_mask.device) #line b\n",
        "    causal_mask = torch.triu(full_mask, diagonal=1)  #line c\n",
        "    output_decoder = self.transformer(src_embeddings,\n",
        "                                      tgt_embeddings,\n",
        "                                      tgt_mask=causal_mask,\n",
        "                                      src_key_padding_mask=src_pad_mask,\n",
        "                                      tgt_key_padding_mask=tgt_pad_mask,\n",
        "                                      memory_key_padding_mask=src_pad_mask)\n",
        "    return self.output(output_decoder).permute(0, 2, 1)\n"
      ],
      "metadata": {
        "id": "aVAdqcPLyv4c"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How line a, line b, line c works :**\n",
        "\n",
        "***Example:***\n",
        "\n",
        "seq_len = 5\n",
        "\n",
        "size = [seq_len] * 2 -> [5, 5]\n",
        "\n",
        "full_mask = torch.full(size, True)\n",
        "\n",
        "full_mask:\n",
        "\n",
        "tensor( [\n",
        "\n",
        "        [ True,  True,  True,  True,  True],\n",
        "        [ True,  True,  True,  True,  True],\n",
        "        [ True,  True,  True,  True,  True],\n",
        "        [ True,  True,  True,  True,  True],\n",
        "        [ True,  True,  True,  True,  True]\n",
        "\n",
        "        ])\n",
        "\n",
        "causal_mask = torch.triu(full_mask, diagonal=1)\n",
        "\n",
        "causal_mask:\n",
        "\n",
        "tensor([\n",
        "\n",
        "        [False,  True,  True,  True,  True],\n",
        "        [False, False,  True,  True,  True],\n",
        "        [False, False, False,  True,  True],\n",
        "        [False, False, False, False,  True],\n",
        "        [False, False, False, False, False]\n",
        "        \n",
        "        ])\n",
        "\n"
      ],
      "metadata": {
        "id": "qpCt5kYH7_ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data for NMT"
      ],
      "metadata": {
        "id": "ILJ9fBaM1Xu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = []\n",
        "filename = \"/content/hinglish_upload_v1.json\"\n",
        "\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "    obj = json.loads(line)\n",
        "    data.append({\n",
        "        \"English\":obj[\"translation\"][\"en\"],\n",
        "        \"Hinglish\":obj[\"translation\"][\"hi_ng\"]\n",
        "    })\n"
      ],
      "metadata": {
        "id": "BEZkS_l01ctK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.DataFrame(data)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "def preprocess_text(text):\n",
        "  return str(text).strip()\n",
        "\n",
        "en_sentence = df[\"English\"].apply(preprocess_text).tolist()\n",
        "hing_sentence = df[\"Hinglish\"].apply(preprocess_text).tolist()\n",
        "\n",
        "for i in range(3):\n",
        "    print(en_sentence[i], \"=>\", hing_sentence[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-mk4oV91hOF",
        "outputId": "b9b69d1b-bf8e-444a-e58d-3db2a11b96d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the forecast in the Caribbean for next week ? => agley haftey ke liye Caribbean me forecast kya hai\n",
            "Will it be windy today ? => kya aj windy hone wala he ?\n",
            "show me traffic on my way please => please mujhe mere raaste par traffic dikhao\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenizers\n",
        "def train_eng_hing():\n",
        "  for en, hi in zip(en_sentence, hing_sentence):\n",
        "    yield en\n",
        "    yield hi\n",
        "max_len = 500\n",
        "vocab_size = 10_000\n",
        "\n",
        "nmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"\")\n",
        "nmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\n",
        "nmt_tokenizer.enable_padding(pad_id=0, pad_token=\"\")\n",
        "nmt_tokenizer.enable_truncation(max_length=max_len)\n",
        "nmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
        "nmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(\n",
        "    vocab_size=vocab_size, special_tokens=[\"\", \"\", \"\", \"\"])\n",
        "nmt_tokenizer.train_from_iterator(train_eng_hing(), nmt_tokenizer_trainer)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ZwHf9krj1kKH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nmt_tokenizer.encode(\"I love football\").ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OY-nPzf1luo",
        "outputId": "7184ed5a-f629-4068-c2e9-819a72865b5e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[41, 1248, 2274]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nmt_tokenizer.encode(\"Muje football pasand hai.\").ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enWuUj8S1nbG",
        "outputId": "92f7941c-5faa-406e-9d98-4b6489670d53"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[755, 2274, 1849, 346, 14]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "fields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"]\n",
        "class NMTPair(namedtuple(\"NmtPairBase\", fields)):\n",
        "  def to(self,device):\n",
        "    return NMTPair(\n",
        "        self.src_token_ids.to(device),\n",
        "        self.src_mask.to(device),\n",
        "        self.tgt_token_ids.to(device),\n",
        "        self.tgt_mask.to(device)\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "TWOgmofv1pa7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nmt_collate_fn(batch):\n",
        "  src_text = [item[\"English\"] for item in batch]\n",
        "  tgt_text = [f\" {item['Hinglish']} \" for item in batch]\n",
        "  src_encodings = nmt_tokenizer.encode_batch(src_text)\n",
        "  tgt_encodings = nmt_tokenizer.encode_batch(tgt_text)\n",
        "  src_token_ids = torch.tensor([enc.ids for enc in src_encodings])\n",
        "  tgt_token_ids = torch.tensor([enc.ids for enc in tgt_encodings])\n",
        "  src_mask = torch.tensor([enc.attention_mask for enc in src_encodings])\n",
        "  tgt_mask = torch.tensor([enc.attention_mask for enc in tgt_encodings])\n",
        "  inputs = NMTPair(src_token_ids,\n",
        "                  src_mask,\n",
        "                  tgt_token_ids[:,:-1],\n",
        "                  tgt_mask[:,:-1])\n",
        "  labels =tgt_token_ids[:,1:]\n",
        "  return inputs, labels"
      ],
      "metadata": {
        "id": "NkUXTPKV1rgk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = df.to_dict(\"records\")[:int(0.8 * len(df))]\n",
        "valid_set = df.to_dict(\"records\")[int(0.8 * len(df)):]\n",
        "train_set[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce2v3NWx1tYn",
        "outputId": "75e21e6e-b82c-4774-8901-a6ed9393c2be"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'English': 'What is the forecast in the Caribbean for next week ?',\n",
              " 'Hinglish': 'agley haftey ke liye Caribbean me forecast kya hai'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=nmt_collate_fn,\n",
        "    shuffle=True\n",
        ")\n",
        "valid_loader =DataLoader(\n",
        "    valid_set,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=nmt_collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "KkknleCy1vO2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "\n",
        "def evaluate_tm(model, data_loader, metric):\n",
        "  model.eval()\n",
        "  metric.reset()\n",
        "  with torch.no_grad:\n",
        "    for X_batch, y_batch in data_loader:\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "      y_pred = model(X_batch)\n",
        "      metric.update(y_pred, y_batch)\n",
        "  return metric.compute()\n",
        "\n",
        "def train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs,\n",
        "          patience=2, factor=0.5):\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "      optimizer, mode='max', patience=patience, factor=factor\n",
        "  )\n",
        "  history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
        "  for epoch in range(n_epochs):\n",
        "    print(f\"Epoch:{epoch+1}/{n_epochs}\")\n",
        "    model.train()\n",
        "    metric.reset()\n",
        "    total_loss = 0\n",
        "    for idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "      y_pred = model(X_batch)\n",
        "      loss = criterion(y_pred, y_batch)\n",
        "      total_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      metric.update(y_pred, y_batch)\n",
        "      print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n",
        "      print(f\", loss ={total_loss/(idx+1 ):.4f} \", end=\"\")\n",
        "    mean_loss = total_loss / len(train_loader)\n",
        "    history[\"train_losses\"].append(mean_loss)\n",
        "    history[\"train_metrics\"].append(metric.compute().item())\n",
        "    val_metric = evaluate_tm(model, valid_loader, metric).item()\n",
        "    history[\"valid_metrics\"].append(val_metric)\n",
        "    scheduler.step(val_metric)\n",
        "    print(f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n",
        "             f\"Train Metric: {history['train_metrics'][-1]:.4f}%, \"\n",
        "             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}%\")\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "DJX5JrKi4eut"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "QtfvyX7x9Usq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NmtTransformer(vocab_size, max_len, embed_dim=512, pad_id=0, num_heads=4, num_layers=2,\n",
        "                       dropout=0.1).to(device)\n",
        "n_epochs = 20\n",
        "xentropy = nn.CrossEntropyLoss(ignore_index=0)\n",
        "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocab_size).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)\n",
        "\n",
        "history = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader,n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "voRgEyrT61Jh",
        "outputId": "e077587a-7b93-4020-985a-21741f118734"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (3968x512 and 2048x512)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-859844617.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2295837583.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs, patience, factor)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1476460655.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pair)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mfull_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_pad_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#line b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcausal_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagonal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#line c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     output_decoder = self.transformer(src_embeddings, \n\u001b[0m\u001b[1;32m     21\u001b[0m                                       \u001b[0mtgt_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                       \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1680639556.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n\u001b[1;32m     19\u001b[0m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     output = self.decoder(tgt, memory, tgt_mask, memory_mask,\n\u001b[0m\u001b[1;32m     21\u001b[0m                           tgt_key_padding_mask, memory_key_padding_mask)\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3601778445.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       Z = layer(Z, memory, tgt_mask, memory_mask, \n\u001b[0m\u001b[1;32m     13\u001b[0m                 tgt_key_padding_mask, memory_key_padding_mask)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2197572381.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                key_padding_mask=memory_key_padding_mask)\n\u001b[1;32m     22\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3968x512 and 2048x512)"
          ]
        }
      ]
    }
  ]
}