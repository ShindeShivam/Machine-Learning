{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13248595,"sourceType":"datasetVersion","datasetId":8394833}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport json","metadata":{"id":"YjBNqJTRCgQO","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:55.223672Z","iopub.execute_input":"2025-10-03T09:45:55.224430Z","iopub.status.idle":"2025-10-03T09:45:57.133571Z","shell.execute_reply.started":"2025-10-03T09:45:55.224401Z","shell.execute_reply":"2025-10-03T09:45:57.133035Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"if torch.cuda.is_available():\n  device = 'cuda'\n  print(torch.cuda.device_count())\nelif torch.backends.mps.is_available():\n  device = 'mps'\nelse:\n  device = 'cpu'\nprint(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bqYkafYCndQ","outputId":"3bf45a10-9a57-4e67-be1f-6695fb35623d","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:57.134843Z","iopub.execute_input":"2025-10-03T09:45:57.135130Z","iopub.status.idle":"2025-10-03T09:45:57.225568Z","shell.execute_reply.started":"2025-10-03T09:45:57.135111Z","shell.execute_reply":"2025-10-03T09:45:57.224876Z"}},"outputs":[{"name":"stdout","text":"2\ncuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Transformer Architecture","metadata":{"id":"j7bfqkfGCRSI"}},{"cell_type":"markdown","source":"## Positional encodings","metadata":{"id":"jcFLqG7pCXtc"}},{"cell_type":"code","source":"class PositionalEncodings(nn.Module):\n  def __init__(self, max_len, embed_dim, dropout=0.1):\n    super().__init__()\n    # pos_embed: learnable positional embeddings for all positions up to max_len\n    # Shape = [max_len, embed_dim]\n    # Example: if max_len=500 and embed_dim=512 → [500, 512]\n    self.pos_embed = nn.Parameter(torch.randn(max_len, embed_dim) * 0.02)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, X):\n    \"\"\"\n    X: token embeddings\n    Shape = [batch_size, seq_len, embed_dim]\n\n    self.pos_embed[:X.size(1)]:\n        - X.size(1) = seq_len\n        - So we take the first `seq_len` rows from pos_embed\n        - Shape = [seq_len, embed_dim]\n\n    Broadcasting when adding:\n        - X: [batch_size, seq_len, embed_dim]\n        - pos_embed[:seq_len]: [seq_len, embed_dim]\n        - Automatically broadcast to [1, seq_len, embed_dim] → [batch_size, seq_len, embed_dim]\n\n    Final output:\n        - Shape = [batch_size, seq_len, embed_dim]\n    \"\"\"\n    return self.dropout(X + self.pos_embed[:X.size(1)])\n\n","metadata":{"id":"dYBqTShgAxTQ","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:57.226296Z","iopub.execute_input":"2025-10-03T09:45:57.226505Z","iopub.status.idle":"2025-10-03T09:45:57.231713Z","shell.execute_reply.started":"2025-10-03T09:45:57.226487Z","shell.execute_reply":"2025-10-03T09:45:57.230970Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"max_len = 500\nembed_dim = 512\npos_embedding = PositionalEncodings(max_len, embed_dim)\nembeddings = torch.randn(256, 500, 512)\nembeddings_with_pos = pos_embedding(embeddings)\nembeddings_with_pos.shape\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAVJF2c6DIk-","outputId":"0c26feef-7032-4543-dcb6-50e431005fec","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:57.233308Z","iopub.execute_input":"2025-10-03T09:45:57.233502Z","iopub.status.idle":"2025-10-03T09:45:58.354598Z","shell.execute_reply.started":"2025-10-03T09:45:57.233486Z","shell.execute_reply":"2025-10-03T09:45:58.353915Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"torch.Size([256, 500, 512])"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"a = torch.tensor([1,2,3,4,5])\nb = torch.tensor([6,7,8,9,0])","metadata":{"id":"X4N130rNAJ5b","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.355309Z","iopub.execute_input":"2025-10-03T09:45:58.355581Z","iopub.status.idle":"2025-10-03T09:45:58.360193Z","shell.execute_reply.started":"2025-10-03T09:45:58.355556Z","shell.execute_reply":"2025-10-03T09:45:58.359544Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"c = torch.cat((a,b))\nc\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zHrK7MIXAJXi","outputId":"1aa0de92-675c-4884-d178-a34a23d5e574","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.361251Z","iopub.execute_input":"2025-10-03T09:45:58.361508Z","iopub.status.idle":"2025-10-03T09:45:58.379574Z","shell.execute_reply.started":"2025-10-03T09:45:58.361487Z","shell.execute_reply":"2025-10-03T09:45:58.378931Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 0])"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Multi-Head Attention","metadata":{"id":"itTAvHn8AnNS"}},{"cell_type":"markdown","source":"### How splitting works","metadata":{"id":"AHcbeGIHNj0n"}},{"cell_type":"code","source":"import torch\n\n# Input embeddings: (B, L, E) = (1, 3, 6)\nx = torch.tensor([[[1, 2, 3, 4, 5, 6],    # token 1 embedding\n                   [7, 8, 9, 10, 11, 12],   # token 2 embedding\n                   [13, 14, 15, 16, 17, 18]]])   # token 3 embedding\nprint(\"Input embeddings x:\", x)\nprint(\"Shape:\", x.shape)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"33oL-kQfD_ZL","outputId":"631f1c58-6a93-4c3a-a889-0a9c03d33f47","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.380374Z","iopub.execute_input":"2025-10-03T09:45:58.380585Z","iopub.status.idle":"2025-10-03T09:45:58.393242Z","shell.execute_reply.started":"2025-10-03T09:45:58.380570Z","shell.execute_reply":"2025-10-03T09:45:58.392521Z"}},"outputs":[{"name":"stdout","text":"Input embeddings x: tensor([[[ 1,  2,  3,  4,  5,  6],\n         [ 7,  8,  9, 10, 11, 12],\n         [13, 14, 15, 16, 17, 18]]])\nShape: torch.Size([1, 3, 6])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"B, L, E = x.shape\nH = 2\nD = E // H\nx_heads = x.view(B, L, H, D)  # (B, L, H, D)\nprint(x_heads.shape)\nx_heads","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XR85bL5aL_I9","outputId":"2456b442-638e-476d-8b31-2692c796478a","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.394029Z","iopub.execute_input":"2025-10-03T09:45:58.394297Z","iopub.status.idle":"2025-10-03T09:45:58.409961Z","shell.execute_reply.started":"2025-10-03T09:45:58.394281Z","shell.execute_reply":"2025-10-03T09:45:58.409204Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 3, 2, 3])\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"tensor([[[[ 1,  2,  3],\n          [ 4,  5,  6]],\n\n         [[ 7,  8,  9],\n          [10, 11, 12]],\n\n         [[13, 14, 15],\n          [16, 17, 18]]]])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"X = x_heads.transpose(1,2)  # (B, H, L, D)\nprint(X.shape)\nx_heads","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9UHfx8pdMN2J","outputId":"4ad66dca-6d17-41c6-f12a-ba6a32eaae0c","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.410761Z","iopub.execute_input":"2025-10-03T09:45:58.410917Z","iopub.status.idle":"2025-10-03T09:45:58.425172Z","shell.execute_reply.started":"2025-10-03T09:45:58.410905Z","shell.execute_reply":"2025-10-03T09:45:58.424488Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 2, 3, 3])\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor([[[[ 1,  2,  3],\n          [ 4,  5,  6]],\n\n         [[ 7,  8,  9],\n          [10, 11, 12]],\n\n         [[13, 14, 15],\n          [16, 17, 18]]]])"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### Custom MHA","metadata":{"id":"5GZckFVTSMd1"}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n  def __init__(self, embed_dim, num_heads, dropout=0.1):\n    super().__init__()\n    self.H = num_heads\n    self.D = embed_dim // num_heads\n    self.q_proj = nn.Linear(embed_dim, embed_dim)\n    self.k_proj = nn.Linear(embed_dim, embed_dim)\n    self.v_proj = nn.Linear(embed_dim, embed_dim)\n    self.out_proj = nn.Linear(embed_dim, embed_dim)\n    self.dropout = nn.Dropout(dropout)\n\n  def split_heads(self, X):\n    return X.view(X.size(0), X.size(1), self.H, self.D).transpose(1, 2)\n\n  def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n    q = self.split_heads(self.q_proj(query)) # (B, H, Lq, D)\n    k = self.split_heads(self.k_proj(key))  # (B, H, Lk, D)\n    v = self.split_heads(self.v_proj(value)) # (B, H, Lv, D) with Lv=Lk\n    scores = q @ k.transpose(2, 3) / self.D**0.5   # (B, H, Lq, Lk)\n\n    if attn_mask is not None:\n      scores = scores.masked_fill(attn_mask, -torch.inf)  # (B, H, Lq, Lk)\n    if key_padding_mask is not None:\n      mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # (B, 1, 1, Lk)\n      scores = scores.masked_fill(mask, -torch.inf)  # (B, H, Lq, Lk)\n\n    weights = scores.softmax(dim=-1) # (B, H, Lq, Lk)\n    Z = self.dropout(weights) @ v # (B, H, Lq, D)\n    Z = Z.transpose(1, 2)\n    Z = Z.reshape(Z.size(0), Z.size(1), self.H * self.D)\n    return (self.out_proj(Z), weights)\n","metadata":{"id":"28ndz1RlNr9U","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.427705Z","iopub.execute_input":"2025-10-03T09:45:58.428085Z","iopub.status.idle":"2025-10-03T09:45:58.441547Z","shell.execute_reply.started":"2025-10-03T09:45:58.428069Z","shell.execute_reply":"2025-10-03T09:45:58.441060Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Transformer Encoder Layer","metadata":{"id":"bqH0fx-QeZ1W"}},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n  def __init__(self, dim_model, n_heads, dim_ff=2048, dropout=0.1):\n    super().__init__()\n    self.self_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n    self.linear1 = nn.Linear(dim_model, dim_ff)\n    self.linear2 = nn.Linear(dim_ff, dim_model)\n    self.dropout = nn.Dropout(dropout)\n    self.norm1 = nn.LayerNorm(dim_model)\n    self.norm2 = nn.LayerNorm(dim_model)\n\n  def forward(self, src, src_mask=None, src_key_padding_mask=None):\n    attn, _ = self.self_attn(src, src, src, src_mask, src_key_padding_mask)\n    Z = self.norm1(src + self.dropout(attn))\n    ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n\n    return self.norm2(Z + ff)\n\n","metadata":{"id":"keTxTYYyeczb","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.442190Z","iopub.execute_input":"2025-10-03T09:45:58.442492Z","iopub.status.idle":"2025-10-03T09:45:58.468101Z","shell.execute_reply.started":"2025-10-03T09:45:58.442474Z","shell.execute_reply":"2025-10-03T09:45:58.467544Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Transformer Decoder Layer","metadata":{"id":"Lm2Iwrv8go3m"}},{"cell_type":"code","source":"class TransformerDecoderLayer(nn.Module):\n  def __init__(self, dim_model, n_heads, dim_ff=2048, dropout=0.1):\n    super().__init__()\n    self.self_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n    self.multi_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n    self.linear1 = nn.Linear(dim_model, dim_ff)\n    self.linear2 = nn.Linear(dim_ff, dim_model)\n    self.norm1 = nn.LayerNorm(dim_model)\n    self.norm2 = nn.LayerNorm(dim_model)\n    self.norm3 = nn.LayerNorm(dim_model)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n              tgt_key_padding_mask=None, memory_key_padding_mask=None):\n    attn1, _ = self.self_attn(tgt, tgt, tgt,\n                              attn_mask=tgt_mask,\n                              key_padding_mask=tgt_key_padding_mask)\n    Z = self.norm1(tgt + self.dropout(attn1))\n    attn2, _ = self.multi_attn(Z, memory, memory,\n                               attn_mask=memory_mask,\n                               key_padding_mask=memory_key_padding_mask)\n    Z = self.norm2(Z + self.dropout(attn2))\n    ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n    return self.norm3(Z + ff)","metadata":{"id":"kkuFQcCdgrQl","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.468702Z","iopub.execute_input":"2025-10-03T09:45:58.468908Z","iopub.status.idle":"2025-10-03T09:45:58.487443Z","shell.execute_reply.started":"2025-10-03T09:45:58.468893Z","shell.execute_reply":"2025-10-03T09:45:58.486917Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Transformer Encoder","metadata":{"id":"NpfiGPRHoReW"}},{"cell_type":"code","source":"from copy import deepcopy\n\nclass TransformerEncoder(nn.Module):\n  def __init__(self, encoder_layer, num_layers, norm=None):\n    super().__init__()\n    self.layers = nn.ModuleList([deepcopy(encoder_layer)\n                                   for _ in range(num_layers)])\n    self.norm = norm\n\n  def forward(self, src, src_mask=None, src_key_padding_mask=None):\n    Z = src\n    for layer in self.layers:\n      Z = layer(Z, src_mask, src_key_padding_mask)\n\n    if self.norm is not None:\n      Z = self.norm(Z)\n    return Z","metadata":{"id":"3s498sENoLyJ","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.488075Z","iopub.execute_input":"2025-10-03T09:45:58.488240Z","iopub.status.idle":"2025-10-03T09:45:58.503382Z","shell.execute_reply.started":"2025-10-03T09:45:58.488228Z","shell.execute_reply":"2025-10-03T09:45:58.502893Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Transformer Decoder","metadata":{"id":"3TAxBmhup13s"}},{"cell_type":"code","source":"class TransformerDecoder(nn.Module):\n  def __init__(self, decoder_layer, num_layers, norm=None):\n    super().__init__()\n    self.layers = nn.ModuleList([deepcopy(decoder_layer)\n                                  for _ in range(num_layers)])\n    self.norm = norm\n\n  def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n                    tgt_key_padding_mask=None, memory_key_padding_mask=None):\n    Z = tgt\n    for layer in self.layers:\n      Z = layer(Z, memory, tgt_mask, memory_mask,\n                tgt_key_padding_mask, memory_key_padding_mask)\n\n    if self.norm is not None:\n      Z = self.norm(Z)\n    return Z\n","metadata":{"id":"vSnaMDZAp4JB","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.503962Z","iopub.execute_input":"2025-10-03T09:45:58.504181Z","iopub.status.idle":"2025-10-03T09:45:58.525146Z","shell.execute_reply.started":"2025-10-03T09:45:58.504162Z","shell.execute_reply":"2025-10-03T09:45:58.524669Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Transformer","metadata":{"id":"NRJ8nduVrQ2Q"}},{"cell_type":"code","source":"class Transformer(nn.Module):\n  def __init__(self, d_model=512, n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n               dim_ff = 2048, dropout=0.1):\n    super().__init__()\n\n    encoder_layer = TransformerEncoderLayer(d_model, n_heads, dim_ff, dropout)\n    norm1 = nn.LayerNorm(d_model)\n\n    self.encoder = TransformerEncoder(encoder_layer, n_encoder_layers, norm1)\n\n    decoder_layer = TransformerDecoderLayer(d_model, n_heads, dim_ff, dropout)\n    norm2 = nn.LayerNorm(d_model)\n\n    self.decoder = TransformerDecoder(decoder_layer, n_decoder_layers, norm2)\n\n\n  def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None ,\n                src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n    memory = self.encoder(src, src_mask, src_key_padding_mask)\n    output = self.decoder(tgt, memory, tgt_mask, memory_mask,\n                          tgt_key_padding_mask, memory_key_padding_mask)\n\n    return output\n","metadata":{"id":"LxCo_g8rrQLc","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.525777Z","iopub.execute_input":"2025-10-03T09:45:58.526195Z","iopub.status.idle":"2025-10-03T09:45:58.540820Z","shell.execute_reply.started":"2025-10-03T09:45:58.526178Z","shell.execute_reply":"2025-10-03T09:45:58.540289Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Building English-to-Hinglish Transformer","metadata":{"id":"jZGquWj_yztY"}},{"cell_type":"code","source":"class NmtTransformer(nn.Module):\n  def __init__(self, vocab_size, max_length, embed_dim=512, pad_id=0,\n               num_heads=8, num_layers=6,dim_ff = 2048, dropout=0.1):\n    super().__init__()\n    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n    self.pos_embed = PositionalEncodings(max_length, embed_dim, dropout)\n    self.transformer = Transformer(embed_dim, num_heads, n_encoder_layers=num_layers,\n                                   n_decoder_layers=num_layers,\n                                   dim_ff=dim_ff, dropout=dropout)\n    self.output = nn.Linear(embed_dim, vocab_size)\n\n  def forward(self, pair):\n    src_embeddings = self.pos_embed(self.embed(pair.src_token_ids))\n    tgt_embeddings = self.pos_embed(self.embed(pair.tgt_token_ids))\n    src_pad_mask = ~pair.src_mask.bool()\n    tgt_pad_mask = ~pair.tgt_mask.bool()\n    size = [pair.tgt_token_ids.size(1)] * 2     #line a\n    full_mask = torch.full(size, True, device=tgt_pad_mask.device) #line b\n    causal_mask = torch.triu(full_mask, diagonal=1)  #line c\n    output_decoder = self.transformer(src_embeddings,\n                                      tgt_embeddings,\n                                      tgt_mask=causal_mask,\n                                      src_key_padding_mask=src_pad_mask,\n                                      tgt_key_padding_mask=tgt_pad_mask,\n                                      memory_key_padding_mask=src_pad_mask)\n    return self.output(output_decoder).permute(0, 2, 1)\n","metadata":{"id":"aVAdqcPLyv4c","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.541520Z","iopub.execute_input":"2025-10-03T09:45:58.541753Z","iopub.status.idle":"2025-10-03T09:45:58.560830Z","shell.execute_reply.started":"2025-10-03T09:45:58.541730Z","shell.execute_reply":"2025-10-03T09:45:58.560340Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**How line a, line b, line c works :**\n\n***Example:***\n\nseq_len = 5\n\nsize = [seq_len] * 2 -> [5, 5]\n\nfull_mask = torch.full(size, True)\n\nfull_mask:\n\ntensor( [\n\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True]\n\n        ])\n\ncausal_mask = torch.triu(full_mask, diagonal=1)\n\ncausal_mask:\n\ntensor([\n\n        [False,  True,  True,  True,  True],\n        [False, False,  True,  True,  True],\n        [False, False, False,  True,  True],\n        [False, False, False, False,  True],\n        [False, False, False, False, False]\n        \n        ])\n\n","metadata":{"id":"qpCt5kYH7_ua"}},{"cell_type":"markdown","source":"## Data for NMT","metadata":{"id":"ILJ9fBaM1Xu3"}},{"cell_type":"code","source":"\ndata = []\nfilename = \"/kaggle/input/english-hinglish-dataset/hinglish_upload_v1.json\"\n\nwith open(filename, \"r\", encoding=\"utf-8\") as f:\n  for line in f:\n    obj = json.loads(line)\n    data.append({\n        \"English\":obj[\"translation\"][\"en\"],\n        \"Hinglish\":obj[\"translation\"][\"hi_ng\"]\n    })\n","metadata":{"id":"BEZkS_l01ctK","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:58.561492Z","iopub.execute_input":"2025-10-03T09:45:58.561748Z","iopub.status.idle":"2025-10-03T09:45:59.120988Z","shell.execute_reply.started":"2025-10-03T09:45:58.561728Z","shell.execute_reply":"2025-10-03T09:45:59.120388Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\ndf = pd.DataFrame(data)\ndf = df.sample(frac=1).reset_index(drop=True)\n\ndef preprocess_text(text):\n  return str(text).strip()\n\nen_sentence = df[\"English\"].apply(preprocess_text).tolist()\nhing_sentence = df[\"Hinglish\"].apply(preprocess_text).tolist()\n\nfor i in range(3):\n    print(en_sentence[i], \"=>\", hing_sentence[i])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-mk4oV91hOF","outputId":"b9b69d1b-bf8e-444a-e58d-3db2a11b96d5","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:59.121770Z","iopub.execute_input":"2025-10-03T09:45:59.122277Z","iopub.status.idle":"2025-10-03T09:45:59.343503Z","shell.execute_reply.started":"2025-10-03T09:45:59.122252Z","shell.execute_reply":"2025-10-03T09:45:59.342876Z"}},"outputs":[{"name":"stdout","text":"What is the temperature for Mesa today ? => aaj Mesa ka temperature kya hai ?\nI had totally forgotten about using Napster. I use to always use it. Had no idea he co-f\nounded it. => Me Npaster ko use karne ke baare me totally bhool gaya tha. Me use hamesa use karne ka aadi hun. Mujhe koi idea nahi ki wo iska co-founder hai\nSet a night alarm at 10 pm . => 10 pm ko night alarm set kare\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import tokenizers\ndef train_eng_hing():\n  for en, hi in zip(en_sentence, hing_sentence):\n    yield en\n    yield hi\nmax_len = 500\nvocab_size = 10_000\n\nnmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"\")\nnmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\nnmt_tokenizer.enable_padding(pad_id=0, pad_token=\"\")\nnmt_tokenizer.enable_truncation(max_length=max_len)\nnmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\nnmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(\n    vocab_size=vocab_size, special_tokens=[\"\", \"\", \"\", \"\"])\nnmt_tokenizer.train_from_iterator(train_eng_hing(), nmt_tokenizer_trainer)\n\n","metadata":{"id":"ZwHf9krj1kKH","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:45:59.344224Z","iopub.execute_input":"2025-10-03T09:45:59.344469Z","iopub.status.idle":"2025-10-03T09:46:01.875562Z","shell.execute_reply.started":"2025-10-03T09:45:59.344451Z","shell.execute_reply":"2025-10-03T09:46:01.874814Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\nnmt_tokenizer.encode(\"I love football\").ids","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OY-nPzf1luo","outputId":"7184ed5a-f629-4068-c2e9-819a72865b5e","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:46:01.876421Z","iopub.execute_input":"2025-10-03T09:46:01.876610Z","iopub.status.idle":"2025-10-03T09:46:01.881902Z","shell.execute_reply.started":"2025-10-03T09:46:01.876596Z","shell.execute_reply":"2025-10-03T09:46:01.881209Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[41, 1248, 2274]"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"nmt_tokenizer.encode(\"Muje football pasand hai.\").ids","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"enWuUj8S1nbG","outputId":"92f7941c-5faa-406e-9d98-4b6489670d53","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:46:01.882747Z","iopub.execute_input":"2025-10-03T09:46:01.882957Z","iopub.status.idle":"2025-10-03T09:46:01.897661Z","shell.execute_reply.started":"2025-10-03T09:46:01.882943Z","shell.execute_reply":"2025-10-03T09:46:01.897098Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[755, 2274, 1849, 346, 14]"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from collections import namedtuple\n\nfields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"]\nclass NMTPair(namedtuple(\"NmtPairBase\", fields)):\n  def to(self,device):\n    return NMTPair(\n        self.src_token_ids.to(device),\n        self.src_mask.to(device),\n        self.tgt_token_ids.to(device),\n        self.tgt_mask.to(device)\n    )\n\n","metadata":{"id":"TWOgmofv1pa7","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:46:01.898274Z","iopub.execute_input":"2025-10-03T09:46:01.898456Z","iopub.status.idle":"2025-10-03T09:46:01.912225Z","shell.execute_reply.started":"2025-10-03T09:46:01.898442Z","shell.execute_reply":"2025-10-03T09:46:01.911698Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def nmt_collate_fn(batch):\n  src_text = [item[\"English\"] for item in batch]\n  tgt_text = [f\" {item['Hinglish']} \" for item in batch]\n  src_encodings = nmt_tokenizer.encode_batch(src_text)\n  tgt_encodings = nmt_tokenizer.encode_batch(tgt_text)\n  src_token_ids = torch.tensor([enc.ids for enc in src_encodings])\n  tgt_token_ids = torch.tensor([enc.ids for enc in tgt_encodings])\n  src_mask = torch.tensor([enc.attention_mask for enc in src_encodings])\n  tgt_mask = torch.tensor([enc.attention_mask for enc in tgt_encodings])\n  inputs = NMTPair(src_token_ids,\n                  src_mask,\n                  tgt_token_ids[:,:-1],\n                  tgt_mask[:,:-1])\n  labels =tgt_token_ids[:,1:]\n  return inputs, labels","metadata":{"id":"NkUXTPKV1rgk","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:46:01.912816Z","iopub.execute_input":"2025-10-03T09:46:01.912984Z","iopub.status.idle":"2025-10-03T09:46:01.934979Z","shell.execute_reply.started":"2025-10-03T09:46:01.912971Z","shell.execute_reply":"2025-10-03T09:46:01.934454Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"train_set = df.to_dict(\"records\")[:int(0.8 * len(df))]\nvalid_set = df.to_dict(\"records\")[int(0.8 * len(df)):]\ntrain_set[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ce2v3NWx1tYn","outputId":"75e21e6e-b82c-4774-8901-a6ed9393c2be","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:46:01.935514Z","iopub.execute_input":"2025-10-03T09:46:01.935699Z","iopub.status.idle":"2025-10-03T09:46:02.777437Z","shell.execute_reply.started":"2025-10-03T09:46:01.935686Z","shell.execute_reply":"2025-10-03T09:46:02.776607Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'English': 'What is the temperature for Mesa today ?',\n 'Hinglish': 'aaj Mesa ka temperature kya hai ?'}"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 64\ntrain_loader = DataLoader(\n    train_set,\n    batch_size=batch_size,\n    collate_fn=nmt_collate_fn,\n    shuffle=True\n)\nvalid_loader =DataLoader(\n    valid_set,\n    batch_size=batch_size,\n    collate_fn=nmt_collate_fn\n)","metadata":{"id":"KkknleCy1vO2","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:46:02.778470Z","iopub.execute_input":"2025-10-03T09:46:02.778771Z","iopub.status.idle":"2025-10-03T09:46:02.783009Z","shell.execute_reply.started":"2025-10-03T09:46:02.778746Z","shell.execute_reply":"2025-10-03T09:46:02.782396Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import torchmetrics\n\ndef evaluate_tm(model, data_loader, metric):\n  model.eval()\n  metric.reset()\n  with torch.no_grad():\n    for X_batch, y_batch in data_loader:\n      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n      y_pred = model(X_batch)\n      metric.update(y_pred, y_batch)\n  return metric.compute()\n\ndef train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs,\n          patience=2, factor=0.5):\n  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n      optimizer, mode='max', patience=patience, factor=factor\n  )\n  history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n  for epoch in range(n_epochs):\n    print(f\"Epoch:{epoch+1}/{n_epochs}\")\n    model.train()\n    metric.reset()\n    total_loss = 0\n    for idx, (X_batch, y_batch) in enumerate(train_loader):\n      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n      y_pred = model(X_batch)\n      loss = criterion(y_pred, y_batch)\n      total_loss += loss.item()\n      loss.backward()\n      optimizer.step()\n      optimizer.zero_grad()\n      metric.update(y_pred, y_batch)\n      print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n      print(f\", loss ={total_loss/(idx+1 ):.4f} \", end=\"\")\n    mean_loss = total_loss / len(train_loader)\n    history[\"train_losses\"].append(mean_loss)\n    history[\"train_metrics\"].append(metric.compute().item())\n    val_metric = evaluate_tm(model, valid_loader, metric).item()\n    history[\"valid_metrics\"].append(val_metric)\n    scheduler.step(val_metric)\n    print(f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n             f\"Train Metric: {history['train_metrics'][-1]:.4f}%, \"\n             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}%\")\n  print(\"Training Completed!\")\n  return history\n\n","metadata":{"id":"DJX5JrKi4eut","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:47:18.173560Z","iopub.execute_input":"2025-10-03T09:47:18.173903Z","iopub.status.idle":"2025-10-03T09:47:18.182647Z","shell.execute_reply.started":"2025-10-03T09:47:18.173879Z","shell.execute_reply":"2025-10-03T09:47:18.181895Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Train the Model","metadata":{"id":"QtfvyX7x9Usq"}},{"cell_type":"code","source":"model = NmtTransformer(vocab_size, max_len, embed_dim=512, pad_id=0, num_heads=4, num_layers=2,\n                       dropout=0.1).to(device)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nn_epochs = 20\nxentropy = nn.CrossEntropyLoss(ignore_index=0)\naccuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocab_size).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)\n\nhistory = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader,n_epochs)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"voRgEyrT61Jh","outputId":"e077587a-7b93-4020-985a-21741f118734","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:47:22.214015Z","iopub.execute_input":"2025-10-03T09:47:22.214291Z","iopub.status.idle":"2025-10-03T11:05:16.901031Z","shell.execute_reply.started":"2025-10-03T09:47:22.214270Z","shell.execute_reply":"2025-10-03T11:05:16.900329Z"}},"outputs":[{"name":"stdout","text":"Epoch:1/20\nBatch 2364/2364, loss =3.0808 Train Loss: 3.0808, Train Metric: 0.1155%, Valid Metric: 0.1391%\nEpoch:2/20\nBatch 2364/2364, loss =2.0311 Train Loss: 2.0311, Train Metric: 0.1406%, Valid Metric: 0.1506%\nEpoch:3/20\nBatch 2364/2364, loss =1.6668 Train Loss: 1.6668, Train Metric: 0.1521%, Valid Metric: 0.1562%\nEpoch:4/20\nBatch 2364/2364, loss =1.4612 Train Loss: 1.4612, Train Metric: 0.1596%, Valid Metric: 0.1596%\nEpoch:5/20\nBatch 2364/2364, loss =1.3197 Train Loss: 1.3197, Train Metric: 0.1647%, Valid Metric: 0.1624%\nEpoch:6/20\nBatch 2364/2364, loss =1.2122 Train Loss: 1.2122, Train Metric: 0.1682%, Valid Metric: 0.1651%\nEpoch:7/20\nBatch 2364/2364, loss =1.1250 Train Loss: 1.1250, Train Metric: 0.1723%, Valid Metric: 0.1668%\nEpoch:8/20\nBatch 2364/2364, loss =1.0546 Train Loss: 1.0546, Train Metric: 0.1746%, Valid Metric: 0.1683%\nEpoch:9/20\nBatch 2364/2364, loss =0.9939 Train Loss: 0.9939, Train Metric: 0.1774%, Valid Metric: 0.1695%\nEpoch:10/20\nBatch 2364/2364, loss =0.9409 Train Loss: 0.9409, Train Metric: 0.1796%, Valid Metric: 0.1713%\nEpoch:11/20\nBatch 2364/2364, loss =0.8940 Train Loss: 0.8940, Train Metric: 0.1814%, Valid Metric: 0.1721%\nEpoch:12/20\nBatch 2364/2364, loss =0.8548 Train Loss: 0.8548, Train Metric: 0.1835%, Valid Metric: 0.1732%\nEpoch:13/20\nBatch 2364/2364, loss =0.8156 Train Loss: 0.8156, Train Metric: 0.1849%, Valid Metric: 0.1740%\nEpoch:14/20\nBatch 2364/2364, loss =0.7838 Train Loss: 0.7838, Train Metric: 0.1880%, Valid Metric: 0.1752%\nEpoch:15/20\nBatch 2364/2364, loss =0.7530 Train Loss: 0.7530, Train Metric: 0.1882%, Valid Metric: 0.1753%\nEpoch:16/20\nBatch 2364/2364, loss =0.7260 Train Loss: 0.7260, Train Metric: 0.1889%, Valid Metric: 0.1761%\nEpoch:17/20\nBatch 2364/2364, loss =0.7043 Train Loss: 0.7043, Train Metric: 0.1894%, Valid Metric: 0.1765%\nEpoch:18/20\nBatch 2364/2364, loss =0.6790 Train Loss: 0.6790, Train Metric: 0.1913%, Valid Metric: 0.1770%\nEpoch:19/20\nBatch 2364/2364, loss =0.6560 Train Loss: 0.6560, Train Metric: 0.1917%, Valid Metric: 0.1778%\nEpoch:20/20\nBatch 2364/2364, loss =0.6353 Train Loss: 0.6353, Train Metric: 0.1944%, Valid Metric: 0.1778%\nTraining Completed!\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"def translate(model, src_text, max_len=20, eos_id=3):\n    tgt_text = \"\"\n    for idx in range(max_len):\n        batch, _ = nmt_collate_fn([{\"English\":src_text ,\n                                   \"Hinglish\":tgt_text}])\n        with torch.no_grad():\n            y_pred = model(batch.to(device))\n            y_token_ids = y_pred.argmax(dim=1)\n            next_token_id = y_token_ids[0, idx]\n        next_token = nmt_tokenizer.id_to_token(next_token_id)\n        tgt_text += \" \" + next_token\n        if next_token_id == eos_id:\n            break\n    return tgt_text.replace(\"\",\"\")\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:05:23.238085Z","iopub.execute_input":"2025-10-03T11:05:23.238770Z","iopub.status.idle":"2025-10-03T11:05:23.243861Z","shell.execute_reply.started":"2025-10-03T11:05:23.238745Z","shell.execute_reply":"2025-10-03T11:05:23.243071Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"\ntranslate(model, \"I love machine learning\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:05:25.772816Z","iopub.execute_input":"2025-10-03T11:05:25.773456Z","iopub.status.idle":"2025-10-03T11:05:25.858464Z","shell.execute_reply.started":"2025-10-03T11:05:25.773434Z","shell.execute_reply":"2025-10-03T11:05:25.857411Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_127732/1736714340.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I love machine learning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_127732/2017687984.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(model, src_text, max_len, eos_id)\u001b[0m\n\u001b[1;32m      5\u001b[0m                                    \"Hinglish\":tgt_text}])\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0my_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mnext_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_token_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_127732/3164250903.py\", line 14, in forward\n    tgt_embeddings = self.pos_embed(self.embed(pair.tgt_token_ids))\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\", line 190, in forward\n    return F.embedding(\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2551, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\n"],"ename":"RuntimeError","evalue":"Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_127732/3164250903.py\", line 14, in forward\n    tgt_embeddings = self.pos_embed(self.embed(pair.tgt_token_ids))\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\", line 190, in forward\n    return F.embedding(\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2551, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\n","output_type":"error"}],"execution_count":33}]}