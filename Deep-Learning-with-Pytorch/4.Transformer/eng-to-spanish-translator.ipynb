{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13248595,"sourceType":"datasetVersion","datasetId":8394833},{"sourceId":13255611,"sourceType":"datasetVersion","datasetId":8399703}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport json","metadata":{"id":"YjBNqJTRCgQO","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:48.287146Z","iopub.execute_input":"2025-10-06T02:42:48.287393Z","iopub.status.idle":"2025-10-06T02:42:52.638152Z","shell.execute_reply.started":"2025-10-06T02:42:48.287358Z","shell.execute_reply":"2025-10-06T02:42:52.637383Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"if torch.cuda.is_available():\n  device = 'cuda'\n  print(torch.cuda.device_count())\nelif torch.backends.mps.is_available():\n  device = 'mps'\nelse:\n  device = 'cpu'\nprint(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bqYkafYCndQ","outputId":"3bf45a10-9a57-4e67-be1f-6695fb35623d","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:52.640022Z","iopub.execute_input":"2025-10-06T02:42:52.640408Z","iopub.status.idle":"2025-10-06T02:42:52.746427Z","shell.execute_reply.started":"2025-10-06T02:42:52.640391Z","shell.execute_reply":"2025-10-06T02:42:52.745852Z"}},"outputs":[{"name":"stdout","text":"2\ncuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Transformer Architecture","metadata":{"id":"j7bfqkfGCRSI"}},{"cell_type":"markdown","source":"## Positional encodings","metadata":{"id":"jcFLqG7pCXtc"}},{"cell_type":"code","source":"class PositionalEncodings(nn.Module):\n  def __init__(self, max_len, embed_dim, dropout=0.1):\n    super().__init__()\n    # pos_embed: learnable positional embeddings for all positions up to max_len\n    # Shape = [max_len, embed_dim]\n    # Example: if max_len=500 and embed_dim=512 → [500, 512]\n    self.pos_embed = nn.Parameter(torch.randn(max_len, embed_dim) * 0.02)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, X):\n    \"\"\"\n    X: token embeddings\n    Shape = [batch_size, seq_len, embed_dim]\n\n    self.pos_embed[:X.size(1)]:\n        - X.size(1) = seq_len\n        - So we take the first `seq_len` rows from pos_embed\n        - Shape = [seq_len, embed_dim]\n\n    Broadcasting when adding:\n        - X: [batch_size, seq_len, embed_dim]\n        - pos_embed[:seq_len]: [seq_len, embed_dim]\n        - Automatically broadcast to [1, seq_len, embed_dim] → [batch_size, seq_len, embed_dim]\n\n    Final output:\n        - Shape = [batch_size, seq_len, embed_dim]\n    \"\"\"\n    return self.dropout(X + self.pos_embed[:X.size(1)])\n\n","metadata":{"id":"dYBqTShgAxTQ","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:52.747176Z","iopub.execute_input":"2025-10-06T02:42:52.747812Z","iopub.status.idle":"2025-10-06T02:42:52.752663Z","shell.execute_reply.started":"2025-10-06T02:42:52.747786Z","shell.execute_reply":"2025-10-06T02:42:52.752029Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"max_len = 500\nembed_dim = 512\npos_embedding = PositionalEncodings(max_len, embed_dim)\nembeddings = torch.randn(256, 500, 512)\nembeddings_with_pos = pos_embedding(embeddings)\nembeddings_with_pos.shape\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAVJF2c6DIk-","outputId":"0c26feef-7032-4543-dcb6-50e431005fec","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:52.753408Z","iopub.execute_input":"2025-10-06T02:42:52.753568Z","iopub.status.idle":"2025-10-06T02:42:53.903595Z","shell.execute_reply.started":"2025-10-06T02:42:52.753554Z","shell.execute_reply":"2025-10-06T02:42:53.903007Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"torch.Size([256, 500, 512])"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"a = torch.tensor([1,2,3,4,5])\nb = torch.tensor([6,7,8,9,0])","metadata":{"id":"X4N130rNAJ5b","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:53.904242Z","iopub.execute_input":"2025-10-06T02:42:53.904433Z","iopub.status.idle":"2025-10-06T02:42:53.909124Z","shell.execute_reply.started":"2025-10-06T02:42:53.904419Z","shell.execute_reply":"2025-10-06T02:42:53.908455Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"c = torch.cat((a,b))\nc\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zHrK7MIXAJXi","outputId":"1aa0de92-675c-4884-d178-a34a23d5e574","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:53.909891Z","iopub.execute_input":"2025-10-06T02:42:53.910085Z","iopub.status.idle":"2025-10-06T02:42:53.934485Z","shell.execute_reply.started":"2025-10-06T02:42:53.910070Z","shell.execute_reply":"2025-10-06T02:42:53.933975Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 0])"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Multi-Head Attention","metadata":{"id":"itTAvHn8AnNS"}},{"cell_type":"markdown","source":"### How splitting works","metadata":{"id":"AHcbeGIHNj0n"}},{"cell_type":"code","source":"import torch\n\n# Input embeddings: (B, L, E) = (1, 3, 6)\nx = torch.tensor([[[1, 2, 3, 4, 5, 6],    # token 1 embedding\n                   [7, 8, 9, 10, 11, 12],   # token 2 embedding\n                   [13, 14, 15, 16, 17, 18]]])   # token 3 embedding\nprint(\"Input embeddings x:\", x)\nprint(\"Shape:\", x.shape)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"33oL-kQfD_ZL","outputId":"631f1c58-6a93-4c3a-a889-0a9c03d33f47","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:53.936811Z","iopub.execute_input":"2025-10-06T02:42:53.937004Z","iopub.status.idle":"2025-10-06T02:42:53.943275Z","shell.execute_reply.started":"2025-10-06T02:42:53.936989Z","shell.execute_reply":"2025-10-06T02:42:53.942591Z"}},"outputs":[{"name":"stdout","text":"Input embeddings x: tensor([[[ 1,  2,  3,  4,  5,  6],\n         [ 7,  8,  9, 10, 11, 12],\n         [13, 14, 15, 16, 17, 18]]])\nShape: torch.Size([1, 3, 6])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"B, L, E = x.shape\nH = 2\nD = E // H\nx_heads = x.view(B, L, H, D)  # (B, L, H, D)\nprint(x_heads.shape)\nx_heads","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XR85bL5aL_I9","outputId":"2456b442-638e-476d-8b31-2692c796478a","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:53.944028Z","iopub.execute_input":"2025-10-06T02:42:53.944240Z","iopub.status.idle":"2025-10-06T02:42:53.954844Z","shell.execute_reply.started":"2025-10-06T02:42:53.944216Z","shell.execute_reply":"2025-10-06T02:42:53.954145Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 3, 2, 3])\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"tensor([[[[ 1,  2,  3],\n          [ 4,  5,  6]],\n\n         [[ 7,  8,  9],\n          [10, 11, 12]],\n\n         [[13, 14, 15],\n          [16, 17, 18]]]])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"X = x_heads.transpose(1,2)  # (B, H, L, D)\nprint(X.shape)\nx_heads","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9UHfx8pdMN2J","outputId":"4ad66dca-6d17-41c6-f12a-ba6a32eaae0c","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:53.955520Z","iopub.execute_input":"2025-10-06T02:42:53.955821Z","iopub.status.idle":"2025-10-06T02:42:53.966301Z","shell.execute_reply.started":"2025-10-06T02:42:53.955795Z","shell.execute_reply":"2025-10-06T02:42:53.965618Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 2, 3, 3])\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor([[[[ 1,  2,  3],\n          [ 4,  5,  6]],\n\n         [[ 7,  8,  9],\n          [10, 11, 12]],\n\n         [[13, 14, 15],\n          [16, 17, 18]]]])"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### Custom MHA","metadata":{"id":"5GZckFVTSMd1"}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n  def __init__(self, embed_dim, num_heads, dropout=0.1):\n    super().__init__()\n    self.H = num_heads\n    self.D = embed_dim // num_heads\n    self.q_proj = nn.Linear(embed_dim, embed_dim)\n    self.k_proj = nn.Linear(embed_dim, embed_dim)\n    self.v_proj = nn.Linear(embed_dim, embed_dim)\n    self.out_proj = nn.Linear(embed_dim, embed_dim)\n    self.dropout = nn.Dropout(dropout)\n\n  def split_heads(self, X):\n    return X.view(X.size(0), X.size(1), self.H, self.D).transpose(1, 2)\n\n  def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n    q = self.split_heads(self.q_proj(query)) # (B, H, Lq, D)\n    k = self.split_heads(self.k_proj(key))  # (B, H, Lk, D)\n    v = self.split_heads(self.v_proj(value)) # (B, H, Lv, D) with Lv=Lk\n    scores = q @ k.transpose(2, 3) / self.D**0.5   # (B, H, Lq, Lk)\n\n    if attn_mask is not None:\n      scores = scores.masked_fill(attn_mask, -torch.inf)  # (B, H, Lq, Lk)\n    if key_padding_mask is not None:\n      mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # (B, 1, 1, Lk)\n      scores = scores.masked_fill(mask, -torch.inf)  # (B, H, Lq, Lk)\n\n    weights = scores.softmax(dim=-1) # (B, H, Lq, Lk)\n    Z = self.dropout(weights) @ v # (B, H, Lq, D)\n    Z = Z.transpose(1, 2)\n    Z = Z.reshape(Z.size(0), Z.size(1), self.H * self.D)\n    return (self.out_proj(Z), weights)\n","metadata":{"id":"28ndz1RlNr9U","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:53.967009Z","iopub.execute_input":"2025-10-06T02:42:53.967412Z","iopub.status.idle":"2025-10-06T02:42:53.980223Z","shell.execute_reply.started":"2025-10-06T02:42:53.967394Z","shell.execute_reply":"2025-10-06T02:42:53.979689Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Transformer Encoder Layer","metadata":{"id":"bqH0fx-QeZ1W"}},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n  def __init__(self, dim_model, n_heads, dim_ff=2048, dropout=0.1):\n    super().__init__()\n    self.self_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n    self.linear1 = nn.Linear(dim_model, dim_ff)\n    self.linear2 = nn.Linear(dim_ff, dim_model)\n    self.dropout = nn.Dropout(dropout)\n    self.norm1 = nn.LayerNorm(dim_model)\n    self.norm2 = nn.LayerNorm(dim_model)\n\n  def forward(self, src, src_mask=None, src_key_padding_mask=None):\n    attn, _ = self.self_attn(src, src, src, src_mask, src_key_padding_mask)\n    Z = self.norm1(src + self.dropout(attn))\n    ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n\n    return self.norm2(Z + ff)\n\n","metadata":{"id":"keTxTYYyeczb","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:53.980885Z","iopub.execute_input":"2025-10-06T02:42:53.981143Z","iopub.status.idle":"2025-10-06T02:42:53.993615Z","shell.execute_reply.started":"2025-10-06T02:42:53.981120Z","shell.execute_reply":"2025-10-06T02:42:53.993014Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Transformer Decoder Layer","metadata":{"id":"Lm2Iwrv8go3m"}},{"cell_type":"code","source":"class TransformerDecoderLayer(nn.Module):\n  def __init__(self, dim_model, n_heads, dim_ff=2048, dropout=0.1):\n    super().__init__()\n    self.self_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n    self.multi_attn = MultiHeadAttention(dim_model, n_heads, dropout)\n    self.linear1 = nn.Linear(dim_model, dim_ff)\n    self.linear2 = nn.Linear(dim_ff, dim_model)\n    self.norm1 = nn.LayerNorm(dim_model)\n    self.norm2 = nn.LayerNorm(dim_model)\n    self.norm3 = nn.LayerNorm(dim_model)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n              tgt_key_padding_mask=None, memory_key_padding_mask=None):\n    attn1, _ = self.self_attn(tgt, tgt, tgt,\n                              attn_mask=tgt_mask,\n                              key_padding_mask=tgt_key_padding_mask)\n    Z = self.norm1(tgt + self.dropout(attn1))\n    attn2, _ = self.multi_attn(Z, memory, memory,\n                               attn_mask=memory_mask,\n                               key_padding_mask=memory_key_padding_mask)\n    Z = self.norm2(Z + self.dropout(attn2))\n    ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n    return self.norm3(Z + ff)","metadata":{"id":"kkuFQcCdgrQl","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:53.994492Z","iopub.execute_input":"2025-10-06T02:42:53.995250Z","iopub.status.idle":"2025-10-06T02:42:54.007280Z","shell.execute_reply.started":"2025-10-06T02:42:53.995229Z","shell.execute_reply":"2025-10-06T02:42:54.006751Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Transformer Encoder","metadata":{"id":"NpfiGPRHoReW"}},{"cell_type":"code","source":"from copy import deepcopy\n\nclass TransformerEncoder(nn.Module):\n  def __init__(self, encoder_layer, num_layers, norm=None):\n    super().__init__()\n    self.layers = nn.ModuleList([deepcopy(encoder_layer)\n                                   for _ in range(num_layers)])\n    self.norm = norm\n\n  def forward(self, src, src_mask=None, src_key_padding_mask=None):\n    Z = src\n    for layer in self.layers:\n      Z = layer(Z, src_mask, src_key_padding_mask)\n\n    if self.norm is not None:\n      Z = self.norm(Z)\n    return Z","metadata":{"id":"3s498sENoLyJ","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:54.007968Z","iopub.execute_input":"2025-10-06T02:42:54.008146Z","iopub.status.idle":"2025-10-06T02:42:54.018024Z","shell.execute_reply.started":"2025-10-06T02:42:54.008131Z","shell.execute_reply":"2025-10-06T02:42:54.017283Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Transformer Decoder","metadata":{"id":"3TAxBmhup13s"}},{"cell_type":"code","source":"class TransformerDecoder(nn.Module):\n  def __init__(self, decoder_layer, num_layers, norm=None):\n    super().__init__()\n    self.layers = nn.ModuleList([deepcopy(decoder_layer)\n                                  for _ in range(num_layers)])\n    self.norm = norm\n\n  def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n                    tgt_key_padding_mask=None, memory_key_padding_mask=None):\n    Z = tgt\n    for layer in self.layers:\n      Z = layer(Z, memory, tgt_mask, memory_mask,\n                tgt_key_padding_mask, memory_key_padding_mask)\n\n    if self.norm is not None:\n      Z = self.norm(Z)\n    return Z\n","metadata":{"id":"vSnaMDZAp4JB","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:54.018789Z","iopub.execute_input":"2025-10-06T02:42:54.019502Z","iopub.status.idle":"2025-10-06T02:42:54.029147Z","shell.execute_reply.started":"2025-10-06T02:42:54.019478Z","shell.execute_reply":"2025-10-06T02:42:54.028621Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Transformer","metadata":{"id":"NRJ8nduVrQ2Q"}},{"cell_type":"code","source":"class Transformer(nn.Module):\n  def __init__(self, d_model=512, n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n               dim_ff = 2048, dropout=0.1):\n    super().__init__()\n\n    encoder_layer = TransformerEncoderLayer(d_model, n_heads, dim_ff, dropout)\n    norm1 = nn.LayerNorm(d_model)\n\n    self.encoder = TransformerEncoder(encoder_layer, n_encoder_layers, norm1)\n\n    decoder_layer = TransformerDecoderLayer(d_model, n_heads, dim_ff, dropout)\n    norm2 = nn.LayerNorm(d_model)\n\n    self.decoder = TransformerDecoder(decoder_layer, n_decoder_layers, norm2)\n\n\n  def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None ,\n                src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n    memory = self.encoder(src, src_mask, src_key_padding_mask)\n    output = self.decoder(tgt, memory, tgt_mask, memory_mask,\n                          tgt_key_padding_mask, memory_key_padding_mask)\n\n    return output\n","metadata":{"id":"LxCo_g8rrQLc","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:54.029813Z","iopub.execute_input":"2025-10-06T02:42:54.030170Z","iopub.status.idle":"2025-10-06T02:42:54.041566Z","shell.execute_reply.started":"2025-10-06T02:42:54.030154Z","shell.execute_reply":"2025-10-06T02:42:54.041007Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Building English-to-Hinglish Transformer","metadata":{"id":"jZGquWj_yztY"}},{"cell_type":"code","source":"class NmtTransformer(nn.Module):\n  def __init__(self, vocab_size, max_length, embed_dim=512, pad_id=0,\n               num_heads=8, num_layers=6,dim_ff = 2048, dropout=0.1):\n    super().__init__()\n    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n    self.pos_embed = PositionalEncodings(max_length, embed_dim, dropout)\n    self.transformer = Transformer(embed_dim, num_heads, n_encoder_layers=num_layers,\n                                   n_decoder_layers=num_layers,\n                                   dim_ff=dim_ff, dropout=dropout)\n    self.output = nn.Linear(embed_dim, vocab_size)\n\n  def forward(self, pair):\n    src_embeddings = self.pos_embed(self.embed(pair.src_token_ids))\n    tgt_embeddings = self.pos_embed(self.embed(pair.tgt_token_ids))\n    src_pad_mask = ~pair.src_mask.bool()\n    tgt_pad_mask = ~pair.tgt_mask.bool()\n    size = [pair.tgt_token_ids.size(1)] * 2     #line a\n    full_mask = torch.full(size, True, device=tgt_pad_mask.device) #line b\n    causal_mask = torch.triu(full_mask, diagonal=1)  #line c\n    output_decoder = self.transformer(src_embeddings,\n                                      tgt_embeddings,\n                                      tgt_mask=causal_mask,\n                                      src_key_padding_mask=src_pad_mask,\n                                      tgt_key_padding_mask=tgt_pad_mask,\n                                      memory_key_padding_mask=src_pad_mask)\n    return self.output(output_decoder).permute(0, 2, 1)\n","metadata":{"id":"aVAdqcPLyv4c","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:54.042260Z","iopub.execute_input":"2025-10-06T02:42:54.042484Z","iopub.status.idle":"2025-10-06T02:42:54.055215Z","shell.execute_reply.started":"2025-10-06T02:42:54.042469Z","shell.execute_reply":"2025-10-06T02:42:54.054512Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**How line a, line b, line c works :**\n\n***Example:***\n\nseq_len = 5\n\nsize = [seq_len] * 2 -> [5, 5]\n\nfull_mask = torch.full(size, True)\n\nfull_mask:\n\ntensor( [\n\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True]\n\n        ])\n\ncausal_mask = torch.triu(full_mask, diagonal=1)\n\ncausal_mask:\n\ntensor([\n\n        [False,  True,  True,  True,  True],\n        [False, False,  True,  True,  True],\n        [False, False, False,  True,  True],\n        [False, False, False, False,  True],\n        [False, False, False, False, False]\n        \n        ])\n\n","metadata":{"id":"qpCt5kYH7_ua"}},{"cell_type":"markdown","source":"## Data for NMT","metadata":{"id":"ILJ9fBaM1Xu3"}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom torch.utils.data import random_split, DataLoader\nimport tokenizers\n\nnmt_dataset = load_dataset(\"Helsinki-NLP/tatoeba_mt\", language_pair=\"eng-spa\",\n                           trust_remote_code=True)  # only if you trust the user\ntorch.manual_seed(42)\nnmt_train_set, nmt_valid_set = random_split(nmt_dataset[\"validation\"],\n                                            [0.8, 0.2])\nnmt_test_set = nmt_dataset[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:54.144685Z","iopub.execute_input":"2025-10-06T02:42:54.144891Z","iopub.status.idle":"2025-10-06T02:43:04.687525Z","shell.execute_reply.started":"2025-10-06T02:42:54.144877Z","shell.execute_reply":"2025-10-06T02:43:04.687012Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e4e70ea38cd4d16a3937c5d56ef596b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tatoeba_mt.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c41f9b5d441548f0a9b882c3cf47c599"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dc810fc1c864201a92b66a91f9fa18b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test/tatoeba-test.eng-spa.tsv:   0%|          | 0.00/1.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e327f283ad13409cb67934a420edb41d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev/tatoeba-dev.eng-spa.tsv:   0%|          | 0.00/16.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5157705672b54be5a983ad5c7f84f255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8cd47812c6140de9bc77be61c6828ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa5b2ebe158c4b708026521caf5c8b48"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"def train_eng_spa():  # a generator function to iterate over all training text\n    for pair in nmt_train_set:\n        yield pair[\"sourceString\"]\n        yield pair[\"targetString\"]\n\nmax_length = 500\nvocab_size = 10_000\nnmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"<unk>\")\nnmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\nnmt_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\nnmt_tokenizer.enable_truncation(max_length=max_length)\nnmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\nnmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(\n    vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\nnmt_tokenizer.train_from_iterator(train_eng_spa(), nmt_tokenizer_trainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:43:04.688165Z","iopub.execute_input":"2025-10-06T02:43:04.688373Z","iopub.status.idle":"2025-10-06T02:43:15.392675Z","shell.execute_reply.started":"2025-10-06T02:43:04.688357Z","shell.execute_reply":"2025-10-06T02:43:15.392047Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"from collections import namedtuple\n\nfields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"]\nclass NmtPair(namedtuple(\"NmtPairBase\", fields)):\n    def to(self, device):\n        return NmtPair(self.src_token_ids.to(device), self.src_mask.to(device), \n                       self.tgt_token_ids.to(device), self.tgt_mask.to(device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:43:15.393382Z","iopub.execute_input":"2025-10-06T02:43:15.393640Z","iopub.status.idle":"2025-10-06T02:43:15.398255Z","shell.execute_reply.started":"2025-10-06T02:43:15.393616Z","shell.execute_reply":"2025-10-06T02:43:15.397529Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def nmt_collate_fn(batch):\n    src_texts = [pair['sourceString'] for pair in batch]\n    tgt_texts = [f\"<s> {pair['targetString']} </s>\" for pair in batch]\n    src_encodings = nmt_tokenizer.encode_batch(src_texts)\n    tgt_encodings = nmt_tokenizer.encode_batch(tgt_texts)\n    src_token_ids = torch.tensor([enc.ids for enc in src_encodings])\n    tgt_token_ids = torch.tensor([enc.ids for enc in tgt_encodings])\n    src_mask = torch.tensor([enc.attention_mask for enc in src_encodings])\n    tgt_mask = torch.tensor([enc.attention_mask for enc in tgt_encodings])\n    inputs = NmtPair(src_token_ids, src_mask,\n                     tgt_token_ids[:, :-1], tgt_mask[:, :-1])\n    labels = tgt_token_ids[:, 1:]\n    return inputs, labels\n\nbatch_size = 64\ntrain_loader = DataLoader(nmt_train_set, batch_size=batch_size,\n                              collate_fn=nmt_collate_fn, shuffle=True)\nvalid_loader = DataLoader(nmt_valid_set, batch_size=batch_size,\n                              collate_fn=nmt_collate_fn)\ntest_loader = DataLoader(nmt_test_set, batch_size=batch_size,\n                             collate_fn=nmt_collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:43:15.399092Z","iopub.execute_input":"2025-10-06T02:43:15.399343Z","iopub.status.idle":"2025-10-06T02:43:15.424289Z","shell.execute_reply.started":"2025-10-06T02:43:15.399323Z","shell.execute_reply":"2025-10-06T02:43:15.423783Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import torchmetrics\n\ndef evaluate_tm(model, data_loader, metric):\n  model.eval()\n  metric.reset()\n  with torch.no_grad():\n    for X_batch, y_batch in data_loader:\n      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n      y_pred = model(X_batch)\n      metric.update(y_pred, y_batch)\n  return metric.compute()\n\ndef train(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs,\n          patience=2, factor=0.5):\n  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n      optimizer, mode='max', patience=patience, factor=factor\n  )\n  history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n  for epoch in range(n_epochs):\n    print(f\"Epoch:{epoch+1}/{n_epochs}\")\n    model.train()\n    metric.reset()\n    total_loss = 0\n    for idx, (X_batch, y_batch) in enumerate(train_loader):\n      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n      y_pred = model(X_batch)\n      loss = criterion(y_pred, y_batch)\n      total_loss += loss.item()\n      loss.backward()\n      optimizer.step()\n      optimizer.zero_grad()\n      metric.update(y_pred, y_batch)\n      print(f\"\\rBatch {idx+1}/{len(train_loader)}\", end=\"\")\n      print(f\", loss ={total_loss/(idx+1 ):.4f} \", end=\"\")\n    mean_loss = total_loss / len(train_loader)\n    history[\"train_losses\"].append(mean_loss)\n    history[\"train_metrics\"].append(metric.compute().item())\n    val_metric = evaluate_tm(model, valid_loader, metric).item()\n    history[\"valid_metrics\"].append(val_metric)\n    scheduler.step(val_metric)\n    print(f\"Train Loss: {history['train_losses'][-1]:.4f}, \"\n             f\"Train Metric: {history['train_metrics'][-1]:.4f}%, \"\n             f\"Valid Metric: {history['valid_metrics'][-1]:.4f}%\")\n  print(\"Training Completed!\")\n  return history\n\n","metadata":{"id":"DJX5JrKi4eut","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:43:15.425171Z","iopub.execute_input":"2025-10-06T02:43:15.425414Z","iopub.status.idle":"2025-10-06T02:43:22.595767Z","shell.execute_reply.started":"2025-10-06T02:43:15.425391Z","shell.execute_reply":"2025-10-06T02:43:22.595156Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Train the Model","metadata":{"id":"QtfvyX7x9Usq"}},{"cell_type":"code","source":"model = NmtTransformer(vocab_size, max_len, embed_dim=128, pad_id=0, num_heads=8, num_layers=4,dim_ff=1024,\n                       dropout=0.1).to(device)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\nmodel = torch.compile(model, mode='reduce-overhead')\n\nn_epochs = 20\nxentropy = nn.CrossEntropyLoss(ignore_index=0)\naccuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocab_size).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)\n\nhistory = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader,n_epochs)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"voRgEyrT61Jh","outputId":"e077587a-7b93-4020-985a-21741f118734","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:43:22.596429Z","iopub.execute_input":"2025-10-06T02:43:22.596885Z","iopub.status.idle":"2025-10-06T03:46:20.443877Z","shell.execute_reply.started":"2025-10-06T02:43:22.596864Z","shell.execute_reply":"2025-10-06T03:46:20.443098Z"}},"outputs":[{"name":"stderr","text":"W1006 02:43:24.719000 36 torch/_logging/_internal.py:1089] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n","output_type":"stream"},{"name":"stdout","text":"Epoch:1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin sys._getframe. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n  torch._dynamo.utils.warn_once(msg)\n","output_type":"stream"},{"name":"stdout","text":"Batch 2467/2467, loss =4.1975 Train Loss: 4.1975, Train Metric: 0.1092%, Valid Metric: 0.1439%\nEpoch:2/20\nBatch 2467/2467, loss =2.8167 Train Loss: 2.8167, Train Metric: 0.1504%, Valid Metric: 0.1699%\nEpoch:3/20\nBatch 2467/2467, loss =2.3142 Train Loss: 2.3142, Train Metric: 0.1668%, Valid Metric: 0.1808%\nEpoch:4/20\nBatch 2467/2467, loss =2.0560 Train Loss: 2.0560, Train Metric: 0.1769%, Valid Metric: 0.1878%\nEpoch:5/20\nBatch 2467/2467, loss =1.8938 Train Loss: 1.8938, Train Metric: 0.1832%, Valid Metric: 0.1928%\nEpoch:6/20\nBatch 2467/2467, loss =1.7783 Train Loss: 1.7783, Train Metric: 0.1869%, Valid Metric: 0.1963%\nEpoch:7/20\nBatch 2467/2467, loss =1.6887 Train Loss: 1.6887, Train Metric: 0.1918%, Valid Metric: 0.1980%\nEpoch:8/20\nBatch 2467/2467, loss =1.6169 Train Loss: 1.6169, Train Metric: 0.1941%, Valid Metric: 0.2008%\nEpoch:9/20\nBatch 2467/2467, loss =1.5585 Train Loss: 1.5585, Train Metric: 0.1968%, Valid Metric: 0.2027%\nEpoch:10/20\nBatch 2467/2467, loss =1.5076 Train Loss: 1.5076, Train Metric: 0.1991%, Valid Metric: 0.2043%\nEpoch:11/20\nBatch 2467/2467, loss =1.4653 Train Loss: 1.4653, Train Metric: 0.1999%, Valid Metric: 0.2057%\nEpoch:12/20\nBatch 2467/2467, loss =1.4259 Train Loss: 1.4259, Train Metric: 0.2020%, Valid Metric: 0.2067%\nEpoch:13/20\nBatch 2467/2467, loss =1.3916 Train Loss: 1.3916, Train Metric: 0.2039%, Valid Metric: 0.2078%\nEpoch:14/20\nBatch 2467/2467, loss =1.3628 Train Loss: 1.3628, Train Metric: 0.2045%, Valid Metric: 0.2087%\nEpoch:15/20\nBatch 2467/2467, loss =1.3360 Train Loss: 1.3360, Train Metric: 0.2062%, Valid Metric: 0.2091%\nEpoch:16/20\nBatch 2467/2467, loss =1.3108 Train Loss: 1.3108, Train Metric: 0.2064%, Valid Metric: 0.2098%\nEpoch:17/20\nBatch 2467/2467, loss =1.2872 Train Loss: 1.2872, Train Metric: 0.2081%, Valid Metric: 0.2111%\nEpoch:18/20\nBatch 2467/2467, loss =1.2677 Train Loss: 1.2677, Train Metric: 0.2099%, Valid Metric: 0.2112%\nEpoch:19/20\nBatch 2467/2467, loss =1.2491 Train Loss: 1.2491, Train Metric: 0.2093%, Valid Metric: 0.2121%\nEpoch:20/20\nBatch 2467/2467, loss =1.2296 Train Loss: 1.2296, Train Metric: 0.2101%, Valid Metric: 0.2122%\nTraining Completed!\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T03:46:20.444771Z","iopub.execute_input":"2025-10-06T03:46:20.444974Z","iopub.status.idle":"2025-10-06T03:46:20.450470Z","shell.execute_reply.started":"2025-10-06T03:46:20.444958Z","shell.execute_reply":"2025-10-06T03:46:20.449909Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"5538576"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"def translate(model, src_text, max_length=20, pad_id=0, eos_id=3):\n    tgt_text = \"\"\n    token_ids = []\n    for index in range(max_length):\n        batch, _ = nmt_collate_fn([{\"sourceString\": src_text,\n                                    \"targetString\": tgt_text}])\n        with torch.no_grad():\n            Y_logits = model(batch.to(device))\n            Y_token_ids = Y_logits.argmax(dim=1)  # find the best token IDs\n            next_token_id = Y_token_ids[0, index]  # take the last token ID\n\n        next_token = nmt_tokenizer.id_to_token(next_token_id)\n        tgt_text += \" \" + next_token\n        if next_token_id == eos_id:\n            break\n    return tgt_text.replace('</s>', '')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T03:49:08.372133Z","iopub.execute_input":"2025-10-06T03:49:08.372606Z","iopub.status.idle":"2025-10-06T03:49:08.377856Z","shell.execute_reply.started":"2025-10-06T03:49:08.372581Z","shell.execute_reply":"2025-10-06T03:49:08.377149Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"model.eval()\ntranslate(model, \"I like to play soccer with my friends at the beach\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T03:49:10.282512Z","iopub.execute_input":"2025-10-06T03:49:10.282821Z","iopub.status.idle":"2025-10-06T03:49:15.727135Z","shell.execute_reply.started":"2025-10-06T03:49:10.282800Z","shell.execute_reply":"2025-10-06T03:49:15.726381Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"' Me gusta jugar fútbol con mis amigos en la playa en la playa . '"},"metadata":{}}],"execution_count":38}]}