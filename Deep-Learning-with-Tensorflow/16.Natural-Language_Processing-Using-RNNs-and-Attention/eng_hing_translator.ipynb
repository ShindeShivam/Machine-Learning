{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## English to Hinglish Neural Maching Translator\n",
        "\n"
      ],
      "metadata": {
        "id": "Dz1INOon70U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load and preprocess data\n",
        "# -------------------------------\n",
        "data = []\n",
        "filename = \"hinglish_upload_v1.json\"\n",
        "\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        obj = json.loads(line)\n",
        "        data.append({\n",
        "            \"English\": obj[\"translation\"][\"en\"],\n",
        "            \"Hinglish\": obj[\"translation\"][\"hi_ng\"]\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    return str(text).lower().strip()\n",
        "\n",
        "en_sentences = df[\"English\"].apply(preprocess_text).tolist()\n",
        "hing_sentences = df[\"Hinglish\"].apply(preprocess_text).tolist()\n",
        "print(\"step 1 done\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Tokenization\n",
        "# -------------------------------\n",
        "vocab_size = 4000\n",
        "max_length = 50\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(sentences, vocab_size):\n",
        "    words = [w for sent in sentences for w in sent.split()]\n",
        "    most_common = [w for w, _ in Counter(words).most_common(vocab_size-2)]\n",
        "    word2idx = {w: i+2 for i, w in enumerate(most_common)}\n",
        "    word2idx['<PAD>'] = 0\n",
        "    word2idx['<UNK>'] = 1\n",
        "    idx2word = {i:w for w,i in word2idx.items()}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "# Build vocabularies\n",
        "word2idx_en, idx2word_en = build_vocab(en_sentences, vocab_size)\n",
        "word2idx_hing, idx2word_hing = build_vocab(['sos ' + s + ' eos' for s in hing_sentences], vocab_size)\n",
        "\n",
        "def encode_sentence(sent, word2idx, max_length):\n",
        "    tokens = sent.split()\n",
        "    ids = [word2idx.get(w, 1) for w in tokens]\n",
        "    ids = [word2idx_hing.get('sos', 2)] + ids if 'sos' in word2idx_hing else ids\n",
        "    ids = ids[:max_length]\n",
        "    ids += [0]*(max_length - len(ids))\n",
        "    return ids\n",
        "\n",
        "X_enc = np.array([encode_sentence(s, word2idx_en, max_length) for s in en_sentences])\n",
        "X_dec = np.array([encode_sentence('sos '+s, word2idx_hing, max_length) for s in hing_sentences])\n",
        "Y_seq = np.array([encode_sentence(s+' eos', word2idx_hing, max_length) for s in hing_sentences])\n",
        "\n",
        "# Train / Validation split\n",
        "X_train_enc = torch.tensor(X_enc[:150_000], dtype=torch.long)\n",
        "X_valid_enc = torch.tensor(X_enc[150_000:], dtype=torch.long)\n",
        "X_train_dec = torch.tensor(X_dec[:150_000], dtype=torch.long)\n",
        "X_valid_dec = torch.tensor(X_dec[150_000:], dtype=torch.long)\n",
        "Y_train = torch.tensor(Y_seq[:150_000], dtype=torch.long)\n",
        "Y_valid = torch.tensor(Y_seq[150_000:], dtype=torch.long)\n",
        "\n",
        "print(\"step 2 done\")\n",
        "# -------------------------------\n",
        "# 3. Dataset & Dataloader\n",
        "# -------------------------------\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, enc, dec, target):\n",
        "        self.enc = enc\n",
        "        self.dec = dec\n",
        "        self.target = target\n",
        "    def __len__(self):\n",
        "        return len(self.enc)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.enc[idx], self.dec[idx], self.target[idx]\n",
        "\n",
        "train_dataset = TranslationDataset(X_train_enc, X_train_dec, Y_train)\n",
        "valid_dataset = TranslationDataset(X_valid_enc, X_valid_dec, Y_valid)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
        "print(\"step 3 done\")\n",
        "# -------------------------------\n",
        "# 4. Model Definition\n",
        "# -------------------------------\n",
        "class Seq2SeqAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=128, enc_hidden=256, dec_hidden=512, max_length=50):\n",
        "        super().__init__()\n",
        "        self.enc_hidden = enc_hidden\n",
        "        self.dec_hidden = dec_hidden\n",
        "\n",
        "        # Embeddings\n",
        "        self.enc_embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.dec_embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.enc_dropout = nn.Dropout(0.1)\n",
        "        self.dec_dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.LSTM(embed_size, enc_hidden, batch_first=True, bidirectional=True, dropout=0.1)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.LSTM(embed_size, dec_hidden, batch_first=True, dropout=0.1)\n",
        "\n",
        "        # Attention (Additive / Bahdanau)\n",
        "        self.attn = nn.Linear(enc_hidden*2 + dec_hidden, dec_hidden)\n",
        "        self.v = nn.Linear(dec_hidden, 1, bias=False)\n",
        "\n",
        "        # Dense\n",
        "        self.fc1 = nn.Linear(dec_hidden + enc_hidden*2, 256)\n",
        "        self.fc2 = nn.Linear(256, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, enc_input, dec_input):\n",
        "        # Encoder\n",
        "        enc_emb = self.enc_dropout(self.enc_embed(enc_input))\n",
        "        enc_outputs, (h, c) = self.encoder(enc_emb)\n",
        "        h = torch.cat((h[0], h[1]), dim=1).unsqueeze(0)\n",
        "        c = torch.cat((c[0], c[1]), dim=1).unsqueeze(0)\n",
        "\n",
        "        # Decoder\n",
        "        dec_emb = self.dec_dropout(self.dec_embed(dec_input))\n",
        "        dec_outputs, _ = self.decoder(dec_emb, (h, c))\n",
        "\n",
        "        # Attention\n",
        "        B, T, H = dec_outputs.size()\n",
        "        enc_len = enc_outputs.size(1)\n",
        "        context_vectors = []\n",
        "        for t in range(T):\n",
        "            dec_step = dec_outputs[:, t, :].unsqueeze(1).repeat(1, enc_len, 1)\n",
        "            energy = torch.tanh(self.attn(torch.cat((dec_step, enc_outputs), dim=2)))\n",
        "            attn_weight = F.softmax(self.v(energy).squeeze(2), dim=1)\n",
        "            context = torch.bmm(attn_weight.unsqueeze(1), enc_outputs)\n",
        "            context_vectors.append(context)\n",
        "        context_vectors = torch.cat(context_vectors, dim=1)\n",
        "\n",
        "        combined = torch.cat((dec_outputs, context_vectors), dim=2)\n",
        "        out = F.relu(self.fc1(combined))\n",
        "        out = self.dropout(out)\n",
        "        logits = self.fc2(out)\n",
        "        return logits\n",
        "print(\"step 4 done\")\n",
        "# -------------------------------\n",
        "# 5. Training Setup\n",
        "# -------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Seq2SeqAttention(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(\"step 5 done\")\n",
        "# -------------------------------\n",
        "# 6. Training Loop\n",
        "# -------------------------------\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    step = 1\n",
        "    for enc_batch, dec_batch, target_batch in train_loader:\n",
        "        enc_batch, dec_batch, target_batch = enc_batch.to(device), dec_batch.to(device), target_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(enc_batch, dec_batch)  # [B, T, vocab]\n",
        "        loss = criterion(output.view(-1, vocab_size), target_batch.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        step+=1\n",
        "        print(step,end=\"\\r\")\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    step = 0\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for enc_batch, dec_batch, target_batch in valid_loader:\n",
        "            enc_batch, dec_batch, target_batch = enc_batch.to(device), dec_batch.to(device), target_batch.to(device)\n",
        "            output = model(enc_batch, dec_batch)\n",
        "            loss = criterion(output.view(-1, vocab_size), target_batch.view(-1))\n",
        "            val_loss += loss.item()\n",
        "    print(f\"Validation Loss: {val_loss/len(valid_loader):.4f}\")\n",
        "print(\"step 6 done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2xsPevlbVRj",
        "outputId": "47fb151b-0e52-40ed-d109-51b248336405"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1 done\n",
            "step 2 done\n",
            "step 3 done\n",
            "step 4 done\n",
            "step 5 done\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 1.8513\n",
            "Validation Loss: 1.0729\n",
            "Epoch 2/10 - Loss: 0.9762\n",
            "Validation Loss: 0.9029\n",
            "Epoch 3/10 - Loss: 0.8182\n",
            "Validation Loss: 0.8526\n",
            "Epoch 4/10 - Loss: 0.7329\n",
            "Validation Loss: 0.8389\n",
            "Epoch 5/10 - Loss: 0.6770\n",
            "Validation Loss: 0.8317\n",
            "Epoch 6/10 - Loss: 0.6368\n",
            "Validation Loss: 0.8424\n",
            "Epoch 7/10 - Loss: 0.6047\n",
            "Validation Loss: 0.8480\n",
            "Epoch 8/10 - Loss: 0.5803\n",
            "Validation Loss: 0.8552\n",
            "Epoch 9/10 - Loss: 0.5589\n",
            "Validation Loss: 0.8706\n",
            "Epoch 10/10 - Loss: 0.5443\n",
            "Validation Loss: 0.8784\n",
            "step 6 done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 7. Model Testing and Inference\n",
        "# -------------------------------\n",
        "\n",
        "def translate_sentence(model, sentence, word2idx_en, word2idx_hing, idx2word_hing, max_length=50, device='cpu'):\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess input sentence\n",
        "    sentence = preprocess_text(sentence)\n",
        "\n",
        "    # Encode input sentence\n",
        "    enc_input = encode_sentence(sentence, word2idx_en, max_length)\n",
        "    enc_input = torch.tensor([enc_input], dtype=torch.long).to(device)\n",
        "\n",
        "    # Initialize decoder input with SOS token\n",
        "    sos_token = word2idx_hing.get('sos', 2)\n",
        "    dec_input = [sos_token]\n",
        "\n",
        "    # Generate translation token by token\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Pad decoder input to max_length\n",
        "            dec_input_padded = dec_input + [0] * (max_length - len(dec_input))\n",
        "            dec_input_tensor = torch.tensor([dec_input_padded], dtype=torch.long).to(device)\n",
        "\n",
        "            output = model(enc_input, dec_input_tensor)\n",
        "            next_token = output[0, len(dec_input)-1, :].argmax().item()\n",
        "\n",
        "            if next_token == word2idx_hing.get('eos', 3) or next_token == 0:\n",
        "                break\n",
        "\n",
        "            dec_input.append(next_token)\n",
        "\n",
        "    # Convert tokens back to words (skip SOS token and filter out unwanted tokens)\n",
        "    translation = []\n",
        "    for token_id in dec_input[1:]:\n",
        "        word = idx2word_hing.get(token_id, '<UNK>')\n",
        "        if word not in ['<PAD>', 'eos', 'sos']:  # Filter out special tokens\n",
        "            translation.append(word)\n",
        "\n",
        "    return ' '.join(translation)\n",
        "\n",
        "\n",
        "\n",
        "def test_model(model, test_sentences, word2idx_en, word2idx_hing, idx2word_hing, device='cpu'):\n",
        "\n",
        "\n",
        "    for i, sentence in enumerate(test_sentences, 1):\n",
        "        print(f\"\\n{i}. English: {sentence}\")\n",
        "\n",
        "        # Greedy decoding\n",
        "        translation_greedy = translate_sentence(\n",
        "            model, sentence, word2idx_en, word2idx_hing, idx2word_hing, device=device\n",
        "        )\n",
        "        print(f\"   Hinglish : {translation_greedy}\")\n",
        "\n",
        "\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "   \"i can do anything\",\n",
        "   \"i like to play cricket\",\n",
        "   \"who are you \",\n",
        "   \"I like to walk\"\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Run tests\n",
        "print(\"Testing the trained model...\")\n",
        "test_model(model, test_sentences, word2idx_en, word2idx_hing, idx2word_hing, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOrFEF-tyGbk",
        "outputId": "ea0a6c27-04d9-46f2-b657-80bf9065e2df"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the trained model...\n",
            "\n",
            "1. English: i can do anything\n",
            "   Hinglish : mai kuch bhi kar sakta hu\n",
            "\n",
            "2. English: i like to play cricket\n",
            "   Hinglish : mujhe cricket khelne ke like pasand hai\n",
            "\n",
            "3. English: who are you \n",
            "   Hinglish : aap kon ho?\n",
            "\n",
            "4. English: I like to walk\n",
            "   Hinglish : mujhe walk karna hai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_test():\n",
        "    \"\"\"\n",
        "    Interactive testing where user can input sentences\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"INTERACTIVE TESTING\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Enter English sentences to translate (type 'quit' to exit):\")\n",
        "\n",
        "    while True:\n",
        "        sentence = input(\"\\nEnglish: \").strip()\n",
        "        if sentence.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        if sentence:\n",
        "            translation = translate_sentence(\n",
        "                model, sentence, word2idx_en, word2idx_hing, idx2word_hing, device=device\n",
        "            )\n",
        "            print(f\"Hinglish: {translation}\")\n",
        "\n",
        "\n",
        "interactive_test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztmG-NMw3GMf",
        "outputId": "50100880-27ed-4dc4-d233-55c5e701293e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "INTERACTIVE TESTING\n",
            "================================================================================\n",
            "Enter English sentences to translate (type 'quit' to exit):\n",
            "\n",
            "English: who are you\n",
            "Hinglish: aap kon ho?\n",
            "\n",
            "English:  i want to watch this movie\n",
            "Hinglish: muje is movie ko watch karna chahte hai\n",
            "\n",
            "English: i can do anything\n",
            "Hinglish: mai kuch bhi kar sakta hu\n",
            "\n",
            "English: i like to walk\n",
            "Hinglish: mujhe walk karna hai\n",
            "\n",
            "English: i like to play cricket\n",
            "Hinglish: mujhe cricket khelne ke like pasand hai\n",
            "\n",
            "English:  this is magic\n",
            "Hinglish: ye to <UNK> hai\n",
            "\n",
            "English:  this is my friend\n",
            "Hinglish: ye meri friend <UNK> hai\n",
            "\n",
            "English: this is good \n",
            "Hinglish: ye achha hai\n",
            "\n",
            "English: but not too much\n",
            "Hinglish: lekin itna bhi nahi\n",
            "\n",
            "English: but i like it\n",
            "Hinglish: lekin muje pasand hein\n",
            "\n",
            "English: not too bad\n",
            "Hinglish: bura nahin\n",
            "\n",
            "English: this is best action film\n",
            "Hinglish: ye best action film hai\n",
            "\n",
            "English: this is worse action movie\n",
            "Hinglish: ye kya ye sab film hai\n",
            "\n",
            "English: quit\n"
          ]
        }
      ]
    }
  ]
}